commit 565ddaf86cea6461649f22a362409682c9ea1d58
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Feb 23 10:44:08 2019 -0500

    fine-grained control of CPU use
    
    bhw2 has 2 sockets with 10 cores and hyperthreads. add support to
    enable/disable specific CPUs at various levels.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 39c1be04a0..6da8082f5a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -761,7 +761,7 @@ TEXT getret(SB), NOSPLIT, $0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT ·htpause(SB), NOSPLIT, $0-0
+TEXT ·Htpause(SB), NOSPLIT, $0-0
 	PAUSE
 	RET
 

commit 7d320f31de71051a996978dd6ced377e327d03d1
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Feb 13 15:26:13 2019 -0500

    disable syscall fast path
    
    it improves performance only a little and has been broken for a while.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7ca6935b2d..39c1be04a0 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1115,50 +1115,52 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 
 //func Userrun_slow(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
 //    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
-TEXT ·Userrun(SB), $0-0
-	CLI
-
-	SWAPGS
-	MOVQ	0(GS), R9
-	SWAPGS
-	//LEAQ	·cpus(SB), R9
-
-	MOVQ	0x18(SP), AX
-	CMPQ	0x28(R9), AX
-	JNE	out
-
-	MOVQ	0x8(SP), AX
-	MOVQ	0x8(AX), AX
-	CMPQ	0x30(R9), AX
-	JNE	out
-
-	MOVQ	0x8(SP), AX
-	CMPQ	0x38(R9), AX
-	JNE	out
-
-	MOVB	0x20(SP), AX
-	TESTB	AX, AX
-	JZ	out
-
-	MOVQ	ARG1(SP), BX
-	SUBQ	$5*8, SP
-	MOVQ	R9, 0x10(SP)
-	MOVB	AX, 0x8(SP)
-	MOVQ	BX, 0x0(SP)
-	CALL	·_Userrun(SB)
-	MOVQ	0x18(SP), AX
-	MOVQ	0x20(SP), BX
-
-	ADDQ	$5*8, SP
-	MOVQ	AX, 0x30(SP)
-	MOVQ	BX, 0x38(SP)
-	MOVQ	$0, 0x40(SP)
-	MOVQ	$0, 0x48(SP)
-	STI
-	RET
-out:
-	ADDQ	$8, SP
-	JMP	runtime·Userrun_slow(SB)
+// this code is brittle and, at the time this comment was written, broken;
+// saved in case i want the performance back some day
+//TEXT ·Userrun(SB), $0-0
+//	CLI
+//
+//	SWAPGS
+//	MOVQ	0(GS), R9
+//	SWAPGS
+//	//LEAQ	·cpus(SB), R9
+//
+//	MOVQ	0x18(SP), AX
+//	CMPQ	0x28(R9), AX
+//	JNE	out
+//
+//	MOVQ	0x8(SP), AX
+//	MOVQ	0x8(AX), AX
+//	CMPQ	0x30(R9), AX
+//	JNE	out
+//
+//	MOVQ	0x8(SP), AX
+//	CMPQ	0x38(R9), AX
+//	JNE	out
+//
+//	MOVB	0x20(SP), AX
+//	TESTB	AX, AX
+//	JZ	out
+//
+//	MOVQ	ARG1(SP), BX
+//	SUBQ	$5*8, SP
+//	MOVQ	R9, 0x10(SP)
+//	MOVB	AX, 0x8(SP)
+//	MOVQ	BX, 0x0(SP)
+//	CALL	·_Userrun(SB)
+//	MOVQ	0x18(SP), AX
+//	MOVQ	0x20(SP), BX
+//
+//	ADDQ	$5*8, SP
+//	MOVQ	AX, 0x30(SP)
+//	MOVQ	BX, 0x38(SP)
+//	MOVQ	$0, 0x40(SP)
+//	MOVQ	$0, 0x48(SP)
+//	STI
+//	RET
+//out:
+//	ADDQ	$8, SP
+//	JMP	runtime·Userrun_slow(SB)
 
 // if you change the number of arguments, you must adjust the stack offsets in
 // _sysentry and ·_userint.

commit 1dc8454c823a570c52bc57e8613cb41437c1d68d
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Feb 4 21:07:21 2019 -0500

    stores to MMIO registers must not be locked
    
    hurray, bhw2 finally works! goodbye four days of my life

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3ac477802c..7ca6935b2d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -572,6 +572,13 @@ TEXT ·Store32(SB), NOSPLIT, $0-16
 	MOVL	DX, (AX)
 	RET
 
+// Used to guarantee 64bit writes without the lock prefix
+TEXT ·Store64(SB), NOSPLIT, $0-16
+	MOVQ	addr+0(FP), AX
+	MOVQ	v+8(FP), DX
+	MOVQ	DX, (AX)
+	RET
+
 // void lidt(pdesc_t);
 TEXT ·lidt(SB), NOSPLIT, $0-16
 	// lidtq 8(%rsp)

commit 5fc29bd3e1d27383e9d4b20f2693030f80e9d1e0
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Jul 28 18:22:09 2018 -0400

    SSE optimized, distributed size-class reservations
    
    (still uses live bytes since MAXLIVE doesn't support size-classes)
    
    throughput is still good: parrun performance is 16.1k msgs/sec and nginx is
    higher than the submission. using SIMD for adding/subtracting/comparing
    size-classes improves throughput by ~6% compared to iteration.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 81b616f08e..3ac477802c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1341,6 +1341,90 @@ TEXT ·Mfence(SB), NOSPLIT, $0
 	MFENCE
 	RET
 
+// void Objsadd(Resobjs_t *src, Resobjs_t *dst)
+TEXT ·Objsadd(SB), NOSPLIT, $16-0
+	XORL	CX, CX
+	MOVQ	src+0(FP), SI
+	MOVQ	dst+8(FP), DI
+loop:
+	MOVOU	(DI), X0
+	MOVOU	(SI), X1
+	PADDL	X1, X0
+	MOVOU	X0, (DI)
+	ADDQ	$16, SI
+	ADDQ	$16, DI
+	INCL	CX
+	CMPL	CX, $6
+	JEQ	done
+	JMP	loop
+done:
+	RET
+
+// void Objssub(Resobjs_t *src, Resobjs_t *dst)
+TEXT ·Objssub(SB), NOSPLIT, $16-0
+	XORL	CX, CX
+	MOVQ	src+0(FP), SI
+	MOVQ	dst+8(FP), DI
+loop:
+	MOVOU	(DI), X0
+	MOVOU	(SI), X1
+	PSUBL	X1, X0
+	MOVOU	X0, (DI)
+	ADDQ	$16, SI
+	ADDQ	$16, DI
+	INCL	CX
+	CMPL	CX, $6
+	JEQ	done
+	JMP	loop
+done:
+	RET
+
+// uint Objscmp(Resobjs_t *a, Resobjs_t *b)
+TEXT ·Objscmp(SB), NOSPLIT, $0-0x18
+	XORL	R8, R8
+	XORL	AX, AX
+	XORL	DX, DX
+	MOVQ	src+0(FP), SI
+	MOVQ	dst+8(FP), DI
+loop:
+	MOVOU	(DI), X0
+	MOVOU	(SI), X1
+	PCMPGTL	X0, X1
+	PMOVMSKB X1, AX
+	TESTL	AX, AX
+	JNZ	fail
+cont:
+	ADDQ	$16, SI
+	ADDQ	$16, DI
+	INCL	R8
+	CMPL	R8, $6
+	JEQ	done
+	JMP	loop
+fail:
+	// convert to bitmask
+	PEXTRD	$0, X1, R9
+	ANDL	$1, R9
+
+	PEXTRD	$1, X1, BX
+	ANDL	$2, BX
+	ORQ	BX, R9
+
+	PEXTRD	$2, X1, BX
+	ANDL	$4, BX
+	ORQ	BX, R9
+
+	PEXTRD	$3, X1, BX
+	ANDL	$8, BX
+	ORQ	BX, R9
+
+	LEAQ	0(R8*4), CX
+	SHLQ	CX, R9
+	ORQ	R9, DX
+	JMP	cont
+done:
+	MOVQ	DX, ret+0x10(FP)
+	RET
+
 // void backtracetramp(uintptr newsp, uintptr *tf, g *gp)
 TEXT ·backtracetramp(SB), NOSPLIT, $0-0x18
 	MOVQ	SP, AX

commit 2081f1a3e270062fa2418a26dab4f37039c151ea
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Jul 11 12:31:04 2018 -0400

    runtime.[LSM]fence()

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8db742daf3..81b616f08e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1329,6 +1329,18 @@ TEXT ·_Gscpu(SB), NOSPLIT, $0-8
 	SWAPGS
 	RET
 
+TEXT ·Lfence(SB), NOSPLIT, $0
+	LFENCE
+	RET
+
+TEXT ·Sfence(SB), NOSPLIT, $0
+	SFENCE
+	RET
+
+TEXT ·Mfence(SB), NOSPLIT, $0
+	MFENCE
+	RET
+
 // void backtracetramp(uintptr newsp, uintptr *tf, g *gp)
 TEXT ·backtracetramp(SB), NOSPLIT, $0-0x18
 	MOVQ	SP, AX

commit d5ca5f676b0ec7f112e33d2d648c60a7e727e212
Merge: 8294fc745a b8826d12a6
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Apr 20 22:15:47 2018 -0400

    Merge commit 'b8826d12a691bdd63c8d7522e9bc26a1df0d717a' into remerge

commit 06c8387457653b06a2c270fb43470217bbc077b2
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Apr 18 21:58:07 2018 -0400

    record backtrace during PMC interrupt for kernel code
    
    a bit tricky since an NMI can occur when interrupts are masked, like between
    biscuit's swapgs pairs. luckily we can use go's backtrace code without much
    difficulty.
    
    hopefully this will be helpful in optimizing our benchmarks.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 83441ffe90..eed32855ed 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1192,6 +1192,21 @@ TEXT ·_Gscpu(SB), NOSPLIT, $0-8
 	SWAPGS
 	RET
 
+// void backtracetramp(uintptr newsp, uintptr *tf, g *gp)
+TEXT ·backtracetramp(SB), NOSPLIT, $0-0x18
+	MOVQ	SP, AX
+	MOVQ	tf+0x8(FP), DX
+	MOVQ	gp+0x10(FP), CX
+	MOVQ	newsp+0(FP), SP
+	PUSHQ	AX
+	PUSHQ	CX
+	PUSHQ	DX
+	CALL	·nmibacktrace1(SB)
+	POPQ	DX
+	POPQ	DX
+	POPQ	SP
+	RET
+
 /*
  *  go-routine
  */

commit 112690bce58a6b9a2c8b677c20babfbdd8302901
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Apr 15 13:41:44 2018 -0400

    fix Userrun() stack size
    
    compiler changed stack size. someday i will fix it so stack size changes won't
    matter.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0b714ec7c4..7c222d082f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1150,6 +1150,7 @@ TEXT ·Userrun(SB), $0-0
 	STI
 	RET
 out:
+	ADDQ	$8, SP
 	JMP	runtime·Userrun_slow(SB)
 
 // if you change the number of arguments, you must adjust the stack offsets in

commit f5a071578940cdc8c89faabb6b20747ed6a8619d
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 14 11:46:24 2018 -0400

    merge startup path with go1.10.1

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3699dd5569..0b714ec7c4 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -305,26 +305,20 @@ done:
 // i do it this strange way because if i declare fakeargv in C i get 'missing
 // golang type information'. need two 0 entries because go checks for
 // environment variables too.
-DATA	fakeargv+0(SB)/8,$·gostr(SB)
-DATA	fakeargv+8(SB)/8,$0
-DATA	fakeargv+16(SB)/8,$0
-GLOBL	fakeargv(SB),RODATA,$24
+//DATA	fakeargv+0(SB)/8,$·gostr(SB)
+//DATA	fakeargv+8(SB)/8,$0
+//DATA	fakeargv+16(SB)/8,$0
+//GLOBL	fakeargv(SB),RODATA,$24
 
 TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	MOVL	DI, ·P_kpmap(SB)
 	MOVL	SI, ·pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
-	CALL	runtime·sc_setup(SB)
 
-	// copy arguments forward on an even stack
-	//MOVQ	DI, AX		// argc
-	//MOVQ	SI, BX		// argv
-	MOVQ	$0, AX		// argc
-	MOVQ	$0, BX		// argv
 	SUBQ	$(4*8+7), SP		// 2args 2auto
 	ANDQ	$~15, SP
-	MOVQ	AX, 16(SP)
-	MOVQ	BX, 24(SP)
+
+	CALL	runtime·sc_setup(SB)
 
 	// create istack out of the given (operating system) stack.
 	// _cgo_init may update stackguard.
@@ -336,9 +330,10 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	MOVQ	SP, (g_stack+stack_hi)(DI)
 
 	// find out information about the processor we're on
-	MOVQ	$0, AX
+	MOVL	$0, AX
 	CPUID
-	CMPQ	AX, $0
+	MOVL	AX, SI
+	CMPL	AX, $0
 	JE	nocpuinfo
 
 	// Figure out how to serialize RDTSC.
@@ -350,49 +345,85 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	JNE	notintel
 	CMPL	CX, $0x6C65746E  // "ntel"
 	JNE	notintel
+	MOVB	$1, runtime·isIntel(SB)
 	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
 notintel:
-	// Do nothing.
 
-	MOVQ	$1, AX
+	// Load EAX=1 cpuid flags
+	MOVL	$1, AX
 	CPUID
-	MOVL	CX, runtime·cpuid_ecx(SB)
-	MOVL	DX, runtime·cpuid_edx(SB)
-	// Detect AVX and AVX2 as per 14.7.1  Detection of AVX2 chapter of [1]
-	// [1] 64-ia-32-architectures-software-developer-manual-325462.pdf
-	// http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf
-	ANDL    $0x18000000, CX // check for OSXSAVE and AVX bits
-	CMPL    CX, $0x18000000
-	JNE     noavx
-	MOVL    $0, CX
+	MOVL	AX, runtime·processorVersionInfo(SB)
+
+	TESTL	$(1<<26), DX // SSE2
+	SETNE	runtime·support_sse2(SB)
+
+	TESTL	$(1<<9), CX // SSSE3
+	SETNE	runtime·support_ssse3(SB)
+
+	TESTL	$(1<<19), CX // SSE4.1
+	SETNE	runtime·support_sse41(SB)
+
+	TESTL	$(1<<20), CX // SSE4.2
+	SETNE	runtime·support_sse42(SB)
+
+	TESTL	$(1<<23), CX // POPCNT
+	SETNE	runtime·support_popcnt(SB)
+
+	TESTL	$(1<<25), CX // AES
+	SETNE	runtime·support_aes(SB)
+
+	TESTL	$(1<<27), CX // OSXSAVE
+	SETNE	runtime·support_osxsave(SB)
+
+	// If OS support for XMM and YMM is not present
+	// support_avx will be set back to false later.
+	TESTL	$(1<<28), CX // AVX
+	SETNE	runtime·support_avx(SB)
+
+eax7:
+	// Load EAX=7/ECX=0 cpuid flags
+	CMPL	SI, $7
+	JLT	osavx
+	MOVL	$7, AX
+	MOVL	$0, CX
+	CPUID
+
+	TESTL	$(1<<3), BX // BMI1
+	SETNE	runtime·support_bmi1(SB)
+
+	// If OS support for XMM and YMM is not present
+	// support_avx2 will be set back to false later.
+	TESTL	$(1<<5), BX
+	SETNE	runtime·support_avx2(SB)
+
+	TESTL	$(1<<8), BX // BMI2
+	SETNE	runtime·support_bmi2(SB)
+
+	TESTL	$(1<<9), BX // ERMS
+	SETNE	runtime·support_erms(SB)
+
+osavx:
+	CMPB	runtime·support_osxsave(SB), $1
+	JNE	noavx
+	MOVL	$0, CX
 	// For XGETBV, OSXSAVE bit is required and sufficient
 	XGETBV
-	ANDL    $6, AX
-	CMPL    AX, $6 // Check for OS support of YMM registers
-	JNE     noavx
-	MOVB    $1, runtime·support_avx(SB)
-	MOVL    $7, AX
-	MOVL    $0, CX
-	CPUID
-	ANDL    $0x20, BX // check for AVX2 bit
-	CMPL    BX, $0x20
-	JNE     noavx2
-	MOVB    $1, runtime·support_avx2(SB)
-	JMP     nocpuinfo
+	ANDL	$6, AX
+	CMPL	AX, $6 // Check for OS support of XMM and YMM registers.
+	JE nocpuinfo
 noavx:
-	MOVB    $0, runtime·support_avx(SB)
-noavx2:
-	MOVB    $0, runtime·support_avx2(SB)
-nocpuinfo:
+	MOVB $0, runtime·support_avx(SB)
+	MOVB $0, runtime·support_avx2(SB)
 
+nocpuinfo:
 	// if there is an _cgo_init, call it.
-	//MOVQ	_cgo_init(SB), AX
-	//TESTQ	AX, AX
-	//JZ	needtls
-	//// g0 already in DI
-	//MOVQ	DI, CX	// Win64 uses CX for first parameter
-	//MOVQ	$setg_gcc<>(SB), SI
-	//CALL	AX
+	MOVQ	_cgo_init(SB), AX
+	TESTQ	AX, AX
+	JZ	needtls
+	// g0 already in DI
+	MOVQ	DI, CX	// Win64 uses CX for first parameter
+	MOVQ	$setg_gcc<>(SB), SI
+	CALL	AX
 
 	// update stackguard after _cgo_init
 	MOVQ	$runtime·g0(SB), CX
@@ -401,19 +432,18 @@ nocpuinfo:
 	MOVQ	AX, g_stackguard0(CX)
 	MOVQ	AX, g_stackguard1(CX)
 
-//#ifndef GOOS_windows
-//	JMP ok
-//#endif
+#ifndef GOOS_windows
+	JMP ok
+#endif
 needtls:
-//#ifdef GOOS_plan9
-//	// skip TLS setup on Plan 9
-//	JMP ok
-//#endif
-//#ifdef GOOS_solaris
-//	// skip TLS setup on Solaris
-//	JMP ok
-//#endif
-
+#ifdef GOOS_plan9
+	// skip TLS setup on Plan 9
+	JMP ok
+#endif
+#ifdef GOOS_solaris
+	// skip TLS setup on Solaris
+	JMP ok
+#endif
 	//LEAQ	runtime·m0+m_tls(SB), DI
 	//CALL	runtime·settls(SB)
 
@@ -430,13 +460,14 @@ needtls:
 	MOVQ	$0x123, g(BX)
 	MOVQ	runtime·m0+m_tls(SB), AX
 	CMPQ	AX, $0x123
-	JMP	ok
+	JEQ	ok
 	MOVQ	$0x42, (SP)
 	CALL	runtime·putch(SB)
 	MOVQ	$0x46, (SP)
 	CALL	runtime·putch(SB)
 	BYTE	$0xeb;
 	BYTE	$0xfe;
+	CALL	runtime·abort(SB)
 ok:
 	// set the per-goroutine and per-mach "registers"
 	get_tls(BX)
@@ -457,11 +488,9 @@ ok:
 	CALL	runtime·check(SB)
 
 	//MOVL	16(SP), AX		// copy argc
-	MOVL	$1, AX		// copy argc
-	MOVL	AX, 0(SP)
+	//MOVL	AX, 0(SP)
 	//MOVQ	24(SP), AX		// copy argv
-	MOVQ	$fakeargv(SB), AX
-	MOVQ	AX, 8(SP)
+	//MOVQ	AX, 8(SP)
 	//CALL	runtime·args(SB)
 	CALL	runtime·osinit(SB)
 	CALL	runtime·schedinit(SB)
@@ -477,7 +506,7 @@ ok:
 	// start this M
 	CALL	runtime·mstart(SB)
 
-	MOVL	$0xf1, 0xf1  // crash
+	CALL	runtime·abort(SB)	// mstart should never return
 	RET
 
 TEXT runtime·Cpuid(SB), NOSPLIT, $0-24

commit 5c034daf10983f18fe9e629f9daa0c20b09fa1c4
Merge: 8d0a669400 9d4215311b
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 14 07:50:10 2018 -0400

    Merge commit '9d4215311ba573a5b676de85b053eec03e577478' into rememrge

commit 690b83a1ace12d6985b0d3e9235e011acb87b0fc
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Apr 3 14:22:09 2018 -0400

    more to the pile of fast syscall hacks
    
    add syscall fast path. i'm tired of optimizing syscall overhead!
    
    biscuit's syscall design incurs additional overhead. it is convenient to treat
    a return to userspace as a function call (i.e. Userrun()) from the kernel. the
    result is that the corresponding return instruction is executed after switching
    to and back from usermode, which is quite slow (70us/getppid vs 50us/gettpid).
    
    i suspect my CPU flushes the return buffer after switching privilege modes. it
    isn't TLB flushes since the TLB was not flushed (getppid benchmark) and
    everything should have been in the cache...

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e46f6385d7..83441ffe90 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -949,6 +949,8 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 	JMP	alltraps(SB)
 
 #define TFREGS		17
+#define TF_R15		(8*2)
+#define TF_R14		(8*3)
 #define TF_R13		(8*4)
 #define TF_R12		(8*5)
 #define TF_R8		(8*9)
@@ -962,44 +964,85 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 #define TF_RIP		(8*(TFREGS + 2))
 #define TF_RSP		(8*(TFREGS + 5))
 
-// if you change the number of arguments, you must adjust the stack offsets in
-// _sysentry and ·_userint.
-// func _Userrun(tf *[24]int, fastret bool) (int, int)
-TEXT ·_Userrun(SB), NOSPLIT, $8-16
-	MOVQ	tf+0(FP), R9
+#define ARG1(x)		0x8(x)
+#define ARG2(x)		0x10(x)
+#define ARG3(x)		0x18(x)
+#define RET1(x)		0x20(x)
+#define RET2(x)		0x28(x)
+
+//func Userrun_slow(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
+//    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
+TEXT ·Userrun(SB), $0-0
+	CLI
 
 	SWAPGS
-	MOVQ	0(GS), AX
+	MOVQ	0(GS), R9
 	SWAPGS
+	//LEAQ	·cpus(SB), R9
+
+	MOVQ	0x18(SP), AX
+	CMPQ	0x28(R9), AX
+	JNE	out
+
+	MOVQ	0x8(SP), AX
+	MOVQ	0x8(AX), AX
+	CMPQ	0x30(R9), AX
+	JNE	out
+
+	MOVQ	0x8(SP), AX
+	CMPQ	0x38(R9), AX
+	JNE	out
+
+	MOVB	0x20(SP), AX
+	TESTB	AX, AX
+	JZ	out
+
+	MOVQ	ARG1(SP), BX
+	SUBQ	$5*8, SP
+	MOVQ	R9, 0x10(SP)
+	MOVB	AX, 0x8(SP)
+	MOVQ	BX, 0x0(SP)
+	CALL	·_Userrun(SB)
+	MOVQ	0x18(SP), AX
+	MOVQ	0x20(SP), BX
+
+	ADDQ	$5*8, SP
+	MOVQ	AX, 0x30(SP)
+	MOVQ	BX, 0x38(SP)
+	MOVQ	$0, 0x40(SP)
+	MOVQ	$0, 0x48(SP)
+	STI
+	RET
+out:
+	JMP	runtime·Userrun_slow(SB)
+
+// if you change the number of arguments, you must adjust the stack offsets in
+// _sysentry and ·_userint.
+// func _Userrun(tf *[24]int, fastret bool, cpu *cpu_t) (int, int)
+TEXT ·_Userrun(SB), NOSPLIT, $0-0
+dur:
+	MOVQ	ARG1(SP), R9
+	MOVQ	ARG3(SP), AX
+
 	MOVQ	SP, 0x20(AX)
 
 	// fastret or iret?
-	MOVB	fastret+8(FP), AX
+	// ax = fastret
+	MOVB	ARG2(SP), AX
 	CMPB	AX, $0
-	// swap for better branch prediction?
-	JNE	syscallreturn
-	// do full state restore
-	PUSHQ	R9
-	CALL	·_userret(SB)
-	INT	$3
-	MOVL	$0x5cc, DX
-	TESTL	AX, AX
-	MOVL	DX, CX
-	ORL	$0x20, DX
-	DIVL	0x58(CX)
-	ANDL	$0x1f, DX
-	MOVQ	CR4, AX
-	MOVL	$0x16, AX
-	MOVL	0x104(CX), SI
-	MOVL	DI, 8(SP)
-	MOVB	$1, 3(DX)
-
+	JE	slowret
 syscallreturn:
 	MOVQ	TF_RAX(R9), AX
 	MOVQ	TF_RSP(R9), CX
 	MOVQ	TF_RIP(R9), DX
 	MOVQ	TF_RBP(R9), BP
 	MOVQ	TF_RBX(R9), BX
+
+	//MOVQ	TF_R15(R9), R15
+	//MOVQ	TF_R14(R9), R14
+	//MOVQ	TF_R13(R9), R13
+	//MOVQ	TF_R12(R9), R12
+
 	// rcx contains rsp
 	// rdx contains rip
 	STI
@@ -1007,9 +1050,25 @@ syscallreturn:
 	BYTE	$0x48
 	BYTE	$0x0f
 	BYTE	$0x35
-	// not reached; just to trick dead code analysis
-	CALL	·_sysentry(SB)
-	CALL	·_userint(SB)
+	INT	$3
+slowret:
+	// do full state restore
+	PUSHQ	R9
+	//CALL	·_userret(SB)
+	PUSHQ	$0
+	JMP	·_userret(SB)
+
+	MOVL	$0x5cc, DX
+	TESTL	AX, AX
+	MOVL	DX, CX
+	ORL	$0x20, DX
+	DIVL	0x58(CX)
+	ANDL	$0x1f, DX
+	MOVQ	CR4, AX
+	MOVL	$0x16, AX
+	MOVL	0x104(CX), SI
+	MOVL	DI, 8(SP)
+	MOVB	$1, 3(DX)
 
 // this should be a label since it is the bottom half of the Userrun_ function,
 // but i can't figure out how to get the plan9 assembler to let me use lea on a
@@ -1020,10 +1079,12 @@ TEXT ·_sysentry(SB), NOSPLIT, $0-0
 	SWAPGS
 	MOVQ	0(GS), SP
 	SWAPGS
+	//LEAQ	·cpus(SB), SP
+
 	MOVQ	0x20(SP), SP
 
 	// save user state in fake trapframe
-	MOVQ	0x18(SP), R9
+	MOVQ	ARG1(SP), R9
 	MOVQ	R10, TF_RSP(R9)
 	MOVQ	R11, TF_RIP(R9)
 	// syscall args
@@ -1036,11 +1097,57 @@ TEXT ·_sysentry(SB), NOSPLIT, $0-0
 	// kernel preserves rbp and rbx
 	MOVQ	BP,  TF_RBP(R9)
 	MOVQ	BX,  TF_RBX(R9)
+
+	//MOVQ	R15, TF_R15(R9)
+	//MOVQ	R14, TF_R14(R9)
+	//MOVQ	R13, TF_R13(R9)
+	//MOVQ	R12, TF_R12(R9)
+
 	// return val 1
-	MOVQ	TRAP_SYSCALL, 0x28(SP)
+	MOVQ	TRAP_SYSCALL, RET1(SP)
 	// return val 2
-	MOVQ	$0, 0x30(SP)
-	ADDQ	$0x10, SP
+	MOVQ	$0, RET2(SP)
+
+// return direct to userspace testing
+//	MOVQ	TF_RAX(R9), CX
+//	CMPQ	CX, $40
+//	JNE	slowsys
+//
+//	STI
+//	SUBQ	$3*8, SP
+//	get_tls(AX)
+//	MOVQ	g(AX), AX
+//	MOVQ	g_current(AX), AX
+//	MOVQ	(AX), AX
+//	MOVQ	AX, (SP)
+//	MOVQ	·DUR(SB), AX
+//	CALL	AX
+//	MOVQ	0x10(SP), BX
+//	MOVQ	(3*8 + 0x8)(SP), AX
+//	MOVQ	BX, TF_RAX(AX)
+//	ADDQ	$3*8, SP
+//	CLI
+//
+//	MOVQ	ARG1(SP), R9
+//	MOVQ	TF_RAX(R9), AX
+//	MOVQ	TF_RSP(R9), CX
+//	MOVQ	TF_RIP(R9), DX
+//	MOVQ	TF_RBP(R9), BP
+//	MOVQ	TF_RBX(R9), BX
+//
+//	MOVQ	TF_R15(R9), R15
+//	MOVQ	TF_R14(R9), R14
+//	MOVQ	TF_R13(R9), R13
+//	MOVQ	TF_R12(R9), R12
+//	// rcx contains rsp
+//	// rdx contains rip
+//	STI
+//	// rex64 sysexit
+//	BYTE	$0x48
+//	BYTE	$0x0f
+//	BYTE	$0x35
+//
+//slowsys:
 	RET
 
 // this is the bottom half of _userrun() that is executed if a timer int or CPU
@@ -1049,9 +1156,9 @@ TEXT ·_userint(SB), NOSPLIT, $0-0
 	CLI
 	// user state is already saved by trap handler.
 	// AX holds the interrupt number, BX holds aux (cr2 for page fault)
-	MOVQ	AX, 0x28(SP)
-	MOVQ	BX, 0x30(SP)
-	ADDQ	$0x10, SP
+	MOVQ	AX, RET1(SP)
+	MOVQ	BX, RET2(SP)
+	//ADDQ	$0x10, SP
 	RET
 
 TEXT ·gs_null(SB), NOSPLIT, $8-0

commit c950a90d7240a6f2124ae38564c137b86866b191
Author: Austin Clements <austin@google.com>
Date:   Wed Jan 17 15:51:09 2018 -0500

    runtime: call abort instead of raw INT $3 or bad MOV
    
    Everything except for amd64, amd64p32, and 386 currently defines and
    uses an abort function. This CL makes these match. The next CL will
    recognize the abort function to make this more useful.
    
    Change-Id: I7c155871ea48919a9220417df0630005b444f488
    Reviewed-on: https://go-review.googlesource.com/93660
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 953f118146..16e9f5fe40 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -227,7 +227,7 @@ needtls:
 	MOVQ	runtime·m0+m_tls(SB), AX
 	CMPQ	AX, $0x123
 	JEQ 2(PC)
-	MOVL	AX, 0	// abort
+	CALL	runtime·abort(SB)
 ok:
 	// set the per-goroutine and per-mach "registers"
 	get_tls(BX)
@@ -262,7 +262,7 @@ ok:
 	// start this M
 	CALL	runtime·mstart(SB)
 
-	MOVL	$0xf1, 0xf1  // crash
+	CALL	runtime·abort(SB)	// mstart should never return
 	RET
 
 DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)
@@ -446,14 +446,14 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	CMPQ	g(CX), SI
 	JNE	3(PC)
 	CALL	runtime·badmorestackg0(SB)
-	INT	$3
+	CALL	runtime·abort(SB)
 
 	// Cannot grow signal stack (m->gsignal).
 	MOVQ	m_gsignal(BX), SI
 	CMPQ	g(CX), SI
 	JNE	3(PC)
 	CALL	runtime·badmorestackgsignal(SB)
-	INT	$3
+	CALL	runtime·abort(SB)
 
 	// Called from f.
 	// Set m->morebuf to f's caller.
@@ -479,7 +479,7 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	BX, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(BX), SP
 	CALL	runtime·newstack(SB)
-	MOVQ	$0, 0x1003	// crash if newstack returns
+	CALL	runtime·abort(SB)	// crash if newstack returns
 	RET
 
 // morestack but not preserving ctxt.
@@ -886,16 +886,21 @@ TEXT setg_gcc<>(SB),NOSPLIT,$0
 	MOVQ	DI, g(AX)
 	RET
 
+TEXT runtime·abort(SB),NOSPLIT,$0-0
+	INT	$3
+loop:
+	JMP	loop
+
 // check that SP is in range [g->stack.lo, g->stack.hi)
 TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
 	get_tls(CX)
 	MOVQ	g(CX), AX
 	CMPQ	(g_stack+stack_hi)(AX), SP
 	JHI	2(PC)
-	INT	$3
+	CALL	runtime·abort(SB)
 	CMPQ	SP, (g_stack+stack_lo)(AX)
 	JHI	2(PC)
-	INT	$3
+	CALL	runtime·abort(SB)
 	RET
 
 // func cputicks() int64

commit 7f1b2738bb7a8863ee78d5357acbc820b7083821
Author: Austin Clements <austin@google.com>
Date:   Fri Jan 12 12:39:22 2018 -0500

    runtime: make throw safer to call
    
    Currently, throw may grow the stack, which means whenever we call it
    from a context where it's not safe to grow the stack, we first have to
    switch to the system stack. This is pretty easy to get wrong.
    
    Fix this by making throw switch to the system stack so it doesn't grow
    the stack and is hence safe to call without a system stack switch at
    the call site.
    
    The only thing this complicates is badsystemstack itself, which would
    now go into an infinite loop before printing anything (previously it
    would also go into an infinite loop, but would at least print the
    error first). Fix this by making badsystemstack do a direct write and
    then crash hard.
    
    Change-Id: Ic5b4a610df265e47962dcfa341cabac03c31c049
    Reviewed-on: https://go-review.googlesource.com/93659
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ab5407bbcd..953f118146 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -424,6 +424,7 @@ bad:
 	// Bad: g is not gsignal, not g0, not curg. What is it?
 	MOVQ	$runtime·badsystemstack(SB), AX
 	CALL	AX
+	INT	$3
 
 
 /*

commit ee58eccc565c0871d3f16fd702fd8649a3fb61ea
Author: Keith Randall <khr@golang.org>
Date:   Sun Mar 4 09:47:47 2018 -0800

    internal/bytealg: move short string Index implementations into bytealg
    
    Also move the arm64 CountByte implementation while we're here.
    
    Fixes #19792
    
    Change-Id: I1e0fdf1e03e3135af84150a2703b58dad1b0d57e
    Reviewed-on: https://go-review.googlesource.com/98518
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f91a01da72..ab5407bbcd 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1358,274 +1358,6 @@ DATA shifts<>+0xf0(SB)/8, $0x0807060504030201
 DATA shifts<>+0xf8(SB)/8, $0xff0f0e0d0c0b0a09
 GLOBL shifts<>(SB),RODATA,$256
 
-TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
-	MOVQ s+0(FP), DI
-	// We want len in DX and AX, because PCMPESTRI implicitly consumes them
-	MOVQ s_len+8(FP), DX
-	MOVQ c+16(FP), BP
-	MOVQ c_len+24(FP), AX
-	MOVQ DI, R10
-	LEAQ ret+32(FP), R11
-	JMP  runtime·indexShortStr(SB)
-
-TEXT bytes·indexShortStr(SB),NOSPLIT,$0-56
-	MOVQ s+0(FP), DI
-	MOVQ s_len+8(FP), DX
-	MOVQ c+24(FP), BP
-	MOVQ c_len+32(FP), AX
-	MOVQ DI, R10
-	LEAQ ret+48(FP), R11
-	JMP  runtime·indexShortStr(SB)
-
-// AX: length of string, that we are searching for
-// DX: length of string, in which we are searching
-// DI: pointer to string, in which we are searching
-// BP: pointer to string, that we are searching for
-// R11: address, where to put return value
-TEXT runtime·indexShortStr(SB),NOSPLIT,$0
-	CMPQ AX, DX
-	JA fail
-	CMPQ DX, $16
-	JAE sse42
-no_sse42:
-	CMPQ AX, $2
-	JA   _3_or_more
-	MOVW (BP), BP
-	LEAQ -1(DI)(DX*1), DX
-loop2:
-	MOVW (DI), SI
-	CMPW SI,BP
-	JZ success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop2
-	JMP fail
-_3_or_more:
-	CMPQ AX, $3
-	JA   _4_or_more
-	MOVW 1(BP), BX
-	MOVW (BP), BP
-	LEAQ -2(DI)(DX*1), DX
-loop3:
-	MOVW (DI), SI
-	CMPW SI,BP
-	JZ   partial_success3
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop3
-	JMP fail
-partial_success3:
-	MOVW 1(DI), SI
-	CMPW SI,BX
-	JZ success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop3
-	JMP fail
-_4_or_more:
-	CMPQ AX, $4
-	JA   _5_or_more
-	MOVL (BP), BP
-	LEAQ -3(DI)(DX*1), DX
-loop4:
-	MOVL (DI), SI
-	CMPL SI,BP
-	JZ   success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop4
-	JMP fail
-_5_or_more:
-	CMPQ AX, $7
-	JA   _8_or_more
-	LEAQ 1(DI)(DX*1), DX
-	SUBQ AX, DX
-	MOVL -4(BP)(AX*1), BX
-	MOVL (BP), BP
-loop5to7:
-	MOVL (DI), SI
-	CMPL SI,BP
-	JZ   partial_success5to7
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop5to7
-	JMP fail
-partial_success5to7:
-	MOVL -4(AX)(DI*1), SI
-	CMPL SI,BX
-	JZ success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop5to7
-	JMP fail
-_8_or_more:
-	CMPQ AX, $8
-	JA   _9_or_more
-	MOVQ (BP), BP
-	LEAQ -7(DI)(DX*1), DX
-loop8:
-	MOVQ (DI), SI
-	CMPQ SI,BP
-	JZ   success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop8
-	JMP fail
-_9_or_more:
-	CMPQ AX, $15
-	JA   _16_or_more
-	LEAQ 1(DI)(DX*1), DX
-	SUBQ AX, DX
-	MOVQ -8(BP)(AX*1), BX
-	MOVQ (BP), BP
-loop9to15:
-	MOVQ (DI), SI
-	CMPQ SI,BP
-	JZ   partial_success9to15
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop9to15
-	JMP fail
-partial_success9to15:
-	MOVQ -8(AX)(DI*1), SI
-	CMPQ SI,BX
-	JZ success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop9to15
-	JMP fail
-_16_or_more:
-	CMPQ AX, $16
-	JA   _17_or_more
-	MOVOU (BP), X1
-	LEAQ -15(DI)(DX*1), DX
-loop16:
-	MOVOU (DI), X2
-	PCMPEQB X1, X2
-	PMOVMSKB X2, SI
-	CMPQ  SI, $0xffff
-	JE   success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop16
-	JMP fail
-_17_or_more:
-	CMPQ AX, $31
-	JA   _32_or_more
-	LEAQ 1(DI)(DX*1), DX
-	SUBQ AX, DX
-	MOVOU -16(BP)(AX*1), X0
-	MOVOU (BP), X1
-loop17to31:
-	MOVOU (DI), X2
-	PCMPEQB X1,X2
-	PMOVMSKB X2, SI
-	CMPQ  SI, $0xffff
-	JE   partial_success17to31
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop17to31
-	JMP fail
-partial_success17to31:
-	MOVOU -16(AX)(DI*1), X3
-	PCMPEQB X0, X3
-	PMOVMSKB X3, SI
-	CMPQ  SI, $0xffff
-	JE success
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop17to31
-	JMP fail
-// We can get here only when AVX2 is enabled and cutoff for indexShortStr is set to 63
-// So no need to check cpuid
-_32_or_more:
-	CMPQ AX, $32
-	JA   _33_to_63
-	VMOVDQU (BP), Y1
-	LEAQ -31(DI)(DX*1), DX
-loop32:
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPMOVMSKB Y3, SI
-	CMPL  SI, $0xffffffff
-	JE   success_avx2
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop32
-	JMP fail_avx2
-_33_to_63:
-	LEAQ 1(DI)(DX*1), DX
-	SUBQ AX, DX
-	VMOVDQU -32(BP)(AX*1), Y0
-	VMOVDQU (BP), Y1
-loop33to63:
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPMOVMSKB Y3, SI
-	CMPL  SI, $0xffffffff
-	JE   partial_success33to63
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop33to63
-	JMP fail_avx2
-partial_success33to63:
-	VMOVDQU -32(AX)(DI*1), Y3
-	VPCMPEQB Y0, Y3, Y4
-	VPMOVMSKB Y4, SI
-	CMPL  SI, $0xffffffff
-	JE success_avx2
-	ADDQ $1,DI
-	CMPQ DI,DX
-	JB loop33to63
-fail_avx2:
-	VZEROUPPER
-fail:
-	MOVQ $-1, (R11)
-	RET
-success_avx2:
-	VZEROUPPER
-	JMP success
-sse42:
-	CMPB runtime·support_sse42(SB), $1
-	JNE no_sse42
-	CMPQ AX, $12
-	// PCMPESTRI is slower than normal compare,
-	// so using it makes sense only if we advance 4+ bytes per compare
-	// This value was determined experimentally and is the ~same
-	// on Nehalem (first with SSE42) and Haswell.
-	JAE _9_or_more
-	LEAQ 16(BP), SI
-	TESTW $0xff0, SI
-	JEQ no_sse42
-	MOVOU (BP), X1
-	LEAQ -15(DI)(DX*1), SI
-	MOVQ $16, R9
-	SUBQ AX, R9 // We advance by 16-len(sep) each iteration, so precalculate it into R9
-loop_sse42:
-	// 0x0c means: unsigned byte compare (bits 0,1 are 00)
-	// for equality (bits 2,3 are 11)
-	// result is not masked or inverted (bits 4,5 are 00)
-	// and corresponds to first matching byte (bit 6 is 0)
-	PCMPESTRI $0x0c, (DI), X1
-	// CX == 16 means no match,
-	// CX > R9 means partial match at the end of the string,
-	// otherwise sep is at offset CX from X1 start
-	CMPQ CX, R9
-	JBE sse42_success
-	ADDQ R9, DI
-	CMPQ DI, SI
-	JB loop_sse42
-	PCMPESTRI $0x0c, -1(SI), X1
-	CMPQ CX, R9
-	JA fail
-	LEAQ -1(SI), DI
-sse42_success:
-	ADDQ CX, DI
-success:
-	SUBQ R10, DI
-	MOVQ DI, (R11)
-	RET
-
 TEXT runtime·return0(SB), NOSPLIT, $0
 	MOVL	$0, AX
 	RET

commit f6332bb84ad87e958290ae23b29a2b13a41ee2a2
Author: Keith Randall <khr@golang.org>
Date:   Sat Mar 3 14:24:54 2018 -0800

    internal/bytealg: move compare functions to bytealg
    
    Move bytes.Compare and runtime·cmpstring to bytealg.
    
    Update #19792
    
    Change-Id: I139e6d7c59686bef7a3017e3dec99eba5fd10447
    Reviewed-on: https://go-review.googlesource.com/98515
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 386307afa5..f91a01da72 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1358,228 +1358,6 @@ DATA shifts<>+0xf0(SB)/8, $0x0807060504030201
 DATA shifts<>+0xf8(SB)/8, $0xff0f0e0d0c0b0a09
 GLOBL shifts<>(SB),RODATA,$256
 
-TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
-	MOVQ	s1_base+0(FP), SI
-	MOVQ	s1_len+8(FP), BX
-	MOVQ	s2_base+16(FP), DI
-	MOVQ	s2_len+24(FP), DX
-	LEAQ	ret+32(FP), R9
-	JMP	runtime·cmpbody(SB)
-
-TEXT bytes·Compare(SB),NOSPLIT,$0-56
-	MOVQ	s1+0(FP), SI
-	MOVQ	s1+8(FP), BX
-	MOVQ	s2+24(FP), DI
-	MOVQ	s2+32(FP), DX
-	LEAQ	res+48(FP), R9
-	JMP	runtime·cmpbody(SB)
-
-// input:
-//   SI = a
-//   DI = b
-//   BX = alen
-//   DX = blen
-//   R9 = address of output word (stores -1/0/1 here)
-TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
-	CMPQ	SI, DI
-	JEQ	allsame
-	CMPQ	BX, DX
-	MOVQ	DX, R8
-	CMOVQLT	BX, R8 // R8 = min(alen, blen) = # of bytes to compare
-	CMPQ	R8, $8
-	JB	small
-
-	CMPQ	R8, $63
-	JBE	loop
-	CMPB    runtime·support_avx2(SB), $1
-	JEQ     big_loop_avx2
-	JMP	big_loop
-loop:
-	CMPQ	R8, $16
-	JBE	_0through16
-	MOVOU	(SI), X0
-	MOVOU	(DI), X1
-	PCMPEQB X0, X1
-	PMOVMSKB X1, AX
-	XORQ	$0xffff, AX	// convert EQ to NE
-	JNE	diff16	// branch if at least one byte is not equal
-	ADDQ	$16, SI
-	ADDQ	$16, DI
-	SUBQ	$16, R8
-	JMP	loop
-	
-diff64:
-	ADDQ	$48, SI
-	ADDQ	$48, DI
-	JMP	diff16
-diff48:
-	ADDQ	$32, SI
-	ADDQ	$32, DI
-	JMP	diff16
-diff32:
-	ADDQ	$16, SI
-	ADDQ	$16, DI
-	// AX = bit mask of differences
-diff16:
-	BSFQ	AX, BX	// index of first byte that differs
-	XORQ	AX, AX
-	MOVB	(SI)(BX*1), CX
-	CMPB	CX, (DI)(BX*1)
-	SETHI	AX
-	LEAQ	-1(AX*2), AX	// convert 1/0 to +1/-1
-	MOVQ	AX, (R9)
-	RET
-
-	// 0 through 16 bytes left, alen>=8, blen>=8
-_0through16:
-	CMPQ	R8, $8
-	JBE	_0through8
-	MOVQ	(SI), AX
-	MOVQ	(DI), CX
-	CMPQ	AX, CX
-	JNE	diff8
-_0through8:
-	MOVQ	-8(SI)(R8*1), AX
-	MOVQ	-8(DI)(R8*1), CX
-	CMPQ	AX, CX
-	JEQ	allsame
-
-	// AX and CX contain parts of a and b that differ.
-diff8:
-	BSWAPQ	AX	// reverse order of bytes
-	BSWAPQ	CX
-	XORQ	AX, CX
-	BSRQ	CX, CX	// index of highest bit difference
-	SHRQ	CX, AX	// move a's bit to bottom
-	ANDQ	$1, AX	// mask bit
-	LEAQ	-1(AX*2), AX // 1/0 => +1/-1
-	MOVQ	AX, (R9)
-	RET
-
-	// 0-7 bytes in common
-small:
-	LEAQ	(R8*8), CX	// bytes left -> bits left
-	NEGQ	CX		//  - bits lift (== 64 - bits left mod 64)
-	JEQ	allsame
-
-	// load bytes of a into high bytes of AX
-	CMPB	SI, $0xf8
-	JA	si_high
-	MOVQ	(SI), SI
-	JMP	si_finish
-si_high:
-	MOVQ	-8(SI)(R8*1), SI
-	SHRQ	CX, SI
-si_finish:
-	SHLQ	CX, SI
-
-	// load bytes of b in to high bytes of BX
-	CMPB	DI, $0xf8
-	JA	di_high
-	MOVQ	(DI), DI
-	JMP	di_finish
-di_high:
-	MOVQ	-8(DI)(R8*1), DI
-	SHRQ	CX, DI
-di_finish:
-	SHLQ	CX, DI
-
-	BSWAPQ	SI	// reverse order of bytes
-	BSWAPQ	DI
-	XORQ	SI, DI	// find bit differences
-	JEQ	allsame
-	BSRQ	DI, CX	// index of highest bit difference
-	SHRQ	CX, SI	// move a's bit to bottom
-	ANDQ	$1, SI	// mask bit
-	LEAQ	-1(SI*2), AX // 1/0 => +1/-1
-	MOVQ	AX, (R9)
-	RET
-
-allsame:
-	XORQ	AX, AX
-	XORQ	CX, CX
-	CMPQ	BX, DX
-	SETGT	AX	// 1 if alen > blen
-	SETEQ	CX	// 1 if alen == blen
-	LEAQ	-1(CX)(AX*2), AX	// 1,0,-1 result
-	MOVQ	AX, (R9)
-	RET
-
-	// this works for >= 64 bytes of data.
-big_loop:
-	MOVOU	(SI), X0
-	MOVOU	(DI), X1
-	PCMPEQB X0, X1
-	PMOVMSKB X1, AX
-	XORQ	$0xffff, AX
-	JNE	diff16
-
-	MOVOU	16(SI), X0
-	MOVOU	16(DI), X1
-	PCMPEQB X0, X1
-	PMOVMSKB X1, AX
-	XORQ	$0xffff, AX
-	JNE	diff32
-
-	MOVOU	32(SI), X0
-	MOVOU	32(DI), X1
-	PCMPEQB X0, X1
-	PMOVMSKB X1, AX
-	XORQ	$0xffff, AX
-	JNE	diff48
-
-	MOVOU	48(SI), X0
-	MOVOU	48(DI), X1
-	PCMPEQB X0, X1
-	PMOVMSKB X1, AX
-	XORQ	$0xffff, AX
-	JNE	diff64
-
-	ADDQ	$64, SI
-	ADDQ	$64, DI
-	SUBQ	$64, R8
-	CMPQ	R8, $64
-	JBE	loop
-	JMP	big_loop
-
-	// Compare 64-bytes per loop iteration.
-	// Loop is unrolled and uses AVX2.
-big_loop_avx2:
-	VMOVDQU	(SI), Y2
-	VMOVDQU	(DI), Y3
-	VMOVDQU	32(SI), Y4
-	VMOVDQU	32(DI), Y5
-	VPCMPEQB Y2, Y3, Y0
-	VPMOVMSKB Y0, AX
-	XORL	$0xffffffff, AX
-	JNE	diff32_avx2
-	VPCMPEQB Y4, Y5, Y6
-	VPMOVMSKB Y6, AX
-	XORL	$0xffffffff, AX
-	JNE	diff64_avx2
-
-	ADDQ	$64, SI
-	ADDQ	$64, DI
-	SUBQ	$64, R8
-	CMPQ	R8, $64
-	JB	big_loop_avx2_exit
-	JMP	big_loop_avx2
-
-	// Avoid AVX->SSE transition penalty and search first 32 bytes of 64 byte chunk.
-diff32_avx2:
-	VZEROUPPER
-	JMP diff16
-
-	// Same as diff32_avx2, but for last 32 bytes.
-diff64_avx2:
-	VZEROUPPER
-	JMP diff48
-
-	// For <64 bytes remainder jump to normal loop.
-big_loop_avx2_exit:
-	VZEROUPPER
-	JMP loop
-
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
 	// We want len in DX and AX, because PCMPESTRI implicitly consumes them

commit 45964e4f9c950863adcaeb62fbe49f3fa913f27d
Author: Keith Randall <khr@golang.org>
Date:   Sat Mar 3 10:28:58 2018 -0800

    internal/bytealg: move Count to bytealg
    
    Move bytes.Count and strings.Count to bytealg.
    
    Update #19792
    
    Change-Id: I3e4e14b504a0b71758885bb131e5656e342cf8cb
    Reviewed-on: https://go-review.googlesource.com/98495
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 5835443ff6..386307afa5 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1848,195 +1848,6 @@ success:
 	MOVQ DI, (R11)
 	RET
 
-TEXT bytes·countByte(SB),NOSPLIT,$0-40
-	MOVQ s+0(FP), SI
-	MOVQ s_len+8(FP), BX
-	MOVB c+24(FP), AL
-	LEAQ ret+32(FP), R8
-	JMP  runtime·countByte(SB)
-
-TEXT strings·countByte(SB),NOSPLIT,$0-32
-	MOVQ s+0(FP), SI
-	MOVQ s_len+8(FP), BX
-	MOVB c+16(FP), AL
-	LEAQ ret+24(FP), R8
-	JMP  runtime·countByte(SB)
-
-// input:
-//   SI: data
-//   BX: data len
-//   AL: byte sought
-//   R8: address to put result
-// This requires the POPCNT instruction
-TEXT runtime·countByte(SB),NOSPLIT,$0
-	// Shuffle X0 around so that each byte contains
-	// the character we're looking for.
-	MOVD AX, X0
-	PUNPCKLBW X0, X0
-	PUNPCKLBW X0, X0
-	PSHUFL $0, X0, X0
-
-	CMPQ BX, $16
-	JLT small
-
-	MOVQ $0, R12 // Accumulator
-
-	MOVQ SI, DI
-
-	CMPQ BX, $32
-	JA avx2
-sse:
-	LEAQ	-16(SI)(BX*1), AX	// AX = address of last 16 bytes
-	JMP	sseloopentry
-
-sseloop:
-	// Move the next 16-byte chunk of the data into X1.
-	MOVOU	(DI), X1
-	// Compare bytes in X0 to X1.
-	PCMPEQB	X0, X1
-	// Take the top bit of each byte in X1 and put the result in DX.
-	PMOVMSKB X1, DX
-	// Count number of matching bytes
-	POPCNTL DX, DX
-	// Accumulate into R12
-	ADDQ DX, R12
-	// Advance to next block.
-	ADDQ	$16, DI
-sseloopentry:
-	CMPQ	DI, AX
-	JBE	sseloop
-
-	// Get the number of bytes to consider in the last 16 bytes
-	ANDQ $15, BX
-	JZ end
-
-	// Create mask to ignore overlap between previous 16 byte block
-	// and the next.
-	MOVQ $16,CX
-	SUBQ BX, CX
-	MOVQ $0xFFFF, R10
-	SARQ CL, R10
-	SALQ CL, R10
-
-	// Process the last 16-byte chunk. This chunk may overlap with the
-	// chunks we've already searched so we need to mask part of it.
-	MOVOU	(AX), X1
-	PCMPEQB	X0, X1
-	PMOVMSKB X1, DX
-	// Apply mask
-	ANDQ R10, DX
-	POPCNTL DX, DX
-	ADDQ DX, R12
-end:
-	MOVQ R12, (R8)
-	RET
-
-// handle for lengths < 16
-small:
-	TESTQ	BX, BX
-	JEQ	endzero
-
-	// Check if we'll load across a page boundary.
-	LEAQ	16(SI), AX
-	TESTW	$0xff0, AX
-	JEQ	endofpage
-
-	// We must ignore high bytes as they aren't part of our slice.
-	// Create mask.
-	MOVB BX, CX
-	MOVQ $1, R10
-	SALQ CL, R10
-	SUBQ $1, R10
-
-	// Load data
-	MOVOU	(SI), X1
-	// Compare target byte with each byte in data.
-	PCMPEQB	X0, X1
-	// Move result bits to integer register.
-	PMOVMSKB X1, DX
-	// Apply mask
-	ANDQ R10, DX
-	POPCNTL DX, DX
-	// Directly return DX, we don't need to accumulate
-	// since we have <16 bytes.
-	MOVQ	DX, (R8)
-	RET
-endzero:
-	MOVQ $0, (R8)
-	RET
-
-endofpage:
-	// We must ignore low bytes as they aren't part of our slice.
-	MOVQ $16,CX
-	SUBQ BX, CX
-	MOVQ $0xFFFF, R10
-	SARQ CL, R10
-	SALQ CL, R10
-
-	// Load data into the high end of X1.
-	MOVOU	-16(SI)(BX*1), X1
-	// Compare target byte with each byte in data.
-	PCMPEQB	X0, X1
-	// Move result bits to integer register.
-	PMOVMSKB X1, DX
-	// Apply mask
-	ANDQ R10, DX
-	// Directly return DX, we don't need to accumulate
-	// since we have <16 bytes.
-	POPCNTL DX, DX
-	MOVQ	DX, (R8)
-	RET
-
-avx2:
-	CMPB   runtime·support_avx2(SB), $1
-	JNE sse
-	MOVD AX, X0
-	LEAQ -32(SI)(BX*1), R11
-	VPBROADCASTB  X0, Y1
-avx2_loop:
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPMOVMSKB Y3, DX
-	POPCNTL DX, DX
-	ADDQ DX, R12
-	ADDQ $32, DI
-	CMPQ DI, R11
-	JLE avx2_loop
-
-	// If last block is already processed,
-	// skip to the end.
-	CMPQ DI, R11
-	JEQ endavx
-
-	// Load address of the last 32 bytes.
-	// There is an overlap with the previous block.
-	MOVQ R11, DI
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPMOVMSKB Y3, DX
-	// Exit AVX mode.
-	VZEROUPPER
-
-	// Create mask to ignore overlap between previous 32 byte block
-	// and the next.
-	ANDQ $31, BX
-	MOVQ $32,CX
-	SUBQ BX, CX
-	MOVQ $0xFFFFFFFF, R10
-	SARQ CL, R10
-	SALQ CL, R10
-	// Apply mask
-	ANDQ R10, DX
-	POPCNTL DX, DX
-	ADDQ DX, R12
-	MOVQ R12, (R8)
-	RET
-endavx:
-	// Exit AVX mode.
-	VZEROUPPER
-	MOVQ R12, (R8)
-	RET
-
 TEXT runtime·return0(SB), NOSPLIT, $0
 	MOVL	$0, AX
 	RET

commit 1dfa380e3dd9c46aa945205d9e142bf503c1e198
Author: Keith Randall <khr@golang.org>
Date:   Fri Mar 2 16:44:27 2018 -0800

    internal/bytealg: move equal functions to bytealg
    
    Move bytes.Equal, runtime.memequal, and runtime.memequal_varlen
    to the bytealg package.
    
    Update #19792
    
    Change-Id: Ic4175e952936016ea0bda6c7c3dbb33afdc8e4ac
    Reviewed-on: https://go-review.googlesource.com/98355
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 07e3b0b6e9..5835443ff6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1358,153 +1358,6 @@ DATA shifts<>+0xf0(SB)/8, $0x0807060504030201
 DATA shifts<>+0xf8(SB)/8, $0xff0f0e0d0c0b0a09
 GLOBL shifts<>(SB),RODATA,$256
 
-// memequal(p, q unsafe.Pointer, size uintptr) bool
-TEXT runtime·memequal(SB),NOSPLIT,$0-25
-	MOVQ	a+0(FP), SI
-	MOVQ	b+8(FP), DI
-	CMPQ	SI, DI
-	JEQ	eq
-	MOVQ	size+16(FP), BX
-	LEAQ	ret+24(FP), AX
-	JMP	runtime·memeqbody(SB)
-eq:
-	MOVB	$1, ret+24(FP)
-	RET
-
-// memequal_varlen(a, b unsafe.Pointer) bool
-TEXT runtime·memequal_varlen(SB),NOSPLIT,$0-17
-	MOVQ	a+0(FP), SI
-	MOVQ	b+8(FP), DI
-	CMPQ	SI, DI
-	JEQ	eq
-	MOVQ	8(DX), BX    // compiler stores size at offset 8 in the closure
-	LEAQ	ret+16(FP), AX
-	JMP	runtime·memeqbody(SB)
-eq:
-	MOVB	$1, ret+16(FP)
-	RET
-
-// a in SI
-// b in DI
-// count in BX
-// address of result byte in AX
-TEXT runtime·memeqbody(SB),NOSPLIT,$0-0
-	CMPQ	BX, $8
-	JB	small
-	CMPQ	BX, $64
-	JB	bigloop
-	CMPB    runtime·support_avx2(SB), $1
-	JE	hugeloop_avx2
-	
-	// 64 bytes at a time using xmm registers
-hugeloop:
-	CMPQ	BX, $64
-	JB	bigloop
-	MOVOU	(SI), X0
-	MOVOU	(DI), X1
-	MOVOU	16(SI), X2
-	MOVOU	16(DI), X3
-	MOVOU	32(SI), X4
-	MOVOU	32(DI), X5
-	MOVOU	48(SI), X6
-	MOVOU	48(DI), X7
-	PCMPEQB	X1, X0
-	PCMPEQB	X3, X2
-	PCMPEQB	X5, X4
-	PCMPEQB	X7, X6
-	PAND	X2, X0
-	PAND	X6, X4
-	PAND	X4, X0
-	PMOVMSKB X0, DX
-	ADDQ	$64, SI
-	ADDQ	$64, DI
-	SUBQ	$64, BX
-	CMPL	DX, $0xffff
-	JEQ	hugeloop
-	MOVB	$0, (AX)
-	RET
-
-	// 64 bytes at a time using ymm registers
-hugeloop_avx2:
-	CMPQ	BX, $64
-	JB	bigloop_avx2
-	VMOVDQU	(SI), Y0
-	VMOVDQU	(DI), Y1
-	VMOVDQU	32(SI), Y2
-	VMOVDQU	32(DI), Y3
-	VPCMPEQB	Y1, Y0, Y4
-	VPCMPEQB	Y2, Y3, Y5
-	VPAND	Y4, Y5, Y6
-	VPMOVMSKB Y6, DX
-	ADDQ	$64, SI
-	ADDQ	$64, DI
-	SUBQ	$64, BX
-	CMPL	DX, $0xffffffff
-	JEQ	hugeloop_avx2
-	VZEROUPPER
-	MOVB	$0, (AX)
-	RET
-
-bigloop_avx2:
-	VZEROUPPER
-
-	// 8 bytes at a time using 64-bit register
-bigloop:
-	CMPQ	BX, $8
-	JBE	leftover
-	MOVQ	(SI), CX
-	MOVQ	(DI), DX
-	ADDQ	$8, SI
-	ADDQ	$8, DI
-	SUBQ	$8, BX
-	CMPQ	CX, DX
-	JEQ	bigloop
-	MOVB	$0, (AX)
-	RET
-
-	// remaining 0-8 bytes
-leftover:
-	MOVQ	-8(SI)(BX*1), CX
-	MOVQ	-8(DI)(BX*1), DX
-	CMPQ	CX, DX
-	SETEQ	(AX)
-	RET
-
-small:
-	CMPQ	BX, $0
-	JEQ	equal
-
-	LEAQ	0(BX*8), CX
-	NEGQ	CX
-
-	CMPB	SI, $0xf8
-	JA	si_high
-
-	// load at SI won't cross a page boundary.
-	MOVQ	(SI), SI
-	JMP	si_finish
-si_high:
-	// address ends in 11111xxx. Load up to bytes we want, move to correct position.
-	MOVQ	-8(SI)(BX*1), SI
-	SHRQ	CX, SI
-si_finish:
-
-	// same for DI.
-	CMPB	DI, $0xf8
-	JA	di_high
-	MOVQ	(DI), DI
-	JMP	di_finish
-di_high:
-	MOVQ	-8(DI)(BX*1), DI
-	SHRQ	CX, DI
-di_finish:
-
-	SUBQ	SI, DI
-	SHLQ	CX, DI
-equal:
-	SETEQ	(AX)
-	RET
-
 TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
 	MOVQ	s1_base+0(FP), SI
 	MOVQ	s1_len+8(FP), BX
@@ -1995,20 +1848,6 @@ success:
 	MOVQ DI, (R11)
 	RET
 
-TEXT bytes·Equal(SB),NOSPLIT,$0-49
-	MOVQ	a_len+8(FP), BX
-	MOVQ	b_len+32(FP), CX
-	CMPQ	BX, CX
-	JNE	eqret
-	MOVQ	a+0(FP), SI
-	MOVQ	b+24(FP), DI
-	LEAQ	ret+48(FP), AX
-	JMP	runtime·memeqbody(SB)
-eqret:
-	MOVB	$0, ret+48(FP)
-	RET
-
-
 TEXT bytes·countByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX

commit 403ab0f2214f583db84a2dae275389be92072a35
Author: Keith Randall <khr@google.com>
Date:   Thu Mar 1 16:38:41 2018 -0800

    internal/bytealg: move IndexByte asssembly to the new bytealg package
    
    Move the IndexByte function from the runtime to a new bytealg package.
    The new package will eventually hold all the optimized assembly for
    groveling through byte slices and strings. It seems a better home for
    this code than randomly keeping it in runtime.
    
    Once this is in, the next step is to move the other functions
    (Compare, Equal, ...).
    
    Update #19792
    
    This change seems complicated enough that we might just declare
    "not worth it" and abandon.  Opinions welcome.
    
    The core assembly is all unchanged, except minor modifications where
    the code reads cpu feature bits.
    
    The wrapper functions have been cleaned up as they are now actually
    checked by vet.
    
    Change-Id: I9fa75bee5d85db3a65b3fd3b7997e60367523796
    Reviewed-on: https://go-review.googlesource.com/98016
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2376fe0aae..07e3b0b6e9 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1995,148 +1995,6 @@ success:
 	MOVQ DI, (R11)
 	RET
 
-
-TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
-	MOVQ s+0(FP), SI
-	MOVQ s_len+8(FP), BX
-	MOVB c+24(FP), AL
-	LEAQ ret+32(FP), R8
-	JMP  runtime·indexbytebody(SB)
-
-TEXT strings·IndexByte(SB),NOSPLIT,$0-32
-	MOVQ s+0(FP), SI
-	MOVQ s_len+8(FP), BX
-	MOVB c+16(FP), AL
-	LEAQ ret+24(FP), R8
-	JMP  runtime·indexbytebody(SB)
-
-// input:
-//   SI: data
-//   BX: data len
-//   AL: byte sought
-//   R8: address to put result
-TEXT runtime·indexbytebody(SB),NOSPLIT,$0
-	// Shuffle X0 around so that each byte contains
-	// the character we're looking for.
-	MOVD AX, X0
-	PUNPCKLBW X0, X0
-	PUNPCKLBW X0, X0
-	PSHUFL $0, X0, X0
-	
-	CMPQ BX, $16
-	JLT small
-
-	MOVQ SI, DI
-
-	CMPQ BX, $32
-	JA avx2
-sse:
-	LEAQ	-16(SI)(BX*1), AX	// AX = address of last 16 bytes
-	JMP	sseloopentry
-	
-sseloop:
-	// Move the next 16-byte chunk of the data into X1.
-	MOVOU	(DI), X1
-	// Compare bytes in X0 to X1.
-	PCMPEQB	X0, X1
-	// Take the top bit of each byte in X1 and put the result in DX.
-	PMOVMSKB X1, DX
-	// Find first set bit, if any.
-	BSFL	DX, DX
-	JNZ	ssesuccess
-	// Advance to next block.
-	ADDQ	$16, DI
-sseloopentry:
-	CMPQ	DI, AX
-	JB	sseloop
-
-	// Search the last 16-byte chunk. This chunk may overlap with the
-	// chunks we've already searched, but that's ok.
-	MOVQ	AX, DI
-	MOVOU	(AX), X1
-	PCMPEQB	X0, X1
-	PMOVMSKB X1, DX
-	BSFL	DX, DX
-	JNZ	ssesuccess
-
-failure:
-	MOVQ $-1, (R8)
-	RET
-
-// We've found a chunk containing the byte.
-// The chunk was loaded from DI.
-// The index of the matching byte in the chunk is DX.
-// The start of the data is SI.
-ssesuccess:
-	SUBQ SI, DI	// Compute offset of chunk within data.
-	ADDQ DX, DI	// Add offset of byte within chunk.
-	MOVQ DI, (R8)
-	RET
-
-// handle for lengths < 16
-small:
-	TESTQ	BX, BX
-	JEQ	failure
-
-	// Check if we'll load across a page boundary.
-	LEAQ	16(SI), AX
-	TESTW	$0xff0, AX
-	JEQ	endofpage
-
-	MOVOU	(SI), X1 // Load data
-	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
-	PMOVMSKB X1, DX	// Move result bits to integer register.
-	BSFL	DX, DX	// Find first set bit.
-	JZ	failure	// No set bit, failure.
-	CMPL	DX, BX
-	JAE	failure	// Match is past end of data.
-	MOVQ	DX, (R8)
-	RET
-
-endofpage:
-	MOVOU	-16(SI)(BX*1), X1	// Load data into the high end of X1.
-	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
-	PMOVMSKB X1, DX	// Move result bits to integer register.
-	MOVL	BX, CX
-	SHLL	CX, DX
-	SHRL	$16, DX	// Shift desired bits down to bottom of register.
-	BSFL	DX, DX	// Find first set bit.
-	JZ	failure	// No set bit, failure.
-	MOVQ	DX, (R8)
-	RET
-
-avx2:
-	CMPB   runtime·support_avx2(SB), $1
-	JNE sse
-	MOVD AX, X0
-	LEAQ -32(SI)(BX*1), R11
-	VPBROADCASTB  X0, Y1
-avx2_loop:
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPTEST Y3, Y3
-	JNZ avx2success
-	ADDQ $32, DI
-	CMPQ DI, R11
-	JLT avx2_loop
-	MOVQ R11, DI
-	VMOVDQU (DI), Y2
-	VPCMPEQB Y1, Y2, Y3
-	VPTEST Y3, Y3
-	JNZ avx2success
-	VZEROUPPER
-	MOVQ $-1, (R8)
-	RET
-
-avx2success:
-	VPMOVMSKB Y3, DX
-	BSFL DX, DX
-	SUBQ SI, DI
-	ADDQ DI, DX
-	MOVQ DX, (R8)
-	VZEROUPPER
-	RET
-
 TEXT bytes·Equal(SB),NOSPLIT,$0-49
 	MOVQ	a_len+8(FP), BX
 	MOVQ	b_len+32(FP), CX

commit c5d6c42d35059bb911cf9f4a77704438ab4f9de0
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Sat Feb 24 10:50:38 2018 -0800

    runtime: improve 386/amd64 systemstack
    
    Minor improvements, noticed while investigating other things.
    
    Shorten the prologue.
    
    Make branch direction better for static branch prediction;
    the most common case by far is switching stacks (g==curg).
    
    Change-Id: Ib2211d3efecb60446355cda56194221ccb78057d
    Reviewed-on: https://go-review.googlesource.com/97377
    Run-TryBot: Josh Bleecher Snyder <josharian@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 82b7832ae3..2376fe0aae 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -369,23 +369,17 @@ TEXT runtime·systemstack(SB), NOSPLIT, $0-8
 	MOVQ	g(CX), AX	// AX = g
 	MOVQ	g_m(AX), BX	// BX = m
 
-	MOVQ	m_gsignal(BX), DX	// DX = gsignal
-	CMPQ	AX, DX
+	CMPQ	AX, m_gsignal(BX)
 	JEQ	noswitch
 
 	MOVQ	m_g0(BX), DX	// DX = g0
 	CMPQ	AX, DX
 	JEQ	noswitch
 
-	MOVQ	m_curg(BX), R8
-	CMPQ	AX, R8
-	JEQ	switch
-	
-	// Bad: g is not gsignal, not g0, not curg. What is it?
-	MOVQ	$runtime·badsystemstack(SB), AX
-	CALL	AX
+	CMPQ	AX, m_curg(BX)
+	JNE	bad
 
-switch:
+	// switch stacks
 	// save our state in g->sched. Pretend to
 	// be systemstack_switch if the G stack is scanned.
 	MOVQ	$runtime·systemstack_switch(SB), SI
@@ -426,6 +420,12 @@ noswitch:
 	MOVQ	0(DI), DI
 	JMP	DI
 
+bad:
+	// Bad: g is not gsignal, not g0, not curg. What is it?
+	MOVQ	$runtime·badsystemstack(SB), AX
+	CALL	AX
+
+
 /*
  * support for morestack
  */

commit 20101894078199a3a9014ca99ec4e2a0a16a0869
Author: Austin Clements <austin@google.com>
Date:   Mon Jan 15 12:27:17 2018 -0500

    runtime: remove legacy eager write barrier
    
    Now that the buffered write barrier is implemented for all
    architectures, we can remove the old eager write barrier
    implementation. This CL removes the implementation from the runtime,
    support in the compiler for calling it, and updates some compiler
    tests that relied on the old eager barrier support. It also makes sure
    that all of the useful comments from the old write barrier
    implementation still have a place to live.
    
    Fixes #22460.
    
    Updates #21640 since this fixes the layering concerns of the write
    barrier (but not the other things in that issue).
    
    Change-Id: I580f93c152e89607e0a72fe43370237ba97bae74
    Reviewed-on: https://go-review.googlesource.com/92705
    Run-TryBot: Austin Clements <austin@google.com>
    Reviewed-by: Rick Hudson <rlh@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 576a61ca6c..82b7832ae3 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2397,7 +2397,13 @@ TEXT runtime·gcWriteBarrier(SB),NOSPLIT,$120
 	CMPQ	R14, (p_wbBuf+wbBuf_end)(R13)
 	// Record the write.
 	MOVQ	AX, -16(R14)	// Record value
-	MOVQ	(DI), R13	// TODO: This turns bad writes into bad reads.
+	// Note: This turns bad pointer writes into bad
+	// pointer reads, which could be confusing. We could avoid
+	// reading from obviously bad pointers, which would
+	// take care of the vast majority of these. We could
+	// patch this up in the signal handler, or use XCHG to
+	// combine the read and the write.
+	MOVQ	(DI), R13
 	MOVQ	R13, -8(R14)	// Record *slot
 	// Is the buffer full? (flags set in CMPQ above)
 	JEQ	flush

commit bf9ad7080d0a22acf502a60d8bc6ebbc4f5340ef
Author: Austin Clements <austin@google.com>
Date:   Wed Nov 15 14:43:05 2017 -0800

    runtime: remove another TODO
    
    I experimented with having the compiler spill the two registers that
    are clobbered by the write barrier fast path, but it slightly slows
    down compilebench, which is a good write barrier benchmark:
    
    name       old time/op     new time/op     delta
    Template       175ms ± 0%      176ms ± 1%    ~           (p=0.393 n=10+10)
    Unicode       83.6ms ± 1%     85.1ms ± 2%  +1.79%         (p=0.000 n=9+10)
    GoTypes        585ms ± 0%      588ms ± 1%    ~            (p=0.173 n=8+10)
    Compiler       2.78s ± 1%      2.81s ± 2%  +0.81%        (p=0.023 n=10+10)
    SSA            7.11s ± 1%      7.15s ± 1%  +0.59%        (p=0.029 n=10+10)
    Flate          115ms ± 1%      116ms ± 2%    ~           (p=0.853 n=10+10)
    GoParser       144ms ± 2%      145ms ± 2%    ~           (p=1.000 n=10+10)
    Reflect        389ms ± 1%      390ms ± 1%    ~           (p=0.481 n=10+10)
    Tar            185ms ± 2%      185ms ± 2%    ~           (p=0.529 n=10+10)
    XML            205ms ± 0%      207ms ± 2%    ~            (p=0.065 n=9+10)
    
    Since this didn't pan out, remove the TODO.
    
    Change-Id: I2186942c6d1ba10585a5da03cd7c1d26ce906273
    Reviewed-on: https://go-review.googlesource.com/78034
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7e13458b0b..576a61ca6c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2380,11 +2380,8 @@ TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
 // It clobbers FLAGS. It does not clobber any general-purpose registers,
 // but may clobber others (e.g., SSE registers).
 TEXT runtime·gcWriteBarrier(SB),NOSPLIT,$120
-	// Save the registers clobbered by the fast path.
-	//
-	// TODO: Teach the register allocator that this clobbers some registers
-	// so we don't always have to save them? Use regs it's least likely to
-	// care about.
+	// Save the registers clobbered by the fast path. This is slightly
+	// faster than having the caller spill these.
 	MOVQ	R14, 104(SP)
 	MOVQ	R13, 112(SP)
 	// TODO: Consider passing g.m.p in as an argument so they can be shared

commit 366f46fe0084db6740b9db87dd36a52caf289025
Author: Austin Clements <austin@google.com>
Date:   Wed Nov 15 14:26:33 2017 -0800

    runtime: remove TODO
    
    I experimented with changing the write barrier to take the value in SI
    rather than AX to improve register allocation. It had no effect on
    performance and only made the "hello world" text 0.07% smaller, so
    let's just remove the comment.
    
    Change-Id: I6a261d14139b7a02a8467b31e74951dfb927ffb4
    Reviewed-on: https://go-review.googlesource.com/78033
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ea48a8e3c0..7e13458b0b 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2379,8 +2379,6 @@ TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
 // - AX is the value being written at DI
 // It clobbers FLAGS. It does not clobber any general-purpose registers,
 // but may clobber others (e.g., SSE registers).
-//
-// TODO: AX may be a bad choice because regalloc likes to use it.
 TEXT runtime·gcWriteBarrier(SB),NOSPLIT,$120
 	// Save the registers clobbered by the fast path.
 	//

commit e9079a69f34365e99a4787f2e2e463cab8429a66
Author: Austin Clements <austin@google.com>
Date:   Thu Oct 26 12:21:16 2017 -0400

    runtime: buffered write barrier implementation
    
    This implements runtime support for buffered write barriers on amd64.
    The buffered write barrier has a fast path that simply enqueues
    pointers in a per-P buffer. Unlike the current write barrier, this
    fast path is *not* a normal Go call and does not require the compiler
    to spill general-purpose registers or put arguments on the stack. When
    the buffer fills up, the write barrier takes the slow path, which
    spills all general purpose registers and flushes the buffer. We don't
    allow safe-points or stack splits while this frame is active, so it
    doesn't matter that we have no type information for the spilled
    registers in this frame.
    
    One minor complication is cgocheck=2 mode, which uses the write
    barrier to detect Go pointers being written to non-Go memory. We
    obviously can't buffer this, so instead we set the buffer to its
    minimum size, forcing the write barrier into the slow path on every
    call. For this specific case, we pass additional information as
    arguments to the flush function. This also requires enabling the cgo
    write barrier slightly later during runtime initialization, after Ps
    (and the per-P write barrier buffers) have been initialized.
    
    The code in this CL is not yet active. The next CL will modify the
    compiler to generate calls to the new write barrier.
    
    This reduces the average cost of the write barrier by roughly a factor
    of 4, which will pay for the cost of having it enabled more of the
    time after we make the GC pacer less aggressive. (Benchmarks will be
    in the next CL.)
    
    Updates #14951.
    Updates #22460.
    
    Change-Id: I396b5b0e2c5e5c4acfd761a3235fd15abadc6cb1
    Reviewed-on: https://go-review.googlesource.com/73711
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 01a1710046..ea48a8e3c0 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2371,3 +2371,92 @@ TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
 	MOVQ	DI, runtime·lastmoduledatap(SB)
 	POPQ	R15
 	RET
+
+// gcWriteBarrier performs a heap pointer write and informs the GC.
+//
+// gcWriteBarrier does NOT follow the Go ABI. It takes two arguments:
+// - DI is the destination of the write
+// - AX is the value being written at DI
+// It clobbers FLAGS. It does not clobber any general-purpose registers,
+// but may clobber others (e.g., SSE registers).
+//
+// TODO: AX may be a bad choice because regalloc likes to use it.
+TEXT runtime·gcWriteBarrier(SB),NOSPLIT,$120
+	// Save the registers clobbered by the fast path.
+	//
+	// TODO: Teach the register allocator that this clobbers some registers
+	// so we don't always have to save them? Use regs it's least likely to
+	// care about.
+	MOVQ	R14, 104(SP)
+	MOVQ	R13, 112(SP)
+	// TODO: Consider passing g.m.p in as an argument so they can be shared
+	// across a sequence of write barriers.
+	get_tls(R13)
+	MOVQ	g(R13), R13
+	MOVQ	g_m(R13), R13
+	MOVQ	m_p(R13), R13
+	MOVQ	(p_wbBuf+wbBuf_next)(R13), R14
+	// Increment wbBuf.next position.
+	LEAQ	16(R14), R14
+	MOVQ	R14, (p_wbBuf+wbBuf_next)(R13)
+	CMPQ	R14, (p_wbBuf+wbBuf_end)(R13)
+	// Record the write.
+	MOVQ	AX, -16(R14)	// Record value
+	MOVQ	(DI), R13	// TODO: This turns bad writes into bad reads.
+	MOVQ	R13, -8(R14)	// Record *slot
+	// Is the buffer full? (flags set in CMPQ above)
+	JEQ	flush
+ret:
+	MOVQ	104(SP), R14
+	MOVQ	112(SP), R13
+	// Do the write.
+	MOVQ	AX, (DI)
+	RET
+
+flush:
+	// Save all general purpose registers since these could be
+	// clobbered by wbBufFlush and were not saved by the caller.
+	// It is possible for wbBufFlush to clobber other registers
+	// (e.g., SSE registers), but the compiler takes care of saving
+	// those in the caller if necessary. This strikes a balance
+	// with registers that are likely to be used.
+	//
+	// We don't have type information for these, but all code under
+	// here is NOSPLIT, so nothing will observe these.
+	//
+	// TODO: We could strike a different balance; e.g., saving X0
+	// and not saving GP registers that are less likely to be used.
+	MOVQ	DI, 0(SP)	// Also first argument to wbBufFlush
+	MOVQ	AX, 8(SP)	// Also second argument to wbBufFlush
+	MOVQ	BX, 16(SP)
+	MOVQ	CX, 24(SP)
+	MOVQ	DX, 32(SP)
+	// DI already saved
+	MOVQ	SI, 40(SP)
+	MOVQ	BP, 48(SP)
+	MOVQ	R8, 56(SP)
+	MOVQ	R9, 64(SP)
+	MOVQ	R10, 72(SP)
+	MOVQ	R11, 80(SP)
+	MOVQ	R12, 88(SP)
+	// R13 already saved
+	// R14 already saved
+	MOVQ	R15, 96(SP)
+
+	// This takes arguments DI and AX
+	CALL	runtime·wbBufFlush(SB)
+
+	MOVQ	0(SP), DI
+	MOVQ	8(SP), AX
+	MOVQ	16(SP), BX
+	MOVQ	24(SP), CX
+	MOVQ	32(SP), DX
+	MOVQ	40(SP), SI
+	MOVQ	48(SP), BP
+	MOVQ	56(SP), R8
+	MOVQ	64(SP), R9
+	MOVQ	72(SP), R10
+	MOVQ	80(SP), R11
+	MOVQ	88(SP), R12
+	MOVQ	96(SP), R15
+	JMP	ret

commit 15d6ab69fbd8c84cde109def59c7e002296c19e8
Author: Austin Clements <austin@google.com>
Date:   Fri Oct 27 15:20:21 2017 -0400

    runtime: make systemstack tail call if already switched
    
    Currently systemstack always calls its argument, even if we're already
    on the system stack. Unfortunately, traceback with _TraceJump stops at
    the first systemstack it sees, which often cuts off runtime stacks
    early in profiles.
    
    Fix this by performing a tail call if we're already on the system
    stack. This eliminates it from the traceback entirely, so it won't
    stop prematurely (or all get mushed into a single node in the profile
    graph).
    
    Change-Id: Ibc69e8765e899f8d3806078517b8c7314da196f4
    Reviewed-on: https://go-review.googlesource.com/74050
    Reviewed-by: Cherry Zhang <cherryyz@google.com>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2ac879c31d..01a1710046 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -419,11 +419,12 @@ switch:
 	RET
 
 noswitch:
-	// already on m stack, just call directly
+	// already on m stack; tail call the function
+	// Using a tail call here cleans up tracebacks since we won't stop
+	// at an intermediate systemstack.
 	MOVQ	DI, DX
 	MOVQ	0(DI), DI
-	CALL	DI
-	RET
+	JMP	DI
 
 /*
  * support for morestack

commit 3beaf26e4fbf1bfd166fc3b5b7584a58b11e726c
Author: Austin Clements <austin@google.com>
Date:   Sun Oct 22 21:37:05 2017 -0400

    runtime: remove write barriers from newstack, gogo
    
    Currently, newstack and gogo have write barriers for maintaining the
    context register saved in g.sched.ctxt. This is troublesome, because
    newstack can be called from go:nowritebarrierrec places that can't
    allow write barriers. It happens to be benign because g.sched.ctxt
    will always be nil on entry to newstack *and* it so happens the
    incoming ctxt will also always be nil in these contexts (I
    think/hope), but this is playing with fire. It's also desirable to
    mark newstack go:nowritebarrierrec to prevent any other, non-benign
    write barriers from creeping in, but we can't do that right now
    because of this one write barrier.
    
    Fix all of this by observing that g.sched.ctxt is really just a saved
    live pointer register. Hence, we can shade it when we scan g's stack
    and otherwise move it back and forth between the actual context
    register and g.sched.ctxt without write barriers. This means we can
    save it in morestack along with all of the other g.sched, eliminate
    the save from newstack along with its troublesome write barrier, and
    eliminate the shenanigans in gogo to invoke the write barrier when
    restoring it.
    
    Once we've done all of this, we can mark newstack
    go:nowritebarrierrec.
    
    Fixes #22385.
    For #22460.
    
    Change-Id: I43c24958e3f6785b53c1350e1e83c2844e0d1522
    Reviewed-on: https://go-review.googlesource.com/72553
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>
    Reviewed-by: Cherry Zhang <cherryyz@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7c5e8e9ada..2ac879c31d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -304,18 +304,6 @@ TEXT runtime·gosave(SB), NOSPLIT, $0-8
 // restore state from Gobuf; longjmp
 TEXT runtime·gogo(SB), NOSPLIT, $16-8
 	MOVQ	buf+0(FP), BX		// gobuf
-
-	// If ctxt is not nil, invoke deletion barrier before overwriting.
-	MOVQ	gobuf_ctxt(BX), AX
-	TESTQ	AX, AX
-	JZ	nilctxt
-	LEAQ	gobuf_ctxt(BX), AX
-	MOVQ	AX, 0(SP)
-	MOVQ	$0, 8(SP)
-	CALL	runtime·writebarrierptr_prewrite(SB)
-	MOVQ	buf+0(FP), BX
-
-nilctxt:
 	MOVQ	gobuf_g(BX), DX
 	MOVQ	0(DX), CX		// make sure g != nil
 	get_tls(CX)
@@ -482,16 +470,14 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	LEAQ	8(SP), AX // f's SP
 	MOVQ	AX, (g_sched+gobuf_sp)(SI)
 	MOVQ	BP, (g_sched+gobuf_bp)(SI)
-	// newstack will fill gobuf.ctxt.
+	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)
 
 	// Call newstack on m->g0's stack.
 	MOVQ	m_g0(BX), BX
 	MOVQ	BX, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(BX), SP
-	PUSHQ	DX	// ctxt argument
 	CALL	runtime·newstack(SB)
 	MOVQ	$0, 0x1003	// crash if newstack returns
-	POPQ	DX	// keep balance check happy
 	RET
 
 // morestack but not preserving ctxt.

commit 30cb30e596759279b487b835440269989bd08b04
Author: Ian Lance Taylor <iant@golang.org>
Date:   Tue Oct 10 15:11:05 2017 -0700

    runtime: unify amd64 -buildmode=c-archive/c-shared entry point code
    
    This adds the _lib entry point to various GOOS_amd64.s files.
    A future CL will enable c-archive/c-shared mode for those targets.
    
    As far as I can tell, the newosproc0 function in os_darwin.go was
    passing the wrong arguments to bsdthread_create. The newosproc0
    function is never called in the current testsuite.
    
    Change-Id: Ie7c1c2e326cec87013e0fea84f751091b0ea7f51
    Reviewed-on: https://go-review.googlesource.com/69711
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 838a1681da..7c5e8e9ada 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -22,6 +22,68 @@ TEXT _rt0_amd64(SB),NOSPLIT,$-8
 TEXT main(SB),NOSPLIT,$-8
 	JMP	runtime·rt0_go(SB)
 
+// _rt0_amd64_lib is common startup code for most amd64 systems when
+// using -buildmode=c-archive or -buildmode=c-shared. The linker will
+// arrange to invoke this function as a global constructor (for
+// c-archive) or when the shared library is loaded (for c-shared).
+// We expect argc and argv to be passed in the usual C ABI registers
+// DI and SI.
+TEXT _rt0_amd64_lib(SB),NOSPLIT,$0x50
+	// Align stack per ELF ABI requirements.
+	MOVQ	SP, AX
+	ANDQ	$~15, SP
+	// Save C ABI callee-saved registers, as caller may need them.
+	MOVQ	BX, 0x10(SP)
+	MOVQ	BP, 0x18(SP)
+	MOVQ	R12, 0x20(SP)
+	MOVQ	R13, 0x28(SP)
+	MOVQ	R14, 0x30(SP)
+	MOVQ	R15, 0x38(SP)
+	MOVQ	AX, 0x40(SP)
+
+	MOVQ	DI, _rt0_amd64_lib_argc<>(SB)
+	MOVQ	SI, _rt0_amd64_lib_argv<>(SB)
+
+	// Synchronous initialization.
+	CALL	runtime·libpreinit(SB)
+
+	// Create a new thread to finish Go runtime initialization.
+	MOVQ	_cgo_sys_thread_create(SB), AX
+	TESTQ	AX, AX
+	JZ	nocgo
+	MOVQ	$_rt0_amd64_lib_go(SB), DI
+	MOVQ	$0, SI
+	CALL	AX
+	JMP	restore
+
+nocgo:
+	MOVQ	$0x800000, 0(SP)		// stacksize
+	MOVQ	$_rt0_amd64_lib_go(SB), AX
+	MOVQ	AX, 8(SP)			// fn
+	CALL	runtime·newosproc0(SB)
+
+restore:
+	MOVQ	0x10(SP), BX
+	MOVQ	0x18(SP), BP
+	MOVQ	0x20(SP), R12
+	MOVQ	0x28(SP), R13
+	MOVQ	0x30(SP), R14
+	MOVQ	0x38(SP), R15
+	MOVQ	0x40(SP), SP
+	RET
+
+// _rt0_amd64_lib_go initializes the Go runtime.
+// This is started in a separate thread by _rt0_amd64_lib.
+TEXT _rt0_amd64_lib_go(SB),NOSPLIT,$0
+	MOVQ	_rt0_amd64_lib_argc<>(SB), DI
+	MOVQ	_rt0_amd64_lib_argv<>(SB), SI
+	JMP	runtime·rt0_go(SB)
+
+DATA _rt0_amd64_lib_argc<>(SB)/8, $0
+GLOBL _rt0_amd64_lib_argc<>(SB),NOPTR, $8
+DATA _rt0_amd64_lib_argv<>(SB)/8, $0
+GLOBL _rt0_amd64_lib_argv<>(SB),NOPTR, $8
+
 TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// copy arguments forward on an even stack
 	MOVQ	DI, AX		// argc

commit cf3f771203c46d73a84d86e5ef7865d19e983150
Author: Ian Lance Taylor <iant@golang.org>
Date:   Mon Oct 9 11:31:20 2017 -0700

    runtime: unify amd64 -buildmode=exe entry point code
    
    All of the amd64 entry point code is the same except for Plan 9.
    Unify it all into asm_amd64.s.
    
    Change-Id: Id47ce3a7bb2bb0fd48f326a2d88ed18b17dee456
    Reviewed-on: https://go-review.googlesource.com/69292
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d87f454e03..838a1681da 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -7,6 +7,21 @@
 #include "funcdata.h"
 #include "textflag.h"
 
+// _rt0_amd64 is common startup code for most amd64 systems when using
+// internal linking. This is the entry point for the program from the
+// kernel for an ordinary -buildmode=exe program. The stack holds the
+// number of arguments and the C-style argv.
+TEXT _rt0_amd64(SB),NOSPLIT,$-8
+	MOVQ	0(SP), DI	// argc
+	LEAQ	8(SP), SI	// argv
+	JMP	runtime·rt0_go(SB)
+
+// main is common startup code for most amd64 systems when using
+// external linking. The C startup code will call the symbol "main"
+// passing argc and argv in the usual C ABI registers DI and SI.
+TEXT main(SB),NOSPLIT,$-8
+	JMP	runtime·rt0_go(SB)
+
 TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// copy arguments forward on an even stack
 	MOVQ	DI, AX		// argc

commit 6cac100eefbe07ffd2c9bf64c9a782bf93d79081
Author: David Chase <drchase@google.com>
Date:   Mon Oct 24 10:25:05 2016 -0400

    cmd/compile: add intrinsic for reading caller's pc
    
    First step towards removing the mandatory argument for
    getcallerpc, which solves certain problems for the runtime.
    This might also slightly improve performance.
    
    Intrinsic enabled on 386, amd64, amd64p32,
    runtime asm implementation removed on those architectures.
    
    Now-superfluous argument remains in getcallerpc signature
    (for a future CL; non-386/amd64 asm funcs ignore it).
    
    Added getcallerpc to the "not a real function" test
    in dcl.go, that story is a little odd with respect to
    unexported functions but that is not this CL.
    
    Fixes #17327.
    
    Change-Id: I5df1ad91f27ee9ac1f0dd88fa48f1329d6306c3e
    Reviewed-on: https://go-review.googlesource.com/31851
    Run-TryBot: David Chase <drchase@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f992276794..d87f454e03 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -833,12 +833,6 @@ TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
 	INT	$3
 	RET
 
-TEXT runtime·getcallerpc(SB),NOSPLIT,$8-16
-	MOVQ	argp+0(FP),AX		// addr of first arg
-	MOVQ	-8(AX),AX		// get calling pc
-	MOVQ	AX, ret+8(FP)
-	RET
-
 // func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	CMPB	runtime·lfenceBeforeRdtsc(SB), $1

commit 3216e0cefab43670c788a475237f6f4b235fc200
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Mon Aug 7 22:36:22 2017 +0200

    cmd/compile: replace eqstring with memequal
    
    eqstring is only called for strings with equal lengths.
    Instead of pushing a pointer and length for each argument string
    on the stack we can omit pushing one of the lengths on the stack.
    
    Changing eqstrings signature to eqstring(*uint8, *uint8, int) bool
    to implement the above optimization would make it very similar to the
    existing memequal(*any, *any, uintptr) bool function.
    
    Since string lengths are positive we can avoid code redundancy and
    use memequal instead of using eqstring with an optimized signature.
    
    go command binary size reduced by 4128 bytes on amd64.
    
    name                          old time/op    new time/op    delta
    CompareStringEqual              6.03ns ± 1%    5.71ns ± 1%   -5.23%  (p=0.000 n=19+18)
    CompareStringIdentical          2.88ns ± 1%    3.22ns ± 7%  +11.86%  (p=0.000 n=20+20)
    CompareStringSameLength         4.31ns ± 1%    4.01ns ± 1%   -7.17%  (p=0.000 n=19+19)
    CompareStringDifferentLength    0.29ns ± 2%    0.29ns ± 2%     ~     (p=1.000 n=20+20)
    CompareStringBigUnaligned       64.3µs ± 2%    64.1µs ± 3%     ~     (p=0.164 n=20+19)
    CompareStringBig                61.9µs ± 1%    61.6µs ± 2%   -0.46%  (p=0.033 n=20+19)
    
    Change-Id: Ice15f3b937c981f0d3bc8479a9ea0d10658ac8df
    Reviewed-on: https://go-review.googlesource.com/53650
    Run-TryBot: Martin Möhrmann <moehrmann@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index dfa49de544..f992276794 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1326,23 +1326,6 @@ eq:
 	MOVB	$1, ret+16(FP)
 	RET
 
-// eqstring tests whether two strings are equal.
-// The compiler guarantees that strings passed
-// to eqstring have equal length.
-// See runtime_test.go:eqstring_generic for
-// equivalent Go code.
-TEXT runtime·eqstring(SB),NOSPLIT,$0-33
-	MOVQ	s1_base+0(FP), SI
-	MOVQ	s2_base+16(FP), DI
-	CMPQ	SI, DI
-	JEQ	eq
-	MOVQ	s1_len+8(FP), BX
-	LEAQ	ret+32(FP), AX
-	JMP	runtime·memeqbody(SB)
-eq:
-	MOVB	$1, ret+32(FP)
-	RET
-
 // a in SI
 // b in DI
 // count in BX

commit 57bf6aca711a53aa7fea877b98896cd0445c6ad0
Author: Cholerae Hu <choleraehyq@gmail.com>
Date:   Sat Aug 5 14:44:00 2017 +0800

    runtime, cmd/compile: add intrinsic getclosureptr
    
    Intrinsic enabled on all architectures,
    runtime asm implementation removed on all architectures.
    
    Fixes #21258
    
    Change-Id: I2cb86d460b497c2f287a5b3df5c37fdb231c23a7
    Reviewed-on: https://go-review.googlesource.com/53411
    Reviewed-by: Josh Bleecher Snyder <josharian@gmail.com>
    Reviewed-by: David Chase <drchase@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ad19e21be7..dfa49de544 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -854,23 +854,6 @@ done:
 	MOVQ	AX, ret+0(FP)
 	RET
 
-// memhash_varlen(p unsafe.Pointer, h seed) uintptr
-// redirects to memhash(p, h, size) using the size
-// stored in the closure.
-TEXT runtime·memhash_varlen(SB),NOSPLIT,$32-24
-	GO_ARGS
-	NO_LOCAL_POINTERS
-	MOVQ	p+0(FP), AX
-	MOVQ	h+8(FP), BX
-	MOVQ	8(DX), CX
-	MOVQ	AX, 0(SP)
-	MOVQ	BX, 8(SP)
-	MOVQ	CX, 16(SP)
-	CALL	runtime·memhash(SB)
-	MOVQ	24(SP), AX
-	MOVQ	AX, ret+16(FP)
-	RET
-
 // hash function using AES hardware instructions
 TEXT runtime·aeshash(SB),NOSPLIT,$0-32
 	MOVQ	p+0(FP), AX	// ptr to data

commit 7045e6f6c458908e1d5082381b3506a65059eac3
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Mon May 22 07:42:42 2017 +0200

    runtime: remove unused prefetch functions
    
    The only non test user of the assembler prefetch functions is the
    heapBits.prefetch function which is itself unused.
    
    The runtime prefetch functions have no functionality on most platforms
    and are not inlineable since they are written in assembler. The function
    call overhead eliminates the performance gains that could be achieved with
    prefetching and would degrade performance for platforms where the functions
    are no-ops.
    
    If prefetch functions are needed back again later they can be improved
    by avoiding the function call overhead and implementing them as intrinsics.
    
    Change-Id: I52c553cf3607ffe09f0441c6e7a0a818cb21117d
    Reviewed-on: https://go-review.googlesource.com/44370
    Run-TryBot: Martin Möhrmann <moehrmann@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6405be92de..ad19e21be7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2339,26 +2339,6 @@ TEXT runtime·goexit(SB),NOSPLIT,$0-0
 	// traceback from goexit1 must hit code range of goexit
 	BYTE	$0x90	// NOP
 
-TEXT runtime·prefetcht0(SB),NOSPLIT,$0-8
-	MOVQ	addr+0(FP), AX
-	PREFETCHT0	(AX)
-	RET
-
-TEXT runtime·prefetcht1(SB),NOSPLIT,$0-8
-	MOVQ	addr+0(FP), AX
-	PREFETCHT1	(AX)
-	RET
-
-TEXT runtime·prefetcht2(SB),NOSPLIT,$0-8
-	MOVQ	addr+0(FP), AX
-	PREFETCHT2	(AX)
-	RET
-
-TEXT runtime·prefetchnta(SB),NOSPLIT,$0-8
-	MOVQ	addr+0(FP), AX
-	PREFETCHNTA	(AX)
-	RET
-
 // This is called from .init_array and follows the platform, not Go, ABI.
 TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
 	PUSHQ	R15 // The access to global variables below implicitly uses R15, which is callee-save

commit aeee34cb242620ad3d40685227a061818e843a72
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Wed May 10 19:03:48 2017 +0200

    runtime: remove unused cpuid_X variables
    
    They are not exported and not used in the compiler or standard library.
    
    Change-Id: Ie1d210464f826742d282f12258ed1792cbd2d188
    Reviewed-on: https://go-review.googlesource.com/43135
    Run-TryBot: Martin Möhrmann <moehrmann@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index fb428c40db..6405be92de 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -49,8 +49,6 @@ notintel:
 	MOVL	$1, AX
 	CPUID
 	MOVL	AX, runtime·processorVersionInfo(SB)
-	MOVL	CX, runtime·cpuid_ecx(SB)
-	MOVL	DX, runtime·cpuid_edx(SB)
 
 	TESTL	$(1<<26), DX // SSE2
 	SETNE	runtime·support_sse2(SB)
@@ -85,7 +83,6 @@ eax7:
 	MOVL	$7, AX
 	MOVL	$0, CX
 	CPUID
-	MOVL	BX, runtime·cpuid_ebx7(SB)
 
 	TESTL	$(1<<3), BX // BMI1
 	SETNE	runtime·support_bmi1(SB)

commit 69972aea74de6a0397a05281475d1ca006da7bb0
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Mon Apr 3 22:38:09 2017 +0200

    internal/cpu: new package to detect cpu features
    
    Implements detection of x86 cpu features that
    are used in the go standard library.
    
    Changes all standard library packages to use the new cpu package
    instead of using runtime internal variables to check x86 cpu features.
    
    Updates: #15403
    
    Change-Id: I2999a10cb4d9ec4863ffbed72f4e021a1dbc4bb9
    Reviewed-on: https://go-review.googlesource.com/41476
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0dc9a9c542..fb428c40db 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1706,27 +1706,6 @@ big_loop_avx2_exit:
 	VZEROUPPER
 	JMP loop
 
-
-TEXT strings·supportAVX2(SB),NOSPLIT,$0-1
-	MOVBLZX runtime·support_avx2(SB), AX
-	MOVB AX, ret+0(FP)
-	RET
-
-TEXT bytes·supportAVX2(SB),NOSPLIT,$0-1
-	MOVBLZX runtime·support_avx2(SB), AX
-	MOVB AX, ret+0(FP)
-	RET
-
-TEXT strings·supportPOPCNT(SB),NOSPLIT,$0-1
-	MOVBLZX runtime·support_popcnt(SB), AX
-	MOVB AX, ret+0(FP)
-	RET
-
-TEXT bytes·supportPOPCNT(SB),NOSPLIT,$0-1
-	MOVBLZX runtime·support_popcnt(SB), AX
-	MOVB AX, ret+0(FP)
-	RET
-
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
 	// We want len in DX and AX, because PCMPESTRI implicitly consumes them

commit 5a6c58099085a8156bc42b68a7cf51b5b9c72802
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Thu Apr 27 08:30:27 2017 +0200

    runtime: refactor cpu feature detection for 386 & amd64
    
    Changes all cpu features to be detected and stored in bools in rt0_go.
    
    Updates: #15403
    
    Change-Id: I5a9961cdec789b331d09c44d86beb53833d5dc3e
    Reviewed-on: https://go-review.googlesource.com/41950
    Run-TryBot: Martin Möhrmann <moehrmann@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ilya Tocar <ilya.tocar@intel.com>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 65bbf63bf1..0dc9a9c542 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -26,10 +26,10 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	MOVQ	SP, (g_stack+stack_hi)(DI)
 
 	// find out information about the processor we're on
-	MOVQ	$0, AX
+	MOVL	$0, AX
 	CPUID
-	MOVQ	AX, SI
-	CMPQ	AX, $0
+	MOVL	AX, SI
+	CMPL	AX, $0
 	JE	nocpuinfo
 
 	// Figure out how to serialize RDTSC.
@@ -46,62 +46,75 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 notintel:
 
 	// Load EAX=1 cpuid flags
-	MOVQ	$1, AX
+	MOVL	$1, AX
 	CPUID
-	MOVL	AX, runtime·cpuid_eax(SB)
+	MOVL	AX, runtime·processorVersionInfo(SB)
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
 
+	TESTL	$(1<<26), DX // SSE2
+	SETNE	runtime·support_sse2(SB)
+
+	TESTL	$(1<<9), CX // SSSE3
+	SETNE	runtime·support_ssse3(SB)
+
+	TESTL	$(1<<19), CX // SSE4.1
+	SETNE	runtime·support_sse41(SB)
+
+	TESTL	$(1<<20), CX // SSE4.2
+	SETNE	runtime·support_sse42(SB)
+
+	TESTL	$(1<<23), CX // POPCNT
+	SETNE	runtime·support_popcnt(SB)
+
+	TESTL	$(1<<25), CX // AES
+	SETNE	runtime·support_aes(SB)
+
+	TESTL	$(1<<27), CX // OSXSAVE
+	SETNE	runtime·support_osxsave(SB)
+
+	// If OS support for XMM and YMM is not present
+	// support_avx will be set back to false later.
+	TESTL	$(1<<28), CX // AVX
+	SETNE	runtime·support_avx(SB)
+
+eax7:
 	// Load EAX=7/ECX=0 cpuid flags
-	CMPQ	SI, $7
-	JLT	no7
+	CMPL	SI, $7
+	JLT	osavx
 	MOVL	$7, AX
 	MOVL	$0, CX
 	CPUID
 	MOVL	BX, runtime·cpuid_ebx7(SB)
-no7:
-	// Detect AVX and AVX2 as per 14.7.1  Detection of AVX2 chapter of [1]
-	// [1] 64-ia-32-architectures-software-developer-manual-325462.pdf
-	// http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf
-	MOVL	runtime·cpuid_ecx(SB), CX
-	ANDL    $0x18000000, CX // check for OSXSAVE and AVX bits
-	CMPL    CX, $0x18000000
-	JNE     noavx
-	MOVL    $0, CX
+
+	TESTL	$(1<<3), BX // BMI1
+	SETNE	runtime·support_bmi1(SB)
+
+	// If OS support for XMM and YMM is not present
+	// support_avx2 will be set back to false later.
+	TESTL	$(1<<5), BX
+	SETNE	runtime·support_avx2(SB)
+
+	TESTL	$(1<<8), BX // BMI2
+	SETNE	runtime·support_bmi2(SB)
+
+	TESTL	$(1<<9), BX // ERMS
+	SETNE	runtime·support_erms(SB)
+
+osavx:
+	CMPB	runtime·support_osxsave(SB), $1
+	JNE	noavx
+	MOVL	$0, CX
 	// For XGETBV, OSXSAVE bit is required and sufficient
 	XGETBV
-	ANDL    $6, AX
-	CMPL    AX, $6 // Check for OS support of YMM registers
-	JNE     noavx
-	MOVB    $1, runtime·support_avx(SB)
-	TESTL   $(1<<5), runtime·cpuid_ebx7(SB) // check for AVX2 bit
-	JEQ     noavx2
-	MOVB    $1, runtime·support_avx2(SB)
-	JMP     testbmi1
+	ANDL	$6, AX
+	CMPL	AX, $6 // Check for OS support of XMM and YMM registers.
+	JE nocpuinfo
 noavx:
-	MOVB    $0, runtime·support_avx(SB)
-noavx2:
-	MOVB    $0, runtime·support_avx2(SB)
-testbmi1:
-	// Detect BMI1 and BMI2 extensions as per
-	// 5.1.16.1 Detection of VEX-encoded GPR Instructions,
-	//   LZCNT and TZCNT, PREFETCHW chapter of [1]
-	MOVB    $0, runtime·support_bmi1(SB)
-	TESTL   $(1<<3), runtime·cpuid_ebx7(SB) // check for BMI1 bit
-	JEQ     testbmi2
-	MOVB    $1, runtime·support_bmi1(SB)
-testbmi2:
-	MOVB    $0, runtime·support_bmi2(SB)
-	TESTL   $(1<<8), runtime·cpuid_ebx7(SB) // check for BMI2 bit
-	JEQ     testpopcnt
-	MOVB    $1, runtime·support_bmi2(SB)
-testpopcnt:
-	MOVB	$0, runtime·support_popcnt(SB)
-	TESTL	$(1<<23), runtime·cpuid_ecx(SB) // check for POPCNT bit
-	JEQ     nocpuinfo
-	MOVB    $1, runtime·support_popcnt(SB)
-nocpuinfo:	
-	
+	MOVB $0, runtime·support_avx(SB)
+	MOVB $0, runtime·support_avx2(SB)
+
+nocpuinfo:
 	// if there is an _cgo_init, call it.
 	MOVQ	_cgo_init(SB), AX
 	TESTQ	AX, AX
@@ -1942,9 +1955,8 @@ success_avx2:
 	VZEROUPPER
 	JMP success
 sse42:
-	MOVL runtime·cpuid_ecx(SB), CX
-	ANDL $0x100000, CX
-	JZ no_sse42
+	CMPB runtime·support_sse42(SB), $1
+	JNE no_sse42
 	CMPQ AX, $12
 	// PCMPESTRI is slower than normal compare,
 	// so using it makes sense only if we advance 4+ bytes per compare

commit b64e817853531cc73dd5fd13a5038434283d3e5b
Author: Martin Möhrmann <moehrmann@google.com>
Date:   Mon Apr 24 16:59:33 2017 +0200

    runtime: simplify detection of preference to use AVX memmove
    
    Reduces cmd/go by 4464 bytes on amd64.
    
    Removes the duplicate detection of AVX support and
    presence of Intel processors.
    
    Change-Id: I4670189951a63760fae217708f68d65e94a30dc5
    Reviewed-on: https://go-review.googlesource.com/41570
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 36da4cc922..65bbf63bf1 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -41,12 +41,14 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	JNE	notintel
 	CMPL	CX, $0x6C65746E  // "ntel"
 	JNE	notintel
+	MOVB	$1, runtime·isIntel(SB)
 	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
 notintel:
 
 	// Load EAX=1 cpuid flags
 	MOVQ	$1, AX
 	CPUID
+	MOVL	AX, runtime·cpuid_eax(SB)
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
 

commit d206af1e6c53df0c59d9466fe9c50415f9d8dcd5
Author: Josselin Costanzi <josselin@costanzi.fr>
Date:   Mon Mar 27 13:22:59 2017 +0200

    strings: optimize Count for amd64
    
    Move optimized Count implementation from bytes to runtime. Use in
    both bytes and strings packages.
    Add CountByte benchmark to strings.
    
    Strings benchmarks:
    name                       old time/op    new time/op    delta
    CountHard1-4                 226µs ± 1%      226µs ± 2%      ~     (p=0.247 n=10+10)
    CountHard2-4                 316µs ± 1%      315µs ± 0%      ~     (p=0.133 n=9+10)
    CountHard3-4                 919µs ± 1%      920µs ± 1%      ~     (p=0.968 n=10+9)
    CountTorture-4              15.4µs ± 1%     15.7µs ± 1%    +2.47%  (p=0.000 n=10+9)
    CountTortureOverlapping-4   9.60ms ± 0%     9.65ms ± 1%      ~     (p=0.247 n=10+10)
    CountByte/10-4              26.3ns ± 1%     10.9ns ± 1%   -58.71%  (p=0.000 n=9+9)
    CountByte/32-4              42.7ns ± 0%     14.2ns ± 0%   -66.64%  (p=0.000 n=10+10)
    CountByte/4096-4            3.07µs ± 0%     0.31µs ± 2%   -89.99%  (p=0.000 n=9+10)
    CountByte/4194304-4         3.48ms ± 1%     0.34ms ± 1%   -90.09%  (p=0.000 n=10+9)
    CountByte/67108864-4        55.6ms ± 1%      7.0ms ± 0%   -87.49%  (p=0.000 n=9+8)
    
    name                      old speed      new speed       delta
    CountByte/10-4             380MB/s ± 1%    919MB/s ± 1%  +142.21%  (p=0.000 n=9+9)
    CountByte/32-4             750MB/s ± 0%   2247MB/s ± 0%  +199.62%  (p=0.000 n=10+10)
    CountByte/4096-4          1.33GB/s ± 0%  13.32GB/s ± 2%  +898.13%  (p=0.000 n=9+10)
    CountByte/4194304-4       1.21GB/s ± 1%  12.17GB/s ± 1%  +908.87%  (p=0.000 n=10+9)
    CountByte/67108864-4      1.21GB/s ± 1%   9.65GB/s ± 0%  +699.29%  (p=0.000 n=9+8)
    
    Fixes #19411
    
    Change-Id: I8d2d409f0fa6df6d03b60790aa86e540b4a4e3b0
    Reviewed-on: https://go-review.googlesource.com/38693
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index c0a5048eda..36da4cc922 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1702,6 +1702,11 @@ TEXT bytes·supportAVX2(SB),NOSPLIT,$0-1
 	MOVB AX, ret+0(FP)
 	RET
 
+TEXT strings·supportPOPCNT(SB),NOSPLIT,$0-1
+	MOVBLZX runtime·support_popcnt(SB), AX
+	MOVB AX, ret+0(FP)
+	RET
+
 TEXT bytes·supportPOPCNT(SB),NOSPLIT,$0-1
 	MOVBLZX runtime·support_popcnt(SB), AX
 	MOVB AX, ret+0(FP)
@@ -2131,6 +2136,196 @@ eqret:
 	MOVB	$0, ret+48(FP)
 	RET
 
+
+TEXT bytes·countByte(SB),NOSPLIT,$0-40
+	MOVQ s+0(FP), SI
+	MOVQ s_len+8(FP), BX
+	MOVB c+24(FP), AL
+	LEAQ ret+32(FP), R8
+	JMP  runtime·countByte(SB)
+
+TEXT strings·countByte(SB),NOSPLIT,$0-32
+	MOVQ s+0(FP), SI
+	MOVQ s_len+8(FP), BX
+	MOVB c+16(FP), AL
+	LEAQ ret+24(FP), R8
+	JMP  runtime·countByte(SB)
+
+// input:
+//   SI: data
+//   BX: data len
+//   AL: byte sought
+//   R8: address to put result
+// This requires the POPCNT instruction
+TEXT runtime·countByte(SB),NOSPLIT,$0
+	// Shuffle X0 around so that each byte contains
+	// the character we're looking for.
+	MOVD AX, X0
+	PUNPCKLBW X0, X0
+	PUNPCKLBW X0, X0
+	PSHUFL $0, X0, X0
+
+	CMPQ BX, $16
+	JLT small
+
+	MOVQ $0, R12 // Accumulator
+
+	MOVQ SI, DI
+
+	CMPQ BX, $32
+	JA avx2
+sse:
+	LEAQ	-16(SI)(BX*1), AX	// AX = address of last 16 bytes
+	JMP	sseloopentry
+
+sseloop:
+	// Move the next 16-byte chunk of the data into X1.
+	MOVOU	(DI), X1
+	// Compare bytes in X0 to X1.
+	PCMPEQB	X0, X1
+	// Take the top bit of each byte in X1 and put the result in DX.
+	PMOVMSKB X1, DX
+	// Count number of matching bytes
+	POPCNTL DX, DX
+	// Accumulate into R12
+	ADDQ DX, R12
+	// Advance to next block.
+	ADDQ	$16, DI
+sseloopentry:
+	CMPQ	DI, AX
+	JBE	sseloop
+
+	// Get the number of bytes to consider in the last 16 bytes
+	ANDQ $15, BX
+	JZ end
+
+	// Create mask to ignore overlap between previous 16 byte block
+	// and the next.
+	MOVQ $16,CX
+	SUBQ BX, CX
+	MOVQ $0xFFFF, R10
+	SARQ CL, R10
+	SALQ CL, R10
+
+	// Process the last 16-byte chunk. This chunk may overlap with the
+	// chunks we've already searched so we need to mask part of it.
+	MOVOU	(AX), X1
+	PCMPEQB	X0, X1
+	PMOVMSKB X1, DX
+	// Apply mask
+	ANDQ R10, DX
+	POPCNTL DX, DX
+	ADDQ DX, R12
+end:
+	MOVQ R12, (R8)
+	RET
+
+// handle for lengths < 16
+small:
+	TESTQ	BX, BX
+	JEQ	endzero
+
+	// Check if we'll load across a page boundary.
+	LEAQ	16(SI), AX
+	TESTW	$0xff0, AX
+	JEQ	endofpage
+
+	// We must ignore high bytes as they aren't part of our slice.
+	// Create mask.
+	MOVB BX, CX
+	MOVQ $1, R10
+	SALQ CL, R10
+	SUBQ $1, R10
+
+	// Load data
+	MOVOU	(SI), X1
+	// Compare target byte with each byte in data.
+	PCMPEQB	X0, X1
+	// Move result bits to integer register.
+	PMOVMSKB X1, DX
+	// Apply mask
+	ANDQ R10, DX
+	POPCNTL DX, DX
+	// Directly return DX, we don't need to accumulate
+	// since we have <16 bytes.
+	MOVQ	DX, (R8)
+	RET
+endzero:
+	MOVQ $0, (R8)
+	RET
+
+endofpage:
+	// We must ignore low bytes as they aren't part of our slice.
+	MOVQ $16,CX
+	SUBQ BX, CX
+	MOVQ $0xFFFF, R10
+	SARQ CL, R10
+	SALQ CL, R10
+
+	// Load data into the high end of X1.
+	MOVOU	-16(SI)(BX*1), X1
+	// Compare target byte with each byte in data.
+	PCMPEQB	X0, X1
+	// Move result bits to integer register.
+	PMOVMSKB X1, DX
+	// Apply mask
+	ANDQ R10, DX
+	// Directly return DX, we don't need to accumulate
+	// since we have <16 bytes.
+	POPCNTL DX, DX
+	MOVQ	DX, (R8)
+	RET
+
+avx2:
+	CMPB   runtime·support_avx2(SB), $1
+	JNE sse
+	MOVD AX, X0
+	LEAQ -32(SI)(BX*1), R11
+	VPBROADCASTB  X0, Y1
+avx2_loop:
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPMOVMSKB Y3, DX
+	POPCNTL DX, DX
+	ADDQ DX, R12
+	ADDQ $32, DI
+	CMPQ DI, R11
+	JLE avx2_loop
+
+	// If last block is already processed,
+	// skip to the end.
+	CMPQ DI, R11
+	JEQ endavx
+
+	// Load address of the last 32 bytes.
+	// There is an overlap with the previous block.
+	MOVQ R11, DI
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPMOVMSKB Y3, DX
+	// Exit AVX mode.
+	VZEROUPPER
+
+	// Create mask to ignore overlap between previous 32 byte block
+	// and the next.
+	ANDQ $31, BX
+	MOVQ $32,CX
+	SUBQ BX, CX
+	MOVQ $0xFFFFFFFF, R10
+	SARQ CL, R10
+	SALQ CL, R10
+	// Apply mask
+	ANDQ R10, DX
+	POPCNTL DX, DX
+	ADDQ DX, R12
+	MOVQ R12, (R8)
+	RET
+endavx:
+	// Exit AVX mode.
+	VZEROUPPER
+	MOVQ R12, (R8)
+	RET
+
 TEXT runtime·return0(SB), NOSPLIT, $0
 	MOVL	$0, AX
 	RET

commit 01cd22c68792b659ca0912c104b14c86044110cb
Author: Josselin Costanzi <josselin@costanzi.fr>
Date:   Sun Mar 19 12:18:08 2017 +0100

    bytes: add optimized countByte for amd64
    
    Use SSE/AVX2 when counting a single byte.
    Inspired from runtime indexbyte implementation.
    
    Benchmark against previous implementation, where
    1 byte in every 8 is the one we are looking for:
    
    * On a machine without AVX2
    name               old time/op   new time/op     delta
    CountSingle/10-4    61.8ns ±10%     15.6ns ±11%    -74.83%  (p=0.000 n=10+10)
    CountSingle/32-4     100ns ± 4%       17ns ±10%    -82.54%  (p=0.000 n=10+9)
    CountSingle/4K-4    9.66µs ± 3%     0.37µs ± 6%    -96.21%  (p=0.000 n=10+10)
    CountSingle/4M-4    11.0ms ± 6%      0.4ms ± 4%    -96.04%  (p=0.000 n=10+10)
    CountSingle/64M-4    194ms ± 8%        8ms ± 2%    -95.64%  (p=0.000 n=10+10)
    
    name               old speed     new speed       delta
    CountSingle/10-4   162MB/s ±10%    645MB/s ±10%   +297.00%  (p=0.000 n=10+10)
    CountSingle/32-4   321MB/s ± 5%   1844MB/s ± 9%   +474.79%  (p=0.000 n=10+9)
    CountSingle/4K-4   424MB/s ± 3%  11169MB/s ± 6%  +2533.10%  (p=0.000 n=10+10)
    CountSingle/4M-4   381MB/s ± 7%   9609MB/s ± 4%  +2421.88%  (p=0.000 n=10+10)
    CountSingle/64M-4  346MB/s ± 7%   7924MB/s ± 2%  +2188.78%  (p=0.000 n=10+10)
    
    * On a machine with AVX2
    name               old time/op   new time/op     delta
    CountSingle/10-8    37.1ns ± 3%      8.2ns ± 1%    -77.80%  (p=0.000 n=10+10)
    CountSingle/32-8    66.1ns ± 3%      9.8ns ± 2%    -85.23%  (p=0.000 n=10+10)
    CountSingle/4K-8    7.36µs ± 3%     0.11µs ± 1%    -98.54%  (p=0.000 n=10+10)
    CountSingle/4M-8    7.46ms ± 2%     0.15ms ± 2%    -97.95%  (p=0.000 n=10+9)
    CountSingle/64M-8    124ms ± 2%        6ms ± 4%    -95.09%  (p=0.000 n=10+10)
    
    name               old speed     new speed       delta
    CountSingle/10-8   269MB/s ± 3%   1213MB/s ± 1%   +350.32%  (p=0.000 n=10+10)
    CountSingle/32-8   484MB/s ± 4%   3277MB/s ± 2%   +576.66%  (p=0.000 n=10+10)
    CountSingle/4K-8   556MB/s ± 3%  37933MB/s ± 1%  +6718.36%  (p=0.000 n=10+10)
    CountSingle/4M-8   562MB/s ± 2%  27444MB/s ± 3%  +4783.43%  (p=0.000 n=10+9)
    CountSingle/64M-8  543MB/s ± 2%  11054MB/s ± 3%  +1935.81%  (p=0.000 n=10+10)
    
    Fixes #19411
    
    Change-Id: Ieaf20b1fabccabe767c55c66e242e86f3617f883
    Reviewed-on: https://go-review.googlesource.com/38258
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index c6ff8379e6..c0a5048eda 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -91,8 +91,13 @@ testbmi1:
 testbmi2:
 	MOVB    $0, runtime·support_bmi2(SB)
 	TESTL   $(1<<8), runtime·cpuid_ebx7(SB) // check for BMI2 bit
-	JEQ     nocpuinfo
+	JEQ     testpopcnt
 	MOVB    $1, runtime·support_bmi2(SB)
+testpopcnt:
+	MOVB	$0, runtime·support_popcnt(SB)
+	TESTL	$(1<<23), runtime·cpuid_ecx(SB) // check for POPCNT bit
+	JEQ     nocpuinfo
+	MOVB    $1, runtime·support_popcnt(SB)
 nocpuinfo:	
 	
 	// if there is an _cgo_init, call it.
@@ -1697,6 +1702,11 @@ TEXT bytes·supportAVX2(SB),NOSPLIT,$0-1
 	MOVB AX, ret+0(FP)
 	RET
 
+TEXT bytes·supportPOPCNT(SB),NOSPLIT,$0-1
+	MOVBLZX runtime·support_popcnt(SB), AX
+	MOVB AX, ret+0(FP)
+	RET
+
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
 	// We want len in DX and AX, because PCMPESTRI implicitly consumes them

commit c310c688ffa46e2f91e40284c16d71f3921feed9
Author: Matthew Dempsky <mdempsky@google.com>
Date:   Wed Mar 1 15:50:57 2017 -0800

    cmd/compile, runtime: simplify multiway select implementation
    
    This commit reworks multiway select statements to use normal control
    flow primitives instead of the previous setjmp/longjmp-like behavior.
    This simplifies liveness analysis and should prevent issues around
    "returns twice" function calls within SSA passes.
    
    test/live.go is updated because liveness analysis's CFG is more
    representative of actual control flow. The case bodies are the only
    real successors of the selectgo call, but previously the selectsend,
    selectrecv, etc. calls were included in the successors list too.
    
    Updates #19331.
    
    Change-Id: I7f879b103a4b85e62fc36a270d812f54c0aa3e83
    Reviewed-on: https://go-review.googlesource.com/37661
    Run-TryBot: Matthew Dempsky <mdempsky@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0686449cf6..c6ff8379e6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -822,12 +822,6 @@ TEXT runtime·getcallerpc(SB),NOSPLIT,$8-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT runtime·setcallerpc(SB),NOSPLIT,$8-16
-	MOVQ	argp+0(FP),AX		// addr of first arg
-	MOVQ	pc+8(FP), BX
-	MOVQ	BX, -8(AX)		// set calling pc
-	RET
-
 // func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	CMPB	runtime·lfenceBeforeRdtsc(SB), $1

commit d089a6c7187f1ff85277515405ec6c641588a7ff
Author: Austin Clements <austin@google.com>
Date:   Thu Feb 9 14:03:49 2017 -0500

    runtime: remove stack barriers
    
    Now that we don't rescan stacks, stack barriers are unnecessary. This
    removes all of the code and structures supporting them as well as
    tests that were specifically for stack barriers.
    
    Updates #17503.
    
    Change-Id: Ia29221730e0f2bbe7beab4fa757f31a032d9690c
    Reviewed-on: https://go-review.googlesource.com/36620
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 232c7c647d..0686449cf6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -405,28 +405,6 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 	MOVL	$0, DX
 	JMP	runtime·morestack(SB)
 
-TEXT runtime·stackBarrier(SB),NOSPLIT,$0
-	// We came here via a RET to an overwritten return PC.
-	// AX may be live. Other registers are available.
-
-	// Get the original return PC, g.stkbar[g.stkbarPos].savedLRVal.
-	get_tls(CX)
-	MOVQ	g(CX), CX
-	MOVQ	(g_stkbar+slice_array)(CX), DX
-	MOVQ	g_stkbarPos(CX), BX
-	IMULQ	$stkbar__size, BX	// Too big for SIB.
-	MOVQ	stkbar_savedLRPtr(DX)(BX*1), R8
-	MOVQ	stkbar_savedLRVal(DX)(BX*1), BX
-	// Assert that we're popping the right saved LR.
-	ADDQ	$8, R8
-	CMPQ	R8, SP
-	JEQ	2(PC)
-	MOVL	$0, 0
-	// Record that this stack barrier was hit.
-	ADDQ	$1, g_stkbarPos(CX)
-	// Jump to the original return PC.
-	JMP	BX
-
 // reflectcall: call a function with the given argument list
 // func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
 // we don't have variable-sized frames, so we use a small number
@@ -841,28 +819,14 @@ TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
 TEXT runtime·getcallerpc(SB),NOSPLIT,$8-16
 	MOVQ	argp+0(FP),AX		// addr of first arg
 	MOVQ	-8(AX),AX		// get calling pc
-	CMPQ	AX, runtime·stackBarrierPC(SB)
-	JNE	nobar
-	// Get original return PC.
-	CALL	runtime·nextBarrierPC(SB)
-	MOVQ	0(SP), AX
-nobar:
 	MOVQ	AX, ret+8(FP)
 	RET
 
 TEXT runtime·setcallerpc(SB),NOSPLIT,$8-16
 	MOVQ	argp+0(FP),AX		// addr of first arg
 	MOVQ	pc+8(FP), BX
-	MOVQ	-8(AX), CX
-	CMPQ	CX, runtime·stackBarrierPC(SB)
-	JEQ	setbar
 	MOVQ	BX, -8(AX)		// set calling pc
 	RET
-setbar:
-	// Set the stack barrier return PC.
-	MOVQ	BX, 0(SP)
-	CALL	runtime·setNextBarrierPC(SB)
-	RET
 
 // func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0

commit d03c1248604679e1e6a01253144065bc57da48b8
Author: Sokolov Yura <funny.falcon@gmail.com>
Date:   Thu Jan 5 09:36:27 2017 +0300

    runtime: implement fastrand in go
    
    So it could be inlined.
    
    Using bit-tricks it could be implemented without condition
    (improved trick version by Minux Ma).
    
    Simple benchmark shows it is faster on i386 and x86_64, though
    I don't know will it be faster on other architectures?
    
    benchmark                       old ns/op     new ns/op     delta
    BenchmarkFastrand-3             2.79          1.48          -46.95%
    BenchmarkFastrandHashiter-3     25.9          24.9          -3.86%
    
    Change-Id: Ie2eb6d0f598c0bb5fac7f6ad0f8b5e3eddaa361b
    Reviewed-on: https://go-review.googlesource.com/34782
    Reviewed-by: Minux Ma <minux@golang.org>
    Run-TryBot: Minux Ma <minux@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cb428d6de3..232c7c647d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2163,19 +2163,6 @@ eqret:
 	MOVB	$0, ret+48(FP)
 	RET
 
-TEXT runtime·fastrand(SB), NOSPLIT, $0-4
-	get_tls(CX)
-	MOVQ	g(CX), AX
-	MOVQ	g_m(AX), AX
-	MOVL	m_fastrand(AX), DX
-	ADDL	DX, DX
-	MOVL	DX, BX
-	XORL	$0x88888eef, DX
-	CMOVLMI	BX, DX
-	MOVL	DX, m_fastrand(AX)
-	MOVL	DX, ret+0(FP)
-	RET
-
 TEXT runtime·return0(SB), NOSPLIT, $0
 	MOVL	$0, AX
 	RET

commit a2b615d5270f0bc2ee1dfcdd7849bdd05ee76a14
Author: Lion Yang <lion@aosc.xyz>
Date:   Thu Jan 5 05:13:53 2017 +0800

    crypto: detect BMI usability on AMD64 for sha1 and sha256
    
    The existing implementations on AMD64 only detects AVX2 usability,
    when they also contains BMI (bit-manipulation instructions).
    These instructions crash the running program as 'unknown instructions'
    on the architecture, e.g. i3-4000M, which supports AVX2 but not
    support BMI.
    
    This change added the detections for BMI1 and BMI2 to AMD64 runtime with
    two flags as the result, `support_bmi1` and `support_bmi2`,
    in runtime/runtime2.go. It also completed the condition to run AVX2 version
    in packages crypto/sha1 and crypto/sha256.
    
    Fixes #18512
    
    Change-Id: I917bf0de365237740999de3e049d2e8f2a4385ad
    Reviewed-on: https://go-review.googlesource.com/34850
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0070e9d203..cb428d6de3 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -75,11 +75,24 @@ no7:
 	TESTL   $(1<<5), runtime·cpuid_ebx7(SB) // check for AVX2 bit
 	JEQ     noavx2
 	MOVB    $1, runtime·support_avx2(SB)
-	JMP     nocpuinfo
+	JMP     testbmi1
 noavx:
 	MOVB    $0, runtime·support_avx(SB)
 noavx2:
 	MOVB    $0, runtime·support_avx2(SB)
+testbmi1:
+	// Detect BMI1 and BMI2 extensions as per
+	// 5.1.16.1 Detection of VEX-encoded GPR Instructions,
+	//   LZCNT and TZCNT, PREFETCHW chapter of [1]
+	MOVB    $0, runtime·support_bmi1(SB)
+	TESTL   $(1<<3), runtime·cpuid_ebx7(SB) // check for BMI1 bit
+	JEQ     testbmi2
+	MOVB    $1, runtime·support_bmi1(SB)
+testbmi2:
+	MOVB    $0, runtime·support_bmi2(SB)
+	TESTL   $(1<<8), runtime·cpuid_ebx7(SB) // check for BMI2 bit
+	JEQ     nocpuinfo
+	MOVB    $1, runtime·support_bmi2(SB)
 nocpuinfo:	
 	
 	// if there is an _cgo_init, call it.

commit fa2c4ef4a2941edebd0fc05fff6c1b2071029878
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 15:57:05 2016 -0500

    updates for runtime upgrade

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7f1d20938c..e46f6385d7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -355,7 +355,7 @@ ok:
 	//MOVQ	24(SP), AX		// copy argv
 	MOVQ	$fakeargv(SB), AX
 	MOVQ	AX, 8(SP)
-	CALL	runtime·args(SB)
+	//CALL	runtime·args(SB)
 	CALL	runtime·osinit(SB)
 	CALL	runtime·schedinit(SB)
 

commit ec960a90d798404cb0a389a7bc60e6f9b5ad2da5
Merge: f1e22ef831 41908a5453
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 15:33:11 2016 -0500

    Merge commit '41908a54530120b68a79e0fd22b5e709d33cced0' into newmaster
    
    Conflicts:
            src/runtime/os_linux.go
            src/runtime/sys_linux_amd64.s

commit 6d8c62768cab561c48376f9d45aa0b2d0acda4d4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 11:05:55 2016 -0500

    fix Userrun()
    
    the fast syscall code is brittle and must know the size of the stack frame. the
    go1.7 compiler apparently changed the size of the stack frame.
    
    the fast syscall code needs an overhaul -- bandaid this for now; i'll fix and
    redo this code later.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 80a6706f08..866b9360f9 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -965,7 +965,7 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 // if you change the number of arguments, you must adjust the stack offsets in
 // _sysentry and ·_userint.
 // func _Userrun(tf *[24]int, fastret bool) (int, int)
-TEXT ·_Userrun(SB), NOSPLIT, $24-32
+TEXT ·_Userrun(SB), NOSPLIT, $8-16
 	MOVQ	tf+0(FP), R9
 
 	SWAPGS
@@ -1023,7 +1023,7 @@ TEXT ·_sysentry(SB), NOSPLIT, $0-0
 	MOVQ	0x20(SP), SP
 
 	// save user state in fake trapframe
-	MOVQ	0x20(SP), R9
+	MOVQ	0x18(SP), R9
 	MOVQ	R10, TF_RSP(R9)
 	MOVQ	R11, TF_RIP(R9)
 	// syscall args
@@ -1037,10 +1037,10 @@ TEXT ·_sysentry(SB), NOSPLIT, $0-0
 	MOVQ	BP,  TF_RBP(R9)
 	MOVQ	BX,  TF_RBX(R9)
 	// return val 1
-	MOVQ	TRAP_SYSCALL, 0x30(SP)
+	MOVQ	TRAP_SYSCALL, 0x28(SP)
 	// return val 2
-	MOVQ	$0, 0x38(SP)
-	ADDQ	$0x18, SP
+	MOVQ	$0, 0x30(SP)
+	ADDQ	$0x10, SP
 	RET
 
 // this is the bottom half of _userrun() that is executed if a timer int or CPU
@@ -1049,9 +1049,9 @@ TEXT ·_userint(SB), NOSPLIT, $0-0
 	CLI
 	// user state is already saved by trap handler.
 	// AX holds the interrupt number, BX holds aux (cr2 for page fault)
-	MOVQ	AX, 0x30(SP)
-	MOVQ	BX, 0x38(SP)
-	ADDQ	$0x18, SP
+	MOVQ	AX, 0x28(SP)
+	MOVQ	BX, 0x30(SP)
+	ADDQ	$0x10, SP
 	RET
 
 TEXT ·gs_null(SB), NOSPLIT, $8-0

commit 0ac702e900e2199f6b40d3d7228a29c08d4e730c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 11:03:53 2016 -0500

    initialize SSE earlier in boot

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 44fdb8bc9e..80a6706f08 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -310,6 +310,9 @@ needtls:
 	//LEAQ	runtime·m0+m_tls(SB), DI
 	//CALL	runtime·settls(SB)
 
+	// from this point on, SSE regs may be used. allow SSE instructions
+	// immediately.
+	CALL	·fpuinit0(SB)
 	CALL	·seg_setup(SB)
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.

commit 106db54e283f176737d642420b2ff3ce71d84e32
Merge: e89ca7e2b0 7a62274065
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Dec 15 15:56:06 2016 -0500

    Merge commit '7a622740655bb5fcbd160eb96887032314842e6e' into newmaster
    
    Conflicts:
            src/runtime/os1_linux.go
            src/runtime/os_linux.go
            src/runtime/runtime2.go

commit e89ca7e2b00ad3df864652b000d3932667b0bda1
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Dec 15 15:54:05 2016 -0500

    revert to ancestor of golang/master and go1.6.2

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 53691fc3a7..c4945c697c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -73,8 +73,8 @@ noavx:
 	MOVB    $0, runtime·support_avx(SB)
 noavx2:
 	MOVB    $0, runtime·support_avx2(SB)
-nocpuinfo:
-
+nocpuinfo:	
+	
 	// if there is an _cgo_init, call it.
 	MOVQ	_cgo_init(SB), AX
 	TESTQ	AX, AX

commit 1ea60c136a5782eadb9abf92064e3c1985fe1cdd
Author: Keith Randall <khr@golang.org>
Date:   Fri Dec 2 15:17:52 2016 -0800

    runtime: on stack copy, adjust BP
    
    When we copy the stack, we need to adjust all BPs.
    We correctly adjust the ones on the stack, but we also
    need to adjust the one that is in g.sched.bp.
    
    Like CL 33754, no test as only kernel-gathered profiles will notice.
    Tests will come (in 1.9) with the implementation of #16638.
    
    The invariant should hold that every frame pointer points to
    somewhere within its stack.  After this CL, it is mostly true, but
    something about cgo breaks it.  The runtime checks are disabled
    until I figure that out.
    
    Update #16638
    Fixes #18174
    
    Change-Id: I6023ee64adc80574ee3e76491d4f0fa5ede3dbdb
    Reviewed-on: https://go-review.googlesource.com/33895
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9ffd297d84..0070e9d203 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -742,7 +742,7 @@ havem:
 	MOVQ	(g_sched+gobuf_pc)(SI), BX
 	MOVQ	BX, -8(DI)
 	// Compute the size of the frame, including return PC and, if
-	// GOEXPERIMENT=framepointer, the saved based pointer
+	// GOEXPERIMENT=framepointer, the saved base pointer
 	MOVQ	ctxt+24(FP), BX
 	LEAQ	fv+0(FP), AX
 	SUBQ	SP, AX

commit 70c107c68dca7d57a24b35dd81420fb889aa1031
Author: Austin Clements <austin@google.com>
Date:   Wed Oct 19 15:49:31 2016 -0400

    runtime: add deletion barriers on gobuf.ctxt
    
    gobuf.ctxt is set to nil from many places in assembly code and these
    assignments require write barriers with the hybrid barrier.
    
    Conveniently, in most of these places ctxt should already be nil, in
    which case we don't need the barrier. This commit changes these places
    to assert that ctxt is already nil.
    
    gogo is more complicated, since ctxt may not already be nil. For gogo,
    we manually perform the write barrier if ctxt is not nil.
    
    Updates #17503.
    
    Change-Id: I9d75e27c75a1b7f8b715ad112fc5d45ffa856d30
    Reviewed-on: https://go-review.googlesource.com/31764
    Reviewed-by: Cherry Zhang <cherryyz@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index bcc9cad655..9ffd297d84 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -182,8 +182,12 @@ TEXT runtime·gosave(SB), NOSPLIT, $0-8
 	MOVQ	0(SP), BX		// caller's PC
 	MOVQ	BX, gobuf_pc(AX)
 	MOVQ	$0, gobuf_ret(AX)
-	MOVQ	$0, gobuf_ctxt(AX)
 	MOVQ	BP, gobuf_bp(AX)
+	// Assert ctxt is zero. See func save.
+	MOVQ	gobuf_ctxt(AX), BX
+	TESTQ	BX, BX
+	JZ	2(PC)
+	CALL	runtime·badctxt(SB)
 	get_tls(CX)
 	MOVQ	g(CX), BX
 	MOVQ	BX, gobuf_g(AX)
@@ -191,8 +195,20 @@ TEXT runtime·gosave(SB), NOSPLIT, $0-8
 
 // void gogo(Gobuf*)
 // restore state from Gobuf; longjmp
-TEXT runtime·gogo(SB), NOSPLIT, $0-8
+TEXT runtime·gogo(SB), NOSPLIT, $16-8
 	MOVQ	buf+0(FP), BX		// gobuf
+
+	// If ctxt is not nil, invoke deletion barrier before overwriting.
+	MOVQ	gobuf_ctxt(BX), AX
+	TESTQ	AX, AX
+	JZ	nilctxt
+	LEAQ	gobuf_ctxt(BX), AX
+	MOVQ	AX, 0(SP)
+	MOVQ	$0, 8(SP)
+	CALL	runtime·writebarrierptr_prewrite(SB)
+	MOVQ	buf+0(FP), BX
+
+nilctxt:
 	MOVQ	gobuf_g(BX), DX
 	MOVQ	0(DX), CX		// make sure g != nil
 	get_tls(CX)
@@ -546,8 +562,12 @@ TEXT gosave<>(SB),NOSPLIT,$0
 	LEAQ	8(SP), R9
 	MOVQ	R9, (g_sched+gobuf_sp)(R8)
 	MOVQ	$0, (g_sched+gobuf_ret)(R8)
-	MOVQ	$0, (g_sched+gobuf_ctxt)(R8)
 	MOVQ	BP, (g_sched+gobuf_bp)(R8)
+	// Assert ctxt is zero. See func save.
+	MOVQ	(g_sched+gobuf_ctxt)(R8), R9
+	TESTQ	R9, R9
+	JZ	2(PC)
+	CALL	runtime·badctxt(SB)
 	RET
 
 // func asmcgocall(fn, arg unsafe.Pointer) int32

commit bad70500c93c3993e1edbf0e19695f95ca21484a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Oct 26 16:36:09 2016 -0400

    basic TCP sending

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6efcd190a6..53691fc3a7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -972,6 +972,17 @@ TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	PUSHQ	R9
 	CALL	·_userret(SB)
 	INT	$3
+	MOVL	$0x5cc, DX
+	TESTL	AX, AX
+	MOVL	DX, CX
+	ORL	$0x20, DX
+	DIVL	0x58(CX)
+	ANDL	$0x1f, DX
+	MOVQ	CR4, AX
+	MOVL	$0x16, AX
+	MOVL	0x104(CX), SI
+	MOVL	DI, 8(SP)
+	MOVB	$1, 3(DX)
 
 syscallreturn:
 	MOVQ	TF_RAX(R9), AX

commit 79561a84ceb4435c1294767d26b0b8a0dd77809d
Author: Austin Clements <austin@google.com>
Date:   Thu Oct 20 22:45:18 2016 -0400

    runtime: simplify reflectcall write barriers
    
    Currently reflectcall has a subtle dance with write barriers where the
    assembly code copies the result values from the stack to the in-heap
    argument frame without write barriers and then calls into the runtime
    after the fact to invoke the necessary write barriers.
    
    For the hybrid barrier (and for ROC), we need to switch to a
    *pre*-write write barrier, which is very difficult to do with the
    current setup. We could tie ourselves in knots of subtle reasoning
    about why it's okay in this particular case to have a post-write write
    barrier, but this commit instead takes a different approach. Rather
    than making things more complex, this simplifies reflection calls so
    that the argument copy is done in Go using normal bulk write barriers.
    
    The one difficulty with this approach is that calling into Go requires
    putting arguments on the stack, but the call* functions "donate" their
    entire stack frame to the called function. We can get away with this
    now because the copy avoids using the stack and has copied the results
    out before we clobber the stack frame to call into the write barrier.
    The solution in this CL is to call another function, passing arguments
    in registers instead of on the stack, and let that other function
    reserve more stack space and setup the arguments for the runtime.
    
    This approach seemed to work out the best. I also tried making the
    call* functions reserve 32 extra bytes of frame for the write barrier
    arguments and adjust SP up by 32 bytes around the call. However, even
    with the necessary changes to the assembler to correct the spdelta
    table, the runtime was still having trouble with the frame layout (and
    the changes to the assembler caused many other things that do strange
    things with the SP to fail to assemble). The approach I took doesn't
    require any funny business with the SP.
    
    Updates #17503.
    
    Change-Id: Ie2bb0084b24d6cff38b5afb218b9e0534ad2119e
    Reviewed-on: https://go-review.googlesource.com/31655
    Run-TryBot: Austin Clements <austin@google.com>
    Reviewed-by: Cherry Zhang <cherryyz@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 398b14888f..bcc9cad655 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -416,8 +416,6 @@ TEXT reflect·call(SB), NOSPLIT, $0-0
 
 TEXT ·reflectcall(SB), NOSPLIT, $0-32
 	MOVLQZX argsize+24(FP), CX
-	// NOTE(rsc): No call16, because CALLFN needs four words
-	// of argument space to invoke callwritebarrier.
 	DISPATCH(runtime·call32, 32)
 	DISPATCH(runtime·call64, 64)
 	DISPATCH(runtime·call128, 128)
@@ -460,24 +458,28 @@ TEXT NAME(SB), WRAPPER, $MAXSIZE-32;		\
 	PCDATA  $PCDATA_StackMapIndex, $0;	\
 	CALL	(DX);				\
 	/* copy return values back */		\
+	MOVQ	argtype+0(FP), DX;		\
 	MOVQ	argptr+16(FP), DI;		\
 	MOVLQZX	argsize+24(FP), CX;		\
-	MOVLQZX retoffset+28(FP), BX;		\
+	MOVLQZX	retoffset+28(FP), BX;		\
 	MOVQ	SP, SI;				\
 	ADDQ	BX, DI;				\
 	ADDQ	BX, SI;				\
 	SUBQ	BX, CX;				\
-	REP;MOVSB;				\
-	/* execute write barrier updates */	\
-	MOVQ	argtype+0(FP), DX;		\
-	MOVQ	argptr+16(FP), DI;		\
-	MOVLQZX	argsize+24(FP), CX;		\
-	MOVLQZX retoffset+28(FP), BX;		\
-	MOVQ	DX, 0(SP);			\
-	MOVQ	DI, 8(SP);			\
-	MOVQ	CX, 16(SP);			\
-	MOVQ	BX, 24(SP);			\
-	CALL	runtime·callwritebarrier(SB);	\
+	CALL	callRet<>(SB);			\
+	RET
+
+// callRet copies return values back at the end of call*. This is a
+// separate function so it can allocate stack space for the arguments
+// to reflectcallmove. It does not follow the Go ABI; it expects its
+// arguments in registers.
+TEXT callRet<>(SB), NOSPLIT, $32-0
+	NO_LOCAL_POINTERS
+	MOVQ	DX, 0(SP)
+	MOVQ	DI, 8(SP)
+	MOVQ	SI, 16(SP)
+	MOVQ	CX, 24(SP)
+	CALL	runtime·reflectcallmove(SB)
 	RET
 
 CALLFN(·call32, 32)

commit bf9c71cb434a730679f54a3a87c2e9e36ec400d0
Author: Austin Clements <austin@google.com>
Date:   Wed Oct 19 18:27:39 2016 -0400

    runtime: make morestack less subtle
    
    morestack writes the context pointer to gobuf.ctxt, but since
    morestack is written in assembly (and has to be very careful with
    state), it does *not* invoke the requisite write barrier for this
    write. Instead, we patch this up later, in newstack, where we invoke
    an explicit write barrier for ctxt.
    
    This already requires some subtle reasoning, and it's going to get a
    lot hairier with the hybrid barrier.
    
    Fix this by simplifying the whole mechanism. Instead of writing
    gobuf.ctxt in morestack, just pass the value of the context register
    to newstack and let it write it to gobuf.ctxt. This is a normal Go
    pointer write, so it gets the normal Go write barrier. No subtle
    reasoning required.
    
    Updates #17503.
    
    Change-Id: Ia6bf8459bfefc6828f53682ade32c02412e4db63
    Reviewed-on: https://go-review.googlesource.com/31550
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Cherry Zhang <cherryyz@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 34da3bda9f..398b14888f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -358,15 +358,17 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	SI, (g_sched+gobuf_g)(SI)
 	LEAQ	8(SP), AX // f's SP
 	MOVQ	AX, (g_sched+gobuf_sp)(SI)
-	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)
 	MOVQ	BP, (g_sched+gobuf_bp)(SI)
+	// newstack will fill gobuf.ctxt.
 
 	// Call newstack on m->g0's stack.
 	MOVQ	m_g0(BX), BX
 	MOVQ	BX, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(BX), SP
+	PUSHQ	DX	// ctxt argument
 	CALL	runtime·newstack(SB)
 	MOVQ	$0, 0x1003	// crash if newstack returns
+	POPQ	DX	// keep balance check happy
 	RET
 
 // morestack but not preserving ctxt.

commit 687d9d5d78f8a2d09b2052e73be0c83740e17fda
Author: Austin Clements <austin@google.com>
Date:   Thu Oct 13 10:44:57 2016 -0400

    runtime: print a message on bad morestack
    
    If morestack runs on the g0 or gsignal stack, it currently performs
    some abort operation that typically produces a signal (e.g., it does
    an INT $3 on x86). This is useful if you're running in a debugger, but
    if you're not, the runtime tries to trap this signal, which is likely
    to send the program into a deeper spiral of collapse and lead to very
    confusing diagnostic output.
    
    Help out people trying to debug without a debugger by making morestack
    print an informative message before blowing up.
    
    Change-Id: I2814c64509b137bfe20a00091d8551d18c2c4749
    Reviewed-on: https://go-review.googlesource.com/31133
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8d992188de..34da3bda9f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -331,13 +331,15 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	g_m(BX), BX
 	MOVQ	m_g0(BX), SI
 	CMPQ	g(CX), SI
-	JNE	2(PC)
+	JNE	3(PC)
+	CALL	runtime·badmorestackg0(SB)
 	INT	$3
 
 	// Cannot grow signal stack (m->gsignal).
 	MOVQ	m_gsignal(BX), SI
 	CMPQ	g(CX), SI
-	JNE	2(PC)
+	JNE	3(PC)
+	CALL	runtime·badmorestackgsignal(SB)
 	INT	$3
 
 	// Called from f.

commit decc3a0919dbd60c4e2214a9245576be250c50d7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Oct 10 16:35:22 2016 -0400

    remove confusing asm
    
    this instruction is a no op because the immediate is sign-extended to 64-bit,
    resulting in: "andq $-1, %rax". remove it -- it is unnecessary since wrmsr
    writes edx:eax to the MSR.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4fcfbb7d5b..6efcd190a6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -814,7 +814,6 @@ TEXT ·_trapret(SB), NOSPLIT, $0-8
 	MOVQ	IA32_GS_BASE, CX
 	POPQ	AX
 	MOVQ	AX, DX
-	ANDQ	$((1 << 32) - 1), AX
 	SHRQ	$32, DX
 	WRMSR
 

commit 15ab8373f959604fb52502ba7062c8b7ef953694
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Sep 30 12:34:11 2016 -0400

    non-overlapping MSI and IRQ ranges; interrupt cleanup

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 191c3f645c..4fcfbb7d5b 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -667,17 +667,6 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	RET
 // pops are to silence plan9 assembler warnings
 
-#define IH_IRQ(num, fn)			\
-TEXT fn(SB), NOSPLIT, $0-0;		\
-	PUSHQ	$0;			\
-	PUSHQ	$(32 + num);		\
-	JMP	alltraps(SB);		\
-	BYTE	$0xeb;			\
-	BYTE	$0xfe;			\
-	POPQ	AX;			\
-	POPQ	AX;			\
-	RET
-
 #define IH_EC(num, fn)			\
 TEXT fn(SB), NOSPLIT, $0-0;		\
 	PUSHQ	$num;			\
@@ -708,40 +697,47 @@ IH_EC  (17,·Xac )
 IH_NOEC(18,·Xmc )
 IH_NOEC(19,·Xfp )
 IH_NOEC(20,·Xve )
+
+// irqs vectors 32-55
 IH_NOEC(32,·Xtimer )
-IH_NOEC(48,·Xspur )
-IH_NOEC(49,·Xyield )
-IH_NOEC(64,·Xsyscall )
+IH_NOEC(33,·Xirq1 )
+IH_NOEC(34,·Xirq2 )
+IH_NOEC(35,·Xirq3 )
+IH_NOEC(36,·Xirq4 )
+IH_NOEC(37,·Xirq5 )
+IH_NOEC(38,·Xirq6 )
+IH_NOEC(39,·Xirq7 )
+IH_NOEC(40,·Xirq8 )
+IH_NOEC(41,·Xirq9 )
+IH_NOEC(42,·Xirq10 )
+IH_NOEC(43,·Xirq11 )
+IH_NOEC(44,·Xirq12 )
+IH_NOEC(45,·Xirq13 )
+IH_NOEC(46,·Xirq14 )
+IH_NOEC(47,·Xirq15 )
+IH_NOEC(48,·Xirq16 )
+IH_NOEC(49,·Xirq17 )
+IH_NOEC(50,·Xirq18 )
+IH_NOEC(51,·Xirq19 )
+IH_NOEC(52,·Xirq20 )
+IH_NOEC(53,·Xirq21 )
+IH_NOEC(54,·Xirq22 )
+IH_NOEC(55,·Xirq23 )
+
+// MSI interrupts 56-63
+IH_NOEC(56,·Xmsi0 )
+IH_NOEC(57,·Xmsi1 )
+IH_NOEC(58,·Xmsi2 )
+IH_NOEC(59,·Xmsi3 )
+IH_NOEC(60,·Xmsi4 )
+IH_NOEC(61,·Xmsi5 )
+IH_NOEC(62,·Xmsi6 )
+IH_NOEC(63,·Xmsi7 )
+
+IH_NOEC(64,·Xspur )
 IH_NOEC(70,·Xtlbshoot )
-IH_NOEC(71,·Xsigret )
 IH_NOEC(72,·Xperfmask )
 
-// irqs
-IH_IRQ( 0,·Xirq0 )
-IH_IRQ( 1,·Xirq1 )
-IH_IRQ( 2,·Xirq2 )
-IH_IRQ( 3,·Xirq3 )
-IH_IRQ( 4,·Xirq4 )
-IH_IRQ( 5,·Xirq5 )
-IH_IRQ( 6,·Xirq6 )
-IH_IRQ( 7,·Xirq7 )
-IH_IRQ( 8,·Xirq8 )
-IH_IRQ( 9,·Xirq9 )
-IH_IRQ(10,·Xirq10 )
-IH_IRQ(11,·Xirq11 )
-IH_IRQ(12,·Xirq12 )
-IH_IRQ(13,·Xirq13 )
-IH_IRQ(14,·Xirq14 )
-IH_IRQ(15,·Xirq15 )
-IH_IRQ(16,·Xirq16 )
-IH_IRQ(17,·Xirq17 )
-IH_IRQ(18,·Xirq18 )
-IH_IRQ(19,·Xirq19 )
-IH_IRQ(20,·Xirq20 )
-IH_IRQ(21,·Xirq21 )
-IH_IRQ(22,·Xirq22 )
-IH_IRQ(23,·Xirq23 )
-
 #define IA32_FS_BASE		$0xc0000100
 #define IA32_GS_BASE		$0xc0000101
 #define IA32_SYSENTER_EIP	$0x176

commit d211c2d3774d78173e004f0ffb1e2eae9ae19706
Author: Austin Clements <austin@google.com>
Date:   Thu Sep 22 17:02:22 2016 -0400

    runtime: implement getcallersp in Go
    
    This makes it possible to inline getcallersp. getcallersp is on the
    hot path of defers, so this slightly speeds up defer:
    
    name           old time/op  new time/op  delta
    Defer-4        78.3ns ± 2%  75.1ns ± 1%  -4.00%   (p=0.000 n=9+8)
    
    Updates #14939.
    
    Change-Id: Icc1cc4cd2f0a81fc4c8344432d0b2e783accacdd
    Reviewed-on: https://go-review.googlesource.com/29655
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Run-TryBot: Austin Clements <austin@google.com>
    Reviewed-by: David Crawshaw <crawshaw@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 488c34a233..8d992188de 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -825,11 +825,6 @@ setbar:
 	CALL	runtime·setNextBarrierPC(SB)
 	RET
 
-TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
-	MOVQ	argp+0(FP), AX
-	MOVQ	AX, ret+8(FP)
-	RET
-
 // func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	CMPB	runtime·lfenceBeforeRdtsc(SB), $1

commit 682404b7fb59f9cbe186b27bb4b284594d58d80b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Sep 13 12:14:32 2016 -0400

    Store32() to guarantee 32bit writes without lock prefix
    
    all of the storing functions in go's atomic package use xchg, but asserting
    lock while storing to memory mapped registers seems like a bad idea.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index acb3471ffd..191c3f645c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -419,6 +419,13 @@ TEXT ·invlpg(SB), NOSPLIT, $0-8
 	INVLPG	(AX)
 	RET
 
+// Used to guarantee 32bit writes without the lock prefix
+TEXT ·Store32(SB), NOSPLIT, $0-16
+	MOVQ	addr+0(FP), AX
+	MOVL	v+8(FP), DX
+	MOVL	DX, (AX)
+	RET
+
 // void lidt(pdesc_t);
 TEXT ·lidt(SB), NOSPLIT, $0-16
 	// lidtq 8(%rsp)

commit 0cff219c1279cb76f042004bffcefba0a169cb67
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Thu Apr 28 17:39:55 2016 +0300

    strings: use AVX2 for Index if available
    
    IndexHard4-4      1.50ms ± 2%  0.71ms ± 0%  -52.36%  (p=0.000 n=20+19)
    
    This also fixes a bug, that caused a string of length 16 to use
    two 8-byte comparisons instead of one 16-byte. And adds a test for
    cases when partial_match fails.
    
    Change-Id: I1ee8fc4e068bb36c95c45de78f067c822c0d9df0
    Reviewed-on: https://go-review.googlesource.com/22551
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index c9d6b90d80..488c34a233 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1695,6 +1695,16 @@ big_loop_avx2_exit:
 	JMP loop
 
 
+TEXT strings·supportAVX2(SB),NOSPLIT,$0-1
+	MOVBLZX runtime·support_avx2(SB), AX
+	MOVB AX, ret+0(FP)
+	RET
+
+TEXT bytes·supportAVX2(SB),NOSPLIT,$0-1
+	MOVBLZX runtime·support_avx2(SB), AX
+	MOVB AX, ret+0(FP)
+	RET
+
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
 	// We want len in DX and AX, because PCMPESTRI implicitly consumes them
@@ -1809,7 +1819,7 @@ loop8:
 	JB loop8
 	JMP fail
 _9_or_more:
-	CMPQ AX, $16
+	CMPQ AX, $15
 	JA   _16_or_more
 	LEAQ 1(DI)(DX*1), DX
 	SUBQ AX, DX
@@ -1833,7 +1843,7 @@ partial_success9to15:
 	JMP fail
 _16_or_more:
 	CMPQ AX, $16
-	JA   _17_to_31
+	JA   _17_or_more
 	MOVOU (BP), X1
 	LEAQ -15(DI)(DX*1), DX
 loop16:
@@ -1846,7 +1856,9 @@ loop16:
 	CMPQ DI,DX
 	JB loop16
 	JMP fail
-_17_to_31:
+_17_or_more:
+	CMPQ AX, $31
+	JA   _32_or_more
 	LEAQ 1(DI)(DX*1), DX
 	SUBQ AX, DX
 	MOVOU -16(BP)(AX*1), X0
@@ -1870,9 +1882,56 @@ partial_success17to31:
 	ADDQ $1,DI
 	CMPQ DI,DX
 	JB loop17to31
+	JMP fail
+// We can get here only when AVX2 is enabled and cutoff for indexShortStr is set to 63
+// So no need to check cpuid
+_32_or_more:
+	CMPQ AX, $32
+	JA   _33_to_63
+	VMOVDQU (BP), Y1
+	LEAQ -31(DI)(DX*1), DX
+loop32:
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPMOVMSKB Y3, SI
+	CMPL  SI, $0xffffffff
+	JE   success_avx2
+	ADDQ $1,DI
+	CMPQ DI,DX
+	JB loop32
+	JMP fail_avx2
+_33_to_63:
+	LEAQ 1(DI)(DX*1), DX
+	SUBQ AX, DX
+	VMOVDQU -32(BP)(AX*1), Y0
+	VMOVDQU (BP), Y1
+loop33to63:
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPMOVMSKB Y3, SI
+	CMPL  SI, $0xffffffff
+	JE   partial_success33to63
+	ADDQ $1,DI
+	CMPQ DI,DX
+	JB loop33to63
+	JMP fail_avx2
+partial_success33to63:
+	VMOVDQU -32(AX)(DI*1), Y3
+	VPCMPEQB Y0, Y3, Y4
+	VPMOVMSKB Y4, SI
+	CMPL  SI, $0xffffffff
+	JE success_avx2
+	ADDQ $1,DI
+	CMPQ DI,DX
+	JB loop33to63
+fail_avx2:
+	VZEROUPPER
 fail:
 	MOVQ $-1, (R11)
 	RET
+success_avx2:
+	VZEROUPPER
+	JMP success
 sse42:
 	MOVL runtime·cpuid_ecx(SB), CX
 	ANDL $0x100000, CX

commit 44f1854c9dc82d8dba415ef102e93896d57c2c0d
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Thu Apr 28 17:34:24 2016 +0300

    bytes: Use the same algorithm as strings for Index
    
    name                     old time/op    new time/op      delta
    IndexByte32-48             9.05ns ± 7%      9.59ns ±11%     +5.93%  (p=0.001 n=19+20)
    IndexByte4K-48              118ns ± 4%       122ns ± 8%     +3.52%  (p=0.002 n=19+19)
    IndexByte4M-48              172µs ±13%       188µs ±12%     +9.49%  (p=0.000 n=20+20)
    IndexByte64M-48            8.00ms ±14%      8.05ms ±23%       ~     (p=0.799 n=20+20)
    IndexBytePortable32-48     41.7ns ±15%      42.5ns ±12%       ~     (p=0.372 n=20+20)
    IndexBytePortable4K-48     3.08µs ±16%      3.26µs ±10%     +5.77%  (p=0.018 n=20+20)
    IndexBytePortable4M-48     3.12ms ±17%      3.20ms ±10%       ~     (p=0.157 n=20+20)
    IndexBytePortable64M-48    54.0ms ±14%      55.3ms ±14%       ~     (p=0.640 n=20+20)
    Index32-48                  230ns ±12%        46ns ± 6%    -79.87%  (p=0.000 n=20+19)
    Index4K-48                 43.2µs ± 9%       3.2µs ±12%    -92.58%  (p=0.000 n=19+20)
    Index4M-48                 44.4ms ± 7%       3.3ms ±13%    -92.59%  (p=0.000 n=19+20)
    Index64M-48                 714ms ±10%        56ms ± 8%    -92.22%  (p=0.000 n=19+19)
    IndexEasy32-48             52.7ns ±10%      31.0ns ±11%    -41.21%  (p=0.000 n=20+20)
    IndexEasy4K-48              139ns ± 5%      1598ns ± 6%  +1046.37%  (p=0.000 n=19+19)
    IndexEasy4M-48              179µs ± 8%      1674µs ±10%   +834.31%  (p=0.000 n=19+20)
    IndexEasy64M-48            8.56ms ±10%     27.82ms ±16%   +225.14%  (p=0.000 n=19+20)
    
    name                     old speed      new speed        delta
    IndexByte32-48           3.52GB/s ± 7%    3.35GB/s ±11%     -4.99%  (p=0.001 n=20+20)
    IndexByte4K-48           34.5GB/s ± 7%    33.2GB/s ±10%     -3.67%  (p=0.002 n=20+20)
    IndexByte4M-48           24.6GB/s ±14%    22.4GB/s ±14%     -8.73%  (p=0.000 n=20+20)
    IndexByte64M-48          8.42GB/s ±16%    8.42GB/s ±19%       ~     (p=0.799 n=20+20)
    IndexBytePortable32-48    770MB/s ±13%     756MB/s ±11%       ~     (p=0.383 n=20+20)
    IndexBytePortable4K-48   1.34GB/s ±14%    1.26GB/s ±10%     -5.76%  (p=0.018 n=20+20)
    IndexBytePortable4M-48   1.35GB/s ±15%    1.31GB/s ±11%       ~     (p=0.157 n=20+20)
    IndexBytePortable64M-48  1.25GB/s ±16%    1.22GB/s ±13%       ~     (p=0.640 n=20+20)
    Index32-48                138MB/s ± 8%     687MB/s ± 8%   +398.57%  (p=0.000 n=19+20)
    Index4K-48               94.9MB/s ± 9%  1280.5MB/s ±11%  +1249.11%  (p=0.000 n=19+20)
    Index4M-48               94.6MB/s ± 7%  1278.5MB/s ±12%  +1250.99%  (p=0.000 n=19+20)
    Index64M-48              94.2MB/s ±10%  1210.9MB/s ± 8%  +1185.04%  (p=0.000 n=19+19)
    IndexEasy32-48            608MB/s ±10%    1035MB/s ±10%    +70.15%  (p=0.000 n=20+20)
    IndexEasy4K-48           29.3GB/s ± 6%     2.6GB/s ± 6%    -91.24%  (p=0.000 n=19+19)
    IndexEasy4M-48           23.3GB/s ±10%     2.5GB/s ± 9%    -89.23%  (p=0.000 n=20+20)
    IndexEasy64M-48          7.86GB/s ±11%    2.42GB/s ±14%    -69.18%  (p=0.000 n=19+20)
    
    Change-Id: Ia191f0a6ca80e113397d9ed98d25f195768b65bc
    Reviewed-on: https://go-review.googlesource.com/22550
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f44fc1166a..c9d6b90d80 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1695,13 +1695,31 @@ big_loop_avx2_exit:
 	JMP loop
 
 
-// TODO: Also use this in bytes.Index
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
 	// We want len in DX and AX, because PCMPESTRI implicitly consumes them
 	MOVQ s_len+8(FP), DX
 	MOVQ c+16(FP), BP
 	MOVQ c_len+24(FP), AX
+	MOVQ DI, R10
+	LEAQ ret+32(FP), R11
+	JMP  runtime·indexShortStr(SB)
+
+TEXT bytes·indexShortStr(SB),NOSPLIT,$0-56
+	MOVQ s+0(FP), DI
+	MOVQ s_len+8(FP), DX
+	MOVQ c+24(FP), BP
+	MOVQ c_len+32(FP), AX
+	MOVQ DI, R10
+	LEAQ ret+48(FP), R11
+	JMP  runtime·indexShortStr(SB)
+
+// AX: length of string, that we are searching for
+// DX: length of string, in which we are searching
+// DI: pointer to string, in which we are searching
+// BP: pointer to string, that we are searching for
+// R11: address, where to put return value
+TEXT runtime·indexShortStr(SB),NOSPLIT,$0
 	CMPQ AX, DX
 	JA fail
 	CMPQ DX, $16
@@ -1853,7 +1871,7 @@ partial_success17to31:
 	CMPQ DI,DX
 	JB loop17to31
 fail:
-	MOVQ $-1, ret+32(FP)
+	MOVQ $-1, (R11)
 	RET
 sse42:
 	MOVL runtime·cpuid_ecx(SB), CX
@@ -1893,8 +1911,8 @@ loop_sse42:
 sse42_success:
 	ADDQ CX, DI
 success:
-	SUBQ s+0(FP), DI
-	MOVQ DI, ret+32(FP)
+	SUBQ R10, DI
+	MOVQ DI, (R11)
 	RET
 
 

commit 2b74de3ed91c495d63868acef0471b0286e7b432
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Tue Jun 28 09:22:46 2016 -0700

    runtime: rename fastrand1 to fastrand
    
    Change-Id: I37706ff0a3486827c5b072c95ad890ea87ede847
    Reviewed-on: https://go-review.googlesource.com/28210
    Run-TryBot: Josh Bleecher Snyder <josharian@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3383bbe446..f44fc1166a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2052,7 +2052,7 @@ eqret:
 	MOVB	$0, ret+48(FP)
 	RET
 
-TEXT runtime·fastrand1(SB), NOSPLIT, $0-4
+TEXT runtime·fastrand(SB), NOSPLIT, $0-4
 	get_tls(CX)
 	MOVQ	g(CX), AX
 	MOVQ	g_m(AX), AX

commit 0dd6398408ce9631b6ebf53beead93b3f98d3754
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Aug 30 14:01:36 2016 -0400

    put all IO APIC interrupt vectors in IDT

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 919b92a861..acb3471ffd 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -710,7 +710,7 @@ IH_NOEC(71,·Xsigret )
 IH_NOEC(72,·Xperfmask )
 
 // irqs
-// irq0 is Xtimer
+IH_IRQ( 0,·Xirq0 )
 IH_IRQ( 1,·Xirq1 )
 IH_IRQ( 2,·Xirq2 )
 IH_IRQ( 3,·Xirq3 )
@@ -726,6 +726,14 @@ IH_IRQ(12,·Xirq12 )
 IH_IRQ(13,·Xirq13 )
 IH_IRQ(14,·Xirq14 )
 IH_IRQ(15,·Xirq15 )
+IH_IRQ(16,·Xirq16 )
+IH_IRQ(17,·Xirq17 )
+IH_IRQ(18,·Xirq18 )
+IH_IRQ(19,·Xirq19 )
+IH_IRQ(20,·Xirq20 )
+IH_IRQ(21,·Xirq21 )
+IH_IRQ(22,·Xirq22 )
+IH_IRQ(23,·Xirq23 )
 
 #define IA32_FS_BASE		$0xc0000100
 #define IA32_GS_BASE		$0xc0000101

commit 71ab9fa312f8266379dbb358b9ee9303cde7bd6b
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Mon Jul 11 16:05:57 2016 -0700

    all: fix assembly vet issues
    
    Add missing function prototypes.
    Fix function prototypes.
    Use FP references instead of SP references.
    Fix variable names.
    Update comments.
    Clean up whitespace. (Not for vet.)
    
    All fairly minor fixes to make vet happy.
    
    Updates #11041
    
    Change-Id: Ifab2cdf235ff61cdc226ab1d84b8467b5ac9446c
    Reviewed-on: https://go-review.googlesource.com/27713
    Run-TryBot: Josh Bleecher Snyder <josharian@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6103d54ba6..3383bbe446 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1340,15 +1340,15 @@ eq:
 // See runtime_test.go:eqstring_generic for
 // equivalent Go code.
 TEXT runtime·eqstring(SB),NOSPLIT,$0-33
-	MOVQ	s1str+0(FP), SI
-	MOVQ	s2str+16(FP), DI
+	MOVQ	s1_base+0(FP), SI
+	MOVQ	s2_base+16(FP), DI
 	CMPQ	SI, DI
 	JEQ	eq
-	MOVQ	s1len+8(FP), BX
-	LEAQ	v+32(FP), AX
+	MOVQ	s1_len+8(FP), BX
+	LEAQ	ret+32(FP), AX
 	JMP	runtime·memeqbody(SB)
 eq:
-	MOVB	$1, v+32(FP)
+	MOVB	$1, ret+32(FP)
 	RET
 
 // a in SI

commit eebdc9f18d967944e8f9a7841020f5e71c8c6852
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jun 10 23:40:41 2016 -0400

    fix pmap freeing
    
    increase page map reference count when they are loaded/unloaded on each CPU
    too, that way we can safely free them.
    
    it's a real shame that allocating the page maps via the GC is prohibitively
    expensive for fork-intensive benchmarks; it was really convenient.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4fe8632c36..919b92a861 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -197,7 +197,7 @@ DATA	fakeargv+16(SB)/8,$0
 GLOBL	fakeargv(SB),RODATA,$24
 
 TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
-	MOVL	DI, ·p_kpmap(SB)
+	MOVL	DI, ·P_kpmap(SB)
 	MOVL	SI, ·pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 	CALL	runtime·sc_setup(SB)

commit c83e6f50d983d81166d21736ff9ab0ad2182f0fa
Author: Keith Randall <khr@golang.org>
Date:   Thu May 26 08:56:49 2016 -0700

    runtime: aeshash, xor seed in earlier
    
    Instead of doing:
    
    x = input
    one round of aes on x
    x ^= seed
    two rounds of aes on x
    
    Do:
    
    x = input
    x ^= seed
    three rounds of aes on x
    
    This change provides some additional seed-dependent scrambling
    which should help prevent collisions.
    
    Change-Id: I02c774d09c2eb6917cf861513816a1024a9b65d7
    Reviewed-on: https://go-review.googlesource.com/23577
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f9932cd434..6103d54ba6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -914,8 +914,9 @@ aes0to15:
 	MOVQ	$masks<>(SB), AX
 	PAND	(AX)(CX*8), X1
 final1:
-	AESENC	X0, X1	// scramble input, xor in seed
-	AESENC	X1, X1  // scramble combo 2 times
+	PXOR	X0, X1	// xor data with seed
+	AESENC	X1, X1	// scramble combo 3 times
+	AESENC	X1, X1
 	AESENC	X1, X1
 	MOVQ	X1, (DX)
 	RET
@@ -949,9 +950,13 @@ aes17to32:
 	MOVOU	(AX), X2
 	MOVOU	-16(AX)(CX*1), X3
 
+	// xor with seed
+	PXOR	X0, X2
+	PXOR	X1, X3
+
 	// scramble 3 times
-	AESENC	X0, X2
-	AESENC	X1, X3
+	AESENC	X2, X2
+	AESENC	X3, X3
 	AESENC	X2, X2
 	AESENC	X3, X3
 	AESENC	X2, X2
@@ -977,11 +982,16 @@ aes33to64:
 	MOVOU	16(AX), X5
 	MOVOU	-32(AX)(CX*1), X6
 	MOVOU	-16(AX)(CX*1), X7
+
+	PXOR	X0, X4
+	PXOR	X1, X5
+	PXOR	X2, X6
+	PXOR	X3, X7
 	
-	AESENC	X0, X4
-	AESENC	X1, X5
-	AESENC	X2, X6
-	AESENC	X3, X7
+	AESENC	X4, X4
+	AESENC	X5, X5
+	AESENC	X6, X6
+	AESENC	X7, X7
 	
 	AESENC	X4, X4
 	AESENC	X5, X5
@@ -1032,17 +1042,17 @@ aes65to128:
 	MOVOU	-32(AX)(CX*1), X14
 	MOVOU	-16(AX)(CX*1), X15
 
-	// scramble data, xor in seed
-	AESENC	X0, X8
-	AESENC	X1, X9
-	AESENC	X2, X10
-	AESENC	X3, X11
-	AESENC	X4, X12
-	AESENC	X5, X13
-	AESENC	X6, X14
-	AESENC	X7, X15
+	// xor with seed
+	PXOR	X0, X8
+	PXOR	X1, X9
+	PXOR	X2, X10
+	PXOR	X3, X11
+	PXOR	X4, X12
+	PXOR	X5, X13
+	PXOR	X6, X14
+	PXOR	X7, X15
 
-	// scramble twice
+	// scramble 3 times
 	AESENC	X8, X8
 	AESENC	X9, X9
 	AESENC	X10, X10
@@ -1051,7 +1061,16 @@ aes65to128:
 	AESENC	X13, X13
 	AESENC	X14, X14
 	AESENC	X15, X15
-	
+
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
+
 	AESENC	X8, X8
 	AESENC	X9, X9
 	AESENC	X10, X10
@@ -1105,21 +1124,31 @@ aes129plus:
 	MOVOU	-32(AX)(CX*1), X14
 	MOVOU	-16(AX)(CX*1), X15
 
-	// scramble input once, xor in seed
-	AESENC	X0, X8
-	AESENC	X1, X9
-	AESENC	X2, X10
-	AESENC	X3, X11
-	AESENC	X4, X12
-	AESENC	X5, X13
-	AESENC	X6, X14
-	AESENC	X7, X15
+	// xor in seed
+	PXOR	X0, X8
+	PXOR	X1, X9
+	PXOR	X2, X10
+	PXOR	X3, X11
+	PXOR	X4, X12
+	PXOR	X5, X13
+	PXOR	X6, X14
+	PXOR	X7, X15
 	
 	// compute number of remaining 128-byte blocks
 	DECQ	CX
 	SHRQ	$7, CX
 	
 aesloop:
+	// scramble state
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
+
 	// scramble state, xor in a block
 	MOVOU	(AX), X0
 	MOVOU	16(AX), X1
@@ -1138,7 +1167,11 @@ aesloop:
 	AESENC	X6, X14
 	AESENC	X7, X15
 
-	// scramble state
+	ADDQ	$128, AX
+	DECQ	CX
+	JNE	aesloop
+
+	// 3 more scrambles to finish
 	AESENC	X8, X8
 	AESENC	X9, X9
 	AESENC	X10, X10
@@ -1147,12 +1180,6 @@ aesloop:
 	AESENC	X13, X13
 	AESENC	X14, X14
 	AESENC	X15, X15
-
-	ADDQ	$128, AX
-	DECQ	CX
-	JNE	aesloop
-
-	// 2 more scrambles to finish
 	AESENC	X8, X8
 	AESENC	X9, X9
 	AESENC	X10, X10

commit 429bbf331247ef598802a94a23670bfe1cf61d6f
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Wed May 25 16:33:19 2016 +0300

    strings: fix and reenable amd64 Index for 17-31 byte strings
    
    Fixes #15689
    
    Change-Id: I56d0103738cc35cd5bc5e77a0e0341c0dd55530e
    Reviewed-on: https://go-review.googlesource.com/23440
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Nigel Tao <nigeltao@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e50c443044..f9932cd434 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1787,7 +1787,7 @@ partial_success9to15:
 	JB loop9to15
 	JMP fail
 _16_or_more:
-	CMPQ AX, $17
+	CMPQ AX, $16
 	JA   _17_to_31
 	MOVOU (BP), X1
 	LEAQ -15(DI)(DX*1), DX
@@ -1801,7 +1801,6 @@ loop16:
 	CMPQ DI,DX
 	JB loop16
 	JMP fail
-//TODO: the code below is wrong.  Fix it.  See #15679.
 _17_to_31:
 	LEAQ 1(DI)(DX*1), DX
 	SUBQ AX, DX

commit b92f4238790c590168e7dae03165d75deb89fe41
Author: Austin Clements <austin@google.com>
Date:   Wed May 25 20:56:56 2016 -0400

    runtime: unwind BP in jmpdefer to match SP unwind
    
    The irregular calling convention for defers currently incorrectly
    manages the BP if frame pointers are enabled. Specifically, jmpdefer
    manipulates the SP as if its own caller, deferreturn, had returned.
    However, it does not manipulate the BP to match. As a result, when a
    BP-based traceback happens during a deferred function call, it unwinds
    to the function that performed the defer and then thinks that function
    called itself in an infinite regress.
    
    Fix this by making jmpdefer manipulate the BP as if deferreturn had
    actually returned.
    
    Fixes #12968.
    
    Updates #15840.
    
    Change-Id: Ic9cc7c863baeaf977883ed0c25a7e80e592cf066
    Reviewed-on: https://go-review.googlesource.com/23457
    Reviewed-by: Russ Cox <rsc@golang.org>
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d6e5494180..e50c443044 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -526,6 +526,7 @@ TEXT runtime·jmpdefer(SB), NOSPLIT, $0-16
 	MOVQ	fv+0(FP), DX	// fn
 	MOVQ	argp+8(FP), BX	// caller sp
 	LEAQ	-8(BX), SP	// caller sp after CALL
+	MOVQ	-8(SP), BP	// restore BP as if deferreturn returned (harmless if framepointers not in use)
 	SUBQ	$5, (SP)	// return to CALL again
 	MOVQ	0(DX), BX
 	JMP	BX	// but first run the deferred function

commit 0bc14f57ec7e5518af711a64103ca2ac72f19a6e
Author: Keith Randall <khr@golang.org>
Date:   Sat May 14 17:33:23 2016 -0700

    strings: fix Contains on amd64
    
    The 17-31 byte code is broken.  Disabled it.
    
    Added a bunch of tests to at least cover the cases
    in indexShortStr.  I'll channel Brad and wonder why
    this CL ever got in without any tests.
    
    Fixes #15679
    
    Change-Id: I84a7b283a74107db865b9586c955dcf5f2d60161
    Reviewed-on: https://go-review.googlesource.com/23106
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6cd31f951b..d6e5494180 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1800,6 +1800,7 @@ loop16:
 	CMPQ DI,DX
 	JB loop16
 	JMP fail
+//TODO: the code below is wrong.  Fix it.  See #15679.
 _17_to_31:
 	LEAQ 1(DI)(DX*1), DX
 	SUBQ AX, DX

commit cc468922f655cdad22990fbbaf5fa685491ff5c0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu May 5 23:42:11 2016 -0400

    expose dTLB event in time(3) PMU profiling

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 5c547b12bc..4fe8632c36 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -956,6 +956,7 @@ TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	// fastret or iret?
 	MOVB	fastret+8(FP), AX
 	CMPB	AX, $0
+	// swap for better branch prediction?
 	JNE	syscallreturn
 	// do full state restore
 	PUSHQ	R9

commit 5f9a870bf1bf461ca3609502608b12cc4aab189a
Author: Ian Lance Taylor <iant@golang.org>
Date:   Wed Apr 27 14:18:29 2016 -0700

    cmd/cgo, runtime, runtime/cgo: use cgo context function
    
    Add support for the context function set by runtime.SetCgoTraceback.
    The context function was added in CL 17761, without support.
    This CL is the support.
    
    This CL has not been tested for real C code, as a working context
    function for C code requires unwind support that does not seem to exist.
    I wanted to get the CL out before the freeze.
    
    I apologize for the length of this CL.  It's mostly plumbing, but
    unfortunately the plumbing is processor-specific.
    
    Change-Id: I8ce11a0de9b3dafcc29efd2649d776e93bff0e90
    Reviewed-on: https://go-review.googlesource.com/22508
    Reviewed-by: Austin Clements <austin@google.com>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cdda29f347..6cd31f951b 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -622,23 +622,25 @@ nosave:
 	MOVL	AX, ret+16(FP)
 	RET
 
-// cgocallback(void (*fn)(void*), void *frame, uintptr framesize)
+// cgocallback(void (*fn)(void*), void *frame, uintptr framesize, uintptr ctxt)
 // Turn the fn into a Go func (by taking its address) and call
 // cgocallback_gofunc.
-TEXT runtime·cgocallback(SB),NOSPLIT,$24-24
+TEXT runtime·cgocallback(SB),NOSPLIT,$32-32
 	LEAQ	fn+0(FP), AX
 	MOVQ	AX, 0(SP)
 	MOVQ	frame+8(FP), AX
 	MOVQ	AX, 8(SP)
 	MOVQ	framesize+16(FP), AX
 	MOVQ	AX, 16(SP)
+	MOVQ	ctxt+24(FP), AX
+	MOVQ	AX, 24(SP)
 	MOVQ	$runtime·cgocallback_gofunc(SB), AX
 	CALL	AX
 	RET
 
-// cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize)
+// cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize, uintptr ctxt)
 // See cgocall.go for more details.
-TEXT ·cgocallback_gofunc(SB),NOSPLIT,$8-24
+TEXT ·cgocallback_gofunc(SB),NOSPLIT,$16-32
 	NO_LOCAL_POINTERS
 
 	// If g is nil, Go did not create the current thread.
@@ -706,7 +708,7 @@ havem:
 	// so that the traceback will seamlessly trace back into
 	// the earlier calls.
 	//
-	// In the new goroutine, 0(SP) holds the saved R8.
+	// In the new goroutine, 8(SP) holds the saved R8.
 	MOVQ	m_curg(BX), SI
 	MOVQ	SI, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(SI), DI  // prepare stack as DI
@@ -714,14 +716,16 @@ havem:
 	MOVQ	BX, -8(DI)
 	// Compute the size of the frame, including return PC and, if
 	// GOEXPERIMENT=framepointer, the saved based pointer
+	MOVQ	ctxt+24(FP), BX
 	LEAQ	fv+0(FP), AX
 	SUBQ	SP, AX
 	SUBQ	AX, DI
 	MOVQ	DI, SP
 
-	MOVQ	R8, 0(SP)
+	MOVQ	R8, 8(SP)
+	MOVQ	BX, 0(SP)
 	CALL	runtime·cgocallbackg(SB)
-	MOVQ	0(SP), R8
+	MOVQ	8(SP), R8
 
 	// Compute the size of the frame again. FP and SP have
 	// completely different values here than they did above,

commit 6b02a1924725688b4d264065454ac5287fbed535
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Thu Apr 21 18:24:12 2016 +0300

    strings: use SSE4.2 in strings.Index on AMD64
    
    Use PCMPESTRI instruction if available.
    
    Index-4              21.1ns ± 0%  21.1ns ± 0%     ~     (all samples are equal)
    IndexHard1-4          395µs ± 0%   105µs ± 0%  -73.53%        (p=0.000 n=19+20)
    IndexHard2-4          300µs ± 0%   147µs ± 0%  -51.11%        (p=0.000 n=19+20)
    IndexHard3-4          665µs ± 0%   665µs ± 0%     ~           (p=0.942 n=16+19)
    
    Change-Id: I4f66794164740a2b939eb1c78934e2390b489064
    Reviewed-on: https://go-review.googlesource.com/22337
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 83db4d3e81..cdda29f347 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1666,122 +1666,126 @@ big_loop_avx2_exit:
 // TODO: Also use this in bytes.Index
 TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), DI
-	MOVQ s_len+8(FP), CX
-	MOVQ c+16(FP), AX
-	MOVQ c_len+24(FP), BX
-	CMPQ BX, CX
+	// We want len in DX and AX, because PCMPESTRI implicitly consumes them
+	MOVQ s_len+8(FP), DX
+	MOVQ c+16(FP), BP
+	MOVQ c_len+24(FP), AX
+	CMPQ AX, DX
 	JA fail
-	CMPQ BX, $2
+	CMPQ DX, $16
+	JAE sse42
+no_sse42:
+	CMPQ AX, $2
 	JA   _3_or_more
-	MOVW (AX), AX
-	LEAQ -1(DI)(CX*1), CX
+	MOVW (BP), BP
+	LEAQ -1(DI)(DX*1), DX
 loop2:
 	MOVW (DI), SI
-	CMPW SI,AX
+	CMPW SI,BP
 	JZ success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop2
 	JMP fail
 _3_or_more:
-	CMPQ BX, $3
+	CMPQ AX, $3
 	JA   _4_or_more
-	MOVW 1(AX), DX
-	MOVW (AX), AX
-	LEAQ -2(DI)(CX*1), CX
+	MOVW 1(BP), BX
+	MOVW (BP), BP
+	LEAQ -2(DI)(DX*1), DX
 loop3:
 	MOVW (DI), SI
-	CMPW SI,AX
+	CMPW SI,BP
 	JZ   partial_success3
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop3
 	JMP fail
 partial_success3:
 	MOVW 1(DI), SI
-	CMPW SI,DX
+	CMPW SI,BX
 	JZ success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop3
 	JMP fail
 _4_or_more:
-	CMPQ BX, $4
+	CMPQ AX, $4
 	JA   _5_or_more
-	MOVL (AX), AX
-	LEAQ -3(DI)(CX*1), CX
+	MOVL (BP), BP
+	LEAQ -3(DI)(DX*1), DX
 loop4:
 	MOVL (DI), SI
-	CMPL SI,AX
+	CMPL SI,BP
 	JZ   success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop4
 	JMP fail
 _5_or_more:
-	CMPQ BX, $7
+	CMPQ AX, $7
 	JA   _8_or_more
-	LEAQ 1(DI)(CX*1), CX
-	SUBQ BX, CX
-	MOVL -4(AX)(BX*1), DX
-	MOVL (AX), AX
+	LEAQ 1(DI)(DX*1), DX
+	SUBQ AX, DX
+	MOVL -4(BP)(AX*1), BX
+	MOVL (BP), BP
 loop5to7:
 	MOVL (DI), SI
-	CMPL SI,AX
+	CMPL SI,BP
 	JZ   partial_success5to7
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop5to7
 	JMP fail
 partial_success5to7:
-	MOVL -4(BX)(DI*1), SI
-	CMPL SI,DX
+	MOVL -4(AX)(DI*1), SI
+	CMPL SI,BX
 	JZ success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop5to7
 	JMP fail
 _8_or_more:
-	CMPQ BX, $8
+	CMPQ AX, $8
 	JA   _9_or_more
-	MOVQ (AX), AX
-	LEAQ -7(DI)(CX*1), CX
+	MOVQ (BP), BP
+	LEAQ -7(DI)(DX*1), DX
 loop8:
 	MOVQ (DI), SI
-	CMPQ SI,AX
+	CMPQ SI,BP
 	JZ   success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop8
 	JMP fail
 _9_or_more:
-	CMPQ BX, $16
+	CMPQ AX, $16
 	JA   _16_or_more
-	LEAQ 1(DI)(CX*1), CX
-	SUBQ BX, CX
-	MOVQ -8(AX)(BX*1), DX
-	MOVQ (AX), AX
+	LEAQ 1(DI)(DX*1), DX
+	SUBQ AX, DX
+	MOVQ -8(BP)(AX*1), BX
+	MOVQ (BP), BP
 loop9to15:
 	MOVQ (DI), SI
-	CMPQ SI,AX
+	CMPQ SI,BP
 	JZ   partial_success9to15
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop9to15
 	JMP fail
 partial_success9to15:
-	MOVQ -8(BX)(DI*1), SI
-	CMPQ SI,DX
+	MOVQ -8(AX)(DI*1), SI
+	CMPQ SI,BX
 	JZ success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop9to15
 	JMP fail
 _16_or_more:
-	CMPQ BX, $16
+	CMPQ AX, $17
 	JA   _17_to_31
-	MOVOU (AX), X1
-	LEAQ -15(DI)(CX*1), CX
+	MOVOU (BP), X1
+	LEAQ -15(DI)(DX*1), DX
 loop16:
 	MOVOU (DI), X2
 	PCMPEQB X1, X2
@@ -1789,14 +1793,14 @@ loop16:
 	CMPQ  SI, $0xffff
 	JE   success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop16
 	JMP fail
 _17_to_31:
-	LEAQ 1(DI)(CX*1), CX
-	SUBQ BX, CX
-	MOVOU -16(AX)(BX*1), X0
-	MOVOU (AX), X1
+	LEAQ 1(DI)(DX*1), DX
+	SUBQ AX, DX
+	MOVOU -16(BP)(AX*1), X0
+	MOVOU (BP), X1
 loop17to31:
 	MOVOU (DI), X2
 	PCMPEQB X1,X2
@@ -1804,21 +1808,58 @@ loop17to31:
 	CMPQ  SI, $0xffff
 	JE   partial_success17to31
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop17to31
 	JMP fail
 partial_success17to31:
-	MOVOU -16(BX)(DI*1), X3
+	MOVOU -16(AX)(DI*1), X3
 	PCMPEQB X0, X3
 	PMOVMSKB X3, SI
 	CMPQ  SI, $0xffff
 	JE success
 	ADDQ $1,DI
-	CMPQ DI,CX
+	CMPQ DI,DX
 	JB loop17to31
 fail:
 	MOVQ $-1, ret+32(FP)
 	RET
+sse42:
+	MOVL runtime·cpuid_ecx(SB), CX
+	ANDL $0x100000, CX
+	JZ no_sse42
+	CMPQ AX, $12
+	// PCMPESTRI is slower than normal compare,
+	// so using it makes sense only if we advance 4+ bytes per compare
+	// This value was determined experimentally and is the ~same
+	// on Nehalem (first with SSE42) and Haswell.
+	JAE _9_or_more
+	LEAQ 16(BP), SI
+	TESTW $0xff0, SI
+	JEQ no_sse42
+	MOVOU (BP), X1
+	LEAQ -15(DI)(DX*1), SI
+	MOVQ $16, R9
+	SUBQ AX, R9 // We advance by 16-len(sep) each iteration, so precalculate it into R9
+loop_sse42:
+	// 0x0c means: unsigned byte compare (bits 0,1 are 00)
+	// for equality (bits 2,3 are 11)
+	// result is not masked or inverted (bits 4,5 are 00)
+	// and corresponds to first matching byte (bit 6 is 0)
+	PCMPESTRI $0x0c, (DI), X1
+	// CX == 16 means no match,
+	// CX > R9 means partial match at the end of the string,
+	// otherwise sep is at offset CX from X1 start
+	CMPQ CX, R9
+	JBE sse42_success
+	ADDQ R9, DI
+	CMPQ DI, SI
+	JB loop_sse42
+	PCMPESTRI $0x0c, -1(SI), X1
+	CMPQ CX, R9
+	JA fail
+	LEAQ -1(SI), DI
+sse42_success:
+	ADDQ CX, DI
 success:
 	SUBQ s+0(FP), DI
 	MOVQ DI, ret+32(FP)

commit 4354980481920ac314e3e3260cd4cce0665df681
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 22 23:17:02 2016 -0400

    reduce system call overhead
    
    avoid all {rd,wr}msrs during syscalls. there are several necesary steps to
    achieve this:
    
    1) rewrite the kernel binary to use %gs instead of %fs (which is defined as the
    TLS indirect pointer in the amd64 ELF TLS ABI), that way we don't need to
    context switch %fs on every syscall. i tried modifying the go runtime/compiler
    to simply use %gs, but the runtime still panic'ed and the build system requires
    running go binaries natively on the host platform. thus i was unable to build
    my modified runtime fully. instead, i wrote a small script to rewrite my kernel
    binary to use %gs while the runtime build system is left unchanged.
    
    2) since the kernel now uses %gs for TLS, store the per-CPU indirect pointer in
    IA32_KERNEL_GS_BASE MSR. now we simply use swapgs to load the per-CPU indirect
    pointer.
    
    3) expand trap frame to save/restore %gs since it is per thread now. context
    switches never save/restore %fs now; %fs is explicitly loaded in Userrun(). in
    order to avoid a rdmsr to detect whether %fs is correct, we shadow the %fs
    register in a per-CPU variable. thus checking %fs is a memory load instead of
    rdmsr.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0cf68e16dd..5c547b12bc 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -728,6 +728,7 @@ IH_IRQ(14,·Xirq14 )
 IH_IRQ(15,·Xirq15 )
 
 #define IA32_FS_BASE		$0xc0000100
+#define IA32_GS_BASE		$0xc0000101
 #define IA32_SYSENTER_EIP	$0x176
 
 TEXT wrfsb(SB), NOSPLIT, $0-8
@@ -770,6 +771,13 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	ORQ	DX, AX
 	PUSHQ	AX
 
+	// save gsbase
+	MOVQ	IA32_GS_BASE, CX
+	RDMSR
+	SHLQ	$32, DX
+	ORQ	DX, AX
+	PUSHQ	AX
+
 	MOVQ	SP, AX
 	PUSHQ	AX
 
@@ -791,14 +799,51 @@ TEXT ·_trapret(SB), NOSPLIT, $0-8
 				// threads[]
 	MOVQ	AX, SP
 
-	// restore fsbase
-	MOVQ	IA32_FS_BASE, CX
+	// restore gsbase
+	MOVQ	IA32_GS_BASE, CX
 	POPQ	AX
 	MOVQ	AX, DX
 	ANDQ	$((1 << 32) - 1), AX
 	SHRQ	$32, DX
 	WRMSR
 
+	// skip fsbase
+	POPQ	CX
+
+	POPQ	R15
+	POPQ	R14
+	POPQ	R13
+	POPQ	R12
+	POPQ	R11
+	POPQ	R10
+	POPQ	R9
+	POPQ	R8
+	POPQ	BP
+	POPQ	SI
+	POPQ	DI
+	POPQ	DX
+	POPQ	CX
+	POPQ	BX
+	POPQ	AX
+	// skip trapno and error code
+	ADDQ	$16, SP
+
+	// iretq
+	BYTE	$0x48
+	BYTE	$0xcf
+
+// identical to _trapret, but doesn't load gsbase from the trapframe
+TEXT ·_userret(SB), NOSPLIT, $0-8
+	MOVQ	tf+0(FP), AX	// tf is not on the callers stack frame, but in
+				// threads[]
+	MOVQ	AX, SP
+
+	// skip gsbase
+	POPQ	AX
+
+	// skip fsbase
+	POPQ	CX
+
 	POPQ	R15
 	POPQ	R14
 	POPQ	R13
@@ -831,7 +876,10 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 	CLI
 
 	// do hardware trap frame; get CPU's interrupt stack
-	MOVQ	16(GS), DX
+	SWAPGS
+	MOVQ	0(GS), DX
+	SWAPGS
+	MOVQ	16(DX), DX
 
 	// save rflags first
 	MOVQ	AX, -24(DX)
@@ -880,17 +928,17 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 
 	JMP	alltraps(SB)
 
-#define TFREGS		16
-#define TF_R13		(8*3)
-#define TF_R12		(8*4)
-#define TF_R8		(8*8)
-#define TF_RBP		(8*9)
-#define TF_RSI		(8*10)
-#define TF_RDI		(8*11)
-#define TF_RDX		(8*12)
-#define TF_RCX		(8*13)
-#define TF_RBX		(8*14)
-#define TF_RAX		(8*15)
+#define TFREGS		17
+#define TF_R13		(8*4)
+#define TF_R12		(8*5)
+#define TF_R8		(8*9)
+#define TF_RBP		(8*10)
+#define TF_RSI		(8*11)
+#define TF_RDI		(8*12)
+#define TF_RDX		(8*13)
+#define TF_RCX		(8*14)
+#define TF_RBX		(8*15)
+#define TF_RAX		(8*16)
 #define TF_RIP		(8*(TFREGS + 2))
 #define TF_RSP		(8*(TFREGS + 5))
 
@@ -900,7 +948,9 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	MOVQ	tf+0(FP), R9
 
+	SWAPGS
 	MOVQ	0(GS), AX
+	SWAPGS
 	MOVQ	SP, 0x20(AX)
 
 	// fastret or iret?
@@ -909,7 +959,7 @@ TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	JNE	syscallreturn
 	// do full state restore
 	PUSHQ	R9
-	CALL	·_trapret(SB)
+	CALL	·_userret(SB)
 	INT	$3
 
 syscallreturn:
@@ -935,7 +985,9 @@ syscallreturn:
 // are hand-coded.
 //_sysentry:
 TEXT ·_sysentry(SB), NOSPLIT, $0-0
+	SWAPGS
 	MOVQ	0(GS), SP
+	SWAPGS
 	MOVQ	0x20(SP), SP
 
 	// save user state in fake trapframe
@@ -976,6 +1028,18 @@ TEXT ·gs_null(SB), NOSPLIT, $8-0
 	POPQ	GS
 	RET
 
+// called exactly once by each CPU
+TEXT ·gs_set(SB), NOSPLIT, $0-16
+	SWAPGS
+	MOVQ	IA32_GS_BASE, CX
+	MOVQ	cpu+0(FP), AX
+	MOVQ	AX, DX
+	ANDQ	$((1 << 32) - 1), AX
+	SHRQ	$32, DX
+	WRMSR
+	SWAPGS
+	RET
+
 TEXT ·fs_null(SB), NOSPLIT, $8-0
 	XORQ	AX, AX
 	PUSHQ	AX
@@ -983,8 +1047,10 @@ TEXT ·fs_null(SB), NOSPLIT, $8-0
 	RET
 
 TEXT ·_Gscpu(SB), NOSPLIT, $0-8
+	SWAPGS
 	MOVQ	0(GS), AX
 	MOVQ	AX, ret+0(FP)
+	SWAPGS
 	RET
 
 /*

commit d3b976c14798556f5973821b59899b39dcb0a1a8
Merge: 9891544c69 e6305e3425
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 19 19:46:47 2016 -0400

    Merge branch 'master' into g1.6
    
    Conflicts:
            src/runtime/asm_amd64.s
            src/runtime/os_linux.go

commit e6305e34251d6b647ecf90efa038fa2cdec27756
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 19 19:40:27 2016 -0400

    use per-cpu GS to track sysenter stack
    
    that way we avoid using IA32_SYSENTER_ESP which requires wrmsr.
    
    significantly reduces syscall overhead.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3327ca0b40..f699083432 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -648,7 +648,6 @@ IH_IRQ(14,·Xirq14 )
 IH_IRQ(15,·Xirq15 )
 
 #define IA32_FS_BASE		$0xc0000100UL
-#define IA32_SYSENTER_ESP	$0x175UL
 #define IA32_SYSENTER_EIP	$0x176UL
 
 TEXT wrfsb(SB), NOSPLIT, $0-8
@@ -691,13 +690,6 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	ORQ	DX, AX
 	PUSHQ	AX
 
-	// save sysenter rsp
-	MOVQ	IA32_SYSENTER_ESP, CX
-	RDMSR
-	SHLQ	$32, DX
-	ORQ	DX, AX
-	PUSHQ	AX
-
 	MOVQ	SP, AX
 	PUSHQ	AX
 
@@ -719,14 +711,6 @@ TEXT ·_trapret(SB), NOSPLIT, $0-8
 				// threads[]
 	MOVQ	AX, SP
 
-	// restore sysenter esp
-	MOVQ	IA32_SYSENTER_ESP, CX
-	POPQ	AX
-	MOVQ	AX, DX
-	ANDQ	$((1 << 32) - 1), AX
-	SHRQ	$32, DX
-	WRMSR
-
 	// restore fsbase
 	MOVQ	IA32_FS_BASE, CX
 	POPQ	AX
@@ -866,18 +850,17 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 
 	JMP	alltraps(SB)
 
-#define TFREGS		17
-#define TF_SYSRSP	(8*0)
-#define TF_R13		(8*4)
-#define TF_R12		(8*5)
-#define TF_R8		(8*9)
-#define TF_RBP		(8*10)
-#define TF_RSI		(8*11)
-#define TF_RDI		(8*12)
-#define TF_RDX		(8*13)
-#define TF_RCX		(8*14)
-#define TF_RBX		(8*15)
-#define TF_RAX		(8*16)
+#define TFREGS		16
+#define TF_R13		(8*3)
+#define TF_R12		(8*4)
+#define TF_R8		(8*8)
+#define TF_RBP		(8*9)
+#define TF_RSI		(8*10)
+#define TF_RDI		(8*11)
+#define TF_RDX		(8*12)
+#define TF_RCX		(8*13)
+#define TF_RBX		(8*14)
+#define TF_RAX		(8*15)
 #define TF_RIP		(8*(TFREGS + 2))
 #define TF_RSP		(8*(TFREGS + 5))
 
@@ -887,34 +870,19 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	MOVQ	tf+0(FP), R9
 
+	MOVQ	0(GS), AX
+	MOVQ	SP, 0x20(AX)
+
 	// fastret or iret?
 	MOVB	fastret+8(FP), AX
 	CMPB	AX, $0
 	JNE	syscallreturn
-	// do full state restore, make sure the SP we return with is correct
-	MOVQ	SP, TF_SYSRSP(R9)
+	// do full state restore
 	PUSHQ	R9
 	CALL	·_trapret(SB)
 	INT	$3
 
 syscallreturn:
-	// set SP MSRs manually
-	PUSHQ	$0
-	PUSHQ	IA32_SYSENTER_ESP
-	CALL	·Rdmsr(SB)
-	POPQ	AX
-	POPQ	AX
-	CMPQ	SP, AX
-	JEQ	gut
-
-	MOVQ	SP, AX
-	PUSHQ	AX
-	PUSHQ	IA32_SYSENTER_ESP
-	CALL	·Wrmsr(SB)
-	POPQ	AX
-	POPQ	AX
-
-gut:
 	MOVQ	TF_RAX(R9), AX
 	MOVQ	TF_RSP(R9), CX
 	MOVQ	TF_RIP(R9), DX
@@ -937,6 +905,9 @@ gut:
 // are hand-coded.
 //_sysentry:
 TEXT ·_sysentry(SB), NOSPLIT, $0-0
+	MOVQ	0(GS), SP
+	MOVQ	0x20(SP), SP
+
 	// save user state in fake trapframe
 	MOVQ	0x20(SP), R9
 	MOVQ	R10, TF_RSP(R9)

commit c21321b93ca32f94a581d5bc2c17a4c29bf502bf
Merge: f5e1820aea 70b551cdfa
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Apr 17 13:33:25 2016 -0400

    Merge branch 'master' into g1.6

commit 70b551cdfa3e12285dfc4404b24e87250271bec9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Apr 17 13:31:16 2016 -0400

    avoid wrmsr in syscall return fast path

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f46650da91..3327ca0b40 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -899,6 +899,14 @@ TEXT ·_Userrun(SB), NOSPLIT, $24-32
 
 syscallreturn:
 	// set SP MSRs manually
+	PUSHQ	$0
+	PUSHQ	IA32_SYSENTER_ESP
+	CALL	·Rdmsr(SB)
+	POPQ	AX
+	POPQ	AX
+	CMPQ	SP, AX
+	JEQ	gut
+
 	MOVQ	SP, AX
 	PUSHQ	AX
 	PUSHQ	IA32_SYSENTER_ESP
@@ -906,6 +914,7 @@ syscallreturn:
 	POPQ	AX
 	POPQ	AX
 
+gut:
 	MOVQ	TF_RAX(R9), AX
 	MOVQ	TF_RSP(R9), CX
 	MOVQ	TF_RIP(R9), DX

commit 4b209dbf0bf3e5fd4cffda1e11f11bf45ddf212d
Author: Keith Randall <khr@golang.org>
Date:   Tue Mar 29 21:25:33 2016 -0700

    runtime: don't use REP;MOVSB if CPUID doesn't say it is fast
    
    Only use REP;MOVSB if:
     1) The CPUID flag says it is fast, and
     2) The pointers are unaligned
    Otherwise, use REP;MOVSQ.
    
    Update #14630
    
    Change-Id: I946b28b87880c08e5eed1ce2945016466c89db66
    Reviewed-on: https://go-review.googlesource.com/21300
    Reviewed-by: Nigel Tao <nigeltao@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b4df1d80d7..83db4d3e81 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -28,6 +28,7 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// find out information about the processor we're on
 	MOVQ	$0, AX
 	CPUID
+	MOVQ	AX, SI
 	CMPQ	AX, $0
 	JE	nocpuinfo
 
@@ -42,15 +43,25 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	JNE	notintel
 	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
 notintel:
-	// Do nothing.
 
+	// Load EAX=1 cpuid flags
 	MOVQ	$1, AX
 	CPUID
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
+
+	// Load EAX=7/ECX=0 cpuid flags
+	CMPQ	SI, $7
+	JLT	no7
+	MOVL	$7, AX
+	MOVL	$0, CX
+	CPUID
+	MOVL	BX, runtime·cpuid_ebx7(SB)
+no7:
 	// Detect AVX and AVX2 as per 14.7.1  Detection of AVX2 chapter of [1]
 	// [1] 64-ia-32-architectures-software-developer-manual-325462.pdf
 	// http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf
+	MOVL	runtime·cpuid_ecx(SB), CX
 	ANDL    $0x18000000, CX // check for OSXSAVE and AVX bits
 	CMPL    CX, $0x18000000
 	JNE     noavx
@@ -61,12 +72,8 @@ notintel:
 	CMPL    AX, $6 // Check for OS support of YMM registers
 	JNE     noavx
 	MOVB    $1, runtime·support_avx(SB)
-	MOVL    $7, AX
-	MOVL    $0, CX
-	CPUID
-	ANDL    $0x20, BX // check for AVX2 bit
-	CMPL    BX, $0x20
-	JNE     noavx2
+	TESTL   $(1<<5), runtime·cpuid_ebx7(SB) // check for AVX2 bit
+	JEQ     noavx2
 	MOVB    $1, runtime·support_avx2(SB)
 	JMP     nocpuinfo
 noavx:

commit 6aab455cac150466c54183621f099381a7ead533
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 14 09:15:29 2016 -0400

    go1.6 updates
    
    only small changes: they renamed atomic functions, also use sigaltstack(2) to
    query current signal stack instead of only specifying a new one, and made m0's
    struct statically allocated.
    
    go1.6 now works too!

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 45b153065c..1d14598617 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -73,8 +73,8 @@ noavx:
 	MOVB    $0, runtime·support_avx(SB)
 noavx2:
 	MOVB    $0, runtime·support_avx2(SB)
-nocpuinfo:	
-	
+nocpuinfo:
+
 	// if there is an _cgo_init, call it.
 	MOVQ	_cgo_init(SB), AX
 	TESTQ	AX, AX
@@ -238,11 +238,37 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	JNE	notintel
 	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
 notintel:
+	// Do nothing.
 
 	MOVQ	$1, AX
 	CPUID
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
+	// Detect AVX and AVX2 as per 14.7.1  Detection of AVX2 chapter of [1]
+	// [1] 64-ia-32-architectures-software-developer-manual-325462.pdf
+	// http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf
+	ANDL    $0x18000000, CX // check for OSXSAVE and AVX bits
+	CMPL    CX, $0x18000000
+	JNE     noavx
+	MOVL    $0, CX
+	// For XGETBV, OSXSAVE bit is required and sufficient
+	XGETBV
+	ANDL    $6, AX
+	CMPL    AX, $6 // Check for OS support of YMM registers
+	JNE     noavx
+	MOVB    $1, runtime·support_avx(SB)
+	MOVL    $7, AX
+	MOVL    $0, CX
+	CPUID
+	ANDL    $0x20, BX // check for AVX2 bit
+	CMPL    BX, $0x20
+	JNE     noavx2
+	MOVB    $1, runtime·support_avx2(SB)
+	JMP     nocpuinfo
+noavx:
+	MOVB    $0, runtime·support_avx(SB)
+noavx2:
+	MOVB    $0, runtime·support_avx2(SB)
 nocpuinfo:
 
 	// if there is an _cgo_init, call it.
@@ -261,19 +287,20 @@ nocpuinfo:
 	MOVQ	AX, g_stackguard0(CX)
 	MOVQ	AX, g_stackguard1(CX)
 
-	CALL	runtime·cls(SB)
-
-	//CMPL	runtime·iswindows(SB), $0
-	//JEQ ok
+//#ifndef GOOS_windows
+//	JMP ok
+//#endif
 needtls:
-	// skip TLS setup on Plan 9
-	//CMPL	runtime·isplan9(SB), $1
-	//JEQ ok
-	//// skip TLS setup on Solaris
-	//CMPL	runtime·issolaris(SB), $1
-	//JEQ ok
-
-	//LEAQ	runtime·tls0(SB), DI
+//#ifdef GOOS_plan9
+//	// skip TLS setup on Plan 9
+//	JMP ok
+//#endif
+//#ifdef GOOS_solaris
+//	// skip TLS setup on Solaris
+//	JMP ok
+//#endif
+
+	//LEAQ	runtime·m0+m_tls(SB), DI
 	//CALL	runtime·settls(SB)
 
 	CALL	·seg_setup(SB)
@@ -284,9 +311,9 @@ needtls:
 	// store through it, to make sure it works
 	get_tls(BX)
 	MOVQ	$0x123, g(BX)
-	MOVQ	runtime·tls0(SB), AX
+	MOVQ	runtime·m0+m_tls(SB), AX
 	CMPQ	AX, $0x123
-	JEQ	ok
+	JMP	ok
 	MOVQ	$0x42, (SP)
 	CALL	runtime·putch(SB)
 	MOVQ	$0x46, (SP)

commit b193f491121140c325fed031f424c3330a043829
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 12 12:20:38 2016 -0500

    1.5 updates
    
    one notable change: go1.5 no longer uses amd64 ELF TLS address of -16(%fs), but
    -8(%fs).

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4b55909504..45b153065c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -197,29 +197,25 @@ DATA	fakeargv+16(SB)/8,$0
 GLOBL	fakeargv(SB),RODATA,$24
 
 TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
-
-	// magic loop
-	//BYTE	$0xeb
-	//BYTE	$0xfe
-	CALL	runtime·sc_setup(SB)
-
-	// save page table and first free address from bootloader.
 	MOVL	DI, ·p_kpmap(SB)
 	MOVL	SI, ·pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
+	CALL	runtime·sc_setup(SB)
 
+	// copy arguments forward on an even stack
+	//MOVQ	DI, AX		// argc
+	//MOVQ	SI, BX		// argv
+	MOVQ	$0, AX		// argc
+	MOVQ	$0, BX		// argv
+	SUBQ	$(4*8+7), SP		// 2args 2auto
 	ANDQ	$~15, SP
-	//SUBQ	$8, SP	// traceback assumes the initial rsp is writable
+	MOVQ	AX, 16(SP)
+	MOVQ	BX, 24(SP)
 
 	// create istack out of the given (operating system) stack.
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
-	// even though we only have one page of stack, leave stackguard far
-	// below. if the scheduler overflows its stack, we will get a pagefault
-	// which is probably easier to debug than panicking due to stack
-	// growing on scheduler.
 	LEAQ	(-64*1024+104)(SP), BX
-	//LEAQ	(-4*1024+104)(SP), BX
 	MOVQ	BX, g_stackguard0(DI)
 	MOVQ	BX, g_stackguard1(DI)
 	MOVQ	BX, (g_stack+stack_lo)(DI)
@@ -229,14 +225,25 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	MOVQ	$0, AX
 	CPUID
 	CMPQ	AX, $0
-	JE	h_nocpuinfo
+	JE	nocpuinfo
+
+	// Figure out how to serialize RDTSC.
+	// On Intel processors LFENCE is enough. AMD requires MFENCE.
+	// Don't know about the rest, so let's do MFENCE.
+	CMPL	BX, $0x756E6547  // "Genu"
+	JNE	notintel
+	CMPL	DX, $0x49656E69  // "ineI"
+	JNE	notintel
+	CMPL	CX, $0x6C65746E  // "ntel"
+	JNE	notintel
+	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
+notintel:
+
 	MOVQ	$1, AX
 	CPUID
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
-h_nocpuinfo:
-
-	CALL	runtime·cls(SB)
+nocpuinfo:
 
 	// if there is an _cgo_init, call it.
 	//MOVQ	_cgo_init(SB), AX
@@ -247,26 +254,29 @@ h_nocpuinfo:
 	//MOVQ	$setg_gcc<>(SB), SI
 	//CALL	AX
 
-	//// update stackguard after _cgo_init
+	// update stackguard after _cgo_init
 	MOVQ	$runtime·g0(SB), CX
 	MOVQ	(g_stack+stack_lo)(CX), AX
-	ADDQ	$const_StackGuard, AX
+	ADDQ	$const__StackGuard, AX
 	MOVQ	AX, g_stackguard0(CX)
 	MOVQ	AX, g_stackguard1(CX)
 
+	CALL	runtime·cls(SB)
+
 	//CMPL	runtime·iswindows(SB), $0
 	//JEQ ok
-h_needtls:
-
-	//// skip TLS setup on Plan 9
+needtls:
+	// skip TLS setup on Plan 9
 	//CMPL	runtime·isplan9(SB), $1
 	//JEQ ok
 	//// skip TLS setup on Solaris
 	//CMPL	runtime·issolaris(SB), $1
 	//JEQ ok
 
-	CALL	·seg_setup(SB)
+	//LEAQ	runtime·tls0(SB), DI
+	//CALL	runtime·settls(SB)
 
+	CALL	·seg_setup(SB)
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
@@ -276,13 +286,14 @@ h_needtls:
 	MOVQ	$0x123, g(BX)
 	MOVQ	runtime·tls0(SB), AX
 	CMPQ	AX, $0x123
-	JEQ	h_ok
-	MOVW	$0x1742, 0xb8000
-	MOVW	$0x1746, 0xb8002
+	JEQ	ok
+	MOVQ	$0x42, (SP)
+	CALL	runtime·putch(SB)
+	MOVQ	$0x46, (SP)
+	CALL	runtime·putch(SB)
 	BYTE	$0xeb;
 	BYTE	$0xfe;
-h_ok:
-
+ok:
 	// set the per-goroutine and per-mach "registers"
 	get_tls(BX)
 	LEAQ	runtime·g0(SB), CX
@@ -296,36 +307,35 @@ h_ok:
 
 	CALL	·int_setup(SB)
 	CALL	·proc_setup(SB)
+	STI
 
 	CLD				// convention is D is always left cleared
 	CALL	runtime·check(SB)
 
+	//MOVL	16(SP), AX		// copy argc
+	MOVL	$1, AX		// copy argc
+	MOVL	AX, 0(SP)
+	//MOVQ	24(SP), AX		// copy argv
 	MOVQ	$fakeargv(SB), AX
-	PUSHQ	AX
-	PUSHQ	$1
+	MOVQ	AX, 8(SP)
 	CALL	runtime·args(SB)
-	POPQ	AX
-	POPQ	AX
 	CALL	runtime·osinit(SB)
 	CALL	runtime·schedinit(SB)
 
 	// create a new goroutine to start program
-	MOVQ	$runtime·main·f(SB), BP		// entry
-	PUSHQ	BP
+	MOVQ	$runtime·mainPC(SB), AX		// entry
+	PUSHQ	AX
 	PUSHQ	$0			// arg size
 	CALL	runtime·newproc(SB)
 	POPQ	AX
 	POPQ	AX
 
 	// start this M
-	STI
-	//CALL	clone_test(SB)
 	CALL	runtime·mstart(SB)
 
 	MOVL	$0xf1, 0xf1  // crash
 	RET
 
-
 TEXT runtime·Cpuid(SB), NOSPLIT, $0-24
 	XORQ	AX, AX
 	XORQ	CX, CX
@@ -690,9 +700,9 @@ IH_IRQ(13,·Xirq13 )
 IH_IRQ(14,·Xirq14 )
 IH_IRQ(15,·Xirq15 )
 
-#define IA32_FS_BASE		$0xc0000100UL
-#define IA32_SYSENTER_ESP	$0x175UL
-#define IA32_SYSENTER_EIP	$0x176UL
+#define IA32_FS_BASE		$0xc0000100
+#define IA32_SYSENTER_ESP	$0x175
+#define IA32_SYSENTER_EIP	$0x176
 
 TEXT wrfsb(SB), NOSPLIT, $0-8
 	get_tls(BX)
@@ -800,56 +810,6 @@ TEXT ·_trapret(SB), NOSPLIT, $0-8
 	BYTE	$0x48
 	BYTE	$0xcf
 
-TEXT gtest(SB), NOSPLIT, $0
-	MOVQ	$1, AX
-	MOVQ	$2, BX
-	MOVQ	$3, CX
-	MOVQ	$4, DX
-	MOVQ	$5, DI
-	MOVQ	$6, SI
-	MOVQ	$7, R8
-	MOVQ	$8, R9
-	MOVQ	$9, R10
-	MOVQ	$10, R11
-	MOVQ	$11, R12
-	MOVQ	$12, R13
-	MOVQ	$13, R14
-	MOVQ	$14, R15
-	PUSHQ	TRAP_YIELD
-	CALL	·mktrap(SB)
-	ADDQ	$8, SP
-	CMPQ	AX, $1
-	JNE	badinko
-	CMPQ	BX, $2
-	JNE	badinko
-	CMPQ	CX, $3
-	JNE	badinko
-	CMPQ	DX, $4
-	JNE	badinko
-	CMPQ	DI, $5
-	JNE	badinko
-	CMPQ	SI, $6
-	JNE	badinko
-	CMPQ	R8, $7
-	JNE	badinko
-	CMPQ	R9, $8
-	JNE	badinko
-	CMPQ	R10, $9
-	JNE	badinko
-	CMPQ	R11, $10
-	JNE	badinko
-	CMPQ	R12, $11
-	JNE	badinko
-	CMPQ	R13, $12
-	JNE	badinko
-	CMPQ	R14, $13
-	JNE	badinko
-	CMPQ	R15, $14
-	JNE	badinko
-	RET
-badinko:
-	INT	$3
-
 // void ·mktrap(uint64 intn)
 TEXT ·mktrap(SB), NOSPLIT, $0-8
 	PUSHQ	AX
@@ -1003,43 +963,6 @@ TEXT ·_userint(SB), NOSPLIT, $0-0
 	ADDQ	$0x18, SP
 	RET
 
-TEXT old_sysentry(SB), NOSPLIT, $0
-	// r10 contains return rsp, r11 contains return rip
-	PUSHQ	AX
-
-	// build hardware trap frame
-
-	MOVQ	$((UDSEG << 3) | 3), AX
-	PUSHQ	AX
-
-	PUSHQ	R10
-
-#define		TF_FL_IF	$(1 << 9)
-	PUSHFQ
-	POPQ	AX
-	ORQ	TF_FL_IF, AX
-	PUSHQ	AX
-
-	MOVQ	$((UCSEG << 3) | 3), AX
-	PUSHQ	AX
-
-	// ret addr
-	PUSHQ	R11
-
-	// dummy error code
-	PUSHQ	$0
-
-	// interrupt number
-	PUSHQ	TRAP_SYSCALL
-
-	// and finally, restore rax
-	MOVQ	56(SP), AX
-	JMP	alltraps(SB)
-
-	INT	$3
-	CALL	sysentry(SB)
-	INT	$3
-
 TEXT ·gs_null(SB), NOSPLIT, $8-0
 	XORQ	AX, AX
 	PUSHQ	AX

commit d2eb6433fbeb4c4e81dbda541959e71e03c244c6
Merge: e2f500e61c 7bc40ffb05
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Mar 13 11:21:11 2016 -0400

    Merge tag 'go1.6' into tmerge
    
    Conflicts:
            .gitignore
            src/cmd/dist/build.c
            src/cmd/ld/lib.c
            src/liblink/obj6.c
            src/runtime/mgc0.c
            src/runtime/os_linux.c
            src/runtime/os_linux.go
            src/runtime/proc.c
            src/runtime/runtime.c
            src/runtime/stack.h
            src/runtime/sys_linux_amd64.s
            src/runtime/traceback.go

commit 458bebe13f7cf51f41d66388d18ddeac670212f5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 11 11:23:57 2016 -0500

    finish and cleanup of main C file
    
    just trap{sched,wake} C code remains.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d1b98d74ad..f46650da91 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -148,7 +148,7 @@ done:
 // i do it this strange way because if i declare fakeargv in C i get 'missing
 // golang type information'. need two 0 entries because go checks for
 // environment variables too.
-DATA	fakeargv+0(SB)/8,$gostr(SB)
+DATA	fakeargv+0(SB)/8,$·gostr(SB)
 DATA	fakeargv+8(SB)/8,$0
 DATA	fakeargv+16(SB)/8,$0
 GLOBL	fakeargv(SB),RODATA,$24
@@ -234,10 +234,10 @@ h_needtls:
 	MOVQ	runtime·tls0(SB), AX
 	CMPQ	AX, $0x123
 	JEQ	h_ok
-	MOVQ	$0x4242424242424242, AX
-	PUSHQ	AX
-	PUSHQ	$0
-	CALL	runtime·pancake(SB)
+	MOVW	$0x1742, 0xb8000
+	MOVW	$0x1746, 0xb8002
+	BYTE	$0xeb;
+	BYTE	$0xfe;
 h_ok:
 
 	// set the per-goroutine and per-mach "registers"
@@ -299,7 +299,7 @@ TEXT ·finit(SB), NOSPLIT, $0-0
 	FINIT
 	RET
 
-TEXT ·rcr0(SB), NOSPLIT, $0-8
+TEXT ·Rcr0(SB), NOSPLIT, $0-8
 	MOVQ	CR0, AX
 	MOVQ	AX, ret+0(FP)
 	RET
@@ -309,25 +309,22 @@ TEXT ·Rcr2(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT ·rcr4(SB), NOSPLIT, $0-8
+TEXT ·Rcr4(SB), NOSPLIT, $0-8
 	MOVQ	CR4, AX
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT ·Rcr4(SB), NOSPLIT, $0-0
-	JMP	·rcr4(SB)
-
 TEXT tlbflush(SB), NOSPLIT, $0-0
 	MOVQ	CR3, AX
 	MOVQ	AX, CR3
 	RET
 
-TEXT lcr3(SB), NOSPLIT, $0-8
+TEXT ·Lcr3(SB), NOSPLIT, $0-8
 	MOVQ	pgtbl+0(FP), AX
 	MOVQ	AX, CR3
 	RET
 
-TEXT rcr3(SB), NOSPLIT, $0-8
+TEXT ·Rcr3(SB), NOSPLIT, $0-8
 	MOVQ	CR3, AX
 	MOVQ	AX, ret+0(FP)
 	RET
@@ -371,12 +368,12 @@ TEXT ·ltr(SB), NOSPLIT, $0-8
 	BYTE $0xd8
 	RET
 
-TEXT ·lcr0(SB), NOSPLIT, $0-8
+TEXT ·Lcr0(SB), NOSPLIT, $0-8
 	MOVQ	val+0(FP), AX
 	MOVQ	AX, CR0
 	RET
 
-TEXT ·lcr4(SB), NOSPLIT, $0-8
+TEXT ·Lcr4(SB), NOSPLIT, $0-8
 	MOVQ	val+0(FP), AX
 	MOVQ	AX, CR4
 	RET
@@ -471,11 +468,11 @@ TEXT ·Rdtsc(SB), NOSPLIT, $0-8
 	MOVL	DX, ret+4(FP)
 	RET
 
-TEXT ·cli(SB), NOSPLIT, $0-0
+TEXT ·Cli(SB), NOSPLIT, $0-0
 	CLI
 	RET
 
-TEXT ·sti(SB), NOSPLIT, $0-0
+TEXT ·Sti(SB), NOSPLIT, $0-0
 	STI
 	RET
 

commit 19c7622688382187565f3028ca8b70f697663b07
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 10 11:32:26 2016 -0500

    convert trap()

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 67e8f1a691..d1b98d74ad 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -304,7 +304,7 @@ TEXT ·rcr0(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT rcr2(SB), NOSPLIT, $0-8
+TEXT ·Rcr2(SB), NOSPLIT, $0-8
 	MOVQ	CR2, AX
 	MOVQ	AX, ret+0(FP)
 	RET
@@ -704,7 +704,7 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	MOVQ	SP, AX
 	PUSHQ	AX
 
-	CALL	trap(SB)
+	CALL	·trap(SB)
 	// jmp self
 	BYTE	$0xeb
 	BYTE	$0xfe
@@ -714,10 +714,10 @@ TEXT ·trapret(SB), NOSPLIT, $0-16
 	MOVQ	pmap+8(FP), BX
 	MOVQ	BX, CR3
 
-	JMP	_trapret(SB)
+	JMP	·_trapret(SB)
 	INT	$3
 
-TEXT _trapret(SB), NOSPLIT, $0-8
+TEXT ·_trapret(SB), NOSPLIT, $0-8
 	MOVQ	tf+0(FP), AX	// tf is not on the callers stack frame, but in
 				// threads[]
 	MOVQ	AX, SP
@@ -885,7 +885,7 @@ TEXT ·mktrap(SB), NOSPLIT, $0-8
 #define TF_RSP		(8*(TFREGS + 5))
 
 // if you change the number of arguments, you must adjust the stack offsets in
-// _sysentry and _userint.
+// _sysentry and ·_userint.
 // func _Userrun(tf *[24]int, fastret bool) (int, int)
 TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	MOVQ	tf+0(FP), R9
@@ -897,7 +897,7 @@ TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	// do full state restore, make sure the SP we return with is correct
 	MOVQ	SP, TF_SYSRSP(R9)
 	PUSHQ	R9
-	CALL	_trapret(SB)
+	CALL	·_trapret(SB)
 	INT	$3
 
 syscallreturn:
@@ -923,7 +923,7 @@ syscallreturn:
 	BYTE	$0x35
 	// not reached; just to trick dead code analysis
 	CALL	·_sysentry(SB)
-	CALL	_userint(SB)
+	CALL	·_userint(SB)
 
 // this should be a label since it is the bottom half of the Userrun_ function,
 // but i can't figure out how to get the plan9 assembler to let me use lea on a
@@ -954,7 +954,7 @@ TEXT ·_sysentry(SB), NOSPLIT, $0-0
 
 // this is the bottom half of _userrun() that is executed if a timer int or CPU
 // exception is generated during user program execution.
-TEXT _userint(SB), NOSPLIT, $0-0
+TEXT ·_userint(SB), NOSPLIT, $0-0
 	CLI
 	// user state is already saved by trap handler.
 	// AX holds the interrupt number, BX holds aux (cr2 for page fault)

commit 7ce9f81275ddcb9582b1289a8d57134daa7866f7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 17:09:15 2016 -0500

    finish init code: BSP/AP, syscall, and GS init

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f1dc36ac6c..67e8f1a691 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -252,23 +252,11 @@ h_ok:
 	MOVQ	AX, g_m(CX)
 
 	CALL	·int_setup(SB)
-	CALL	proc_setup(SB)
-
-	//MOVQ	CR0, AX
-	//PUSHQ	AX
-	//CALL	exam(SB)
-	//POPQ	AX
-
-	//CALL	pgtest(SB)
-	//CALL	mmap_test(SB)
+	CALL	·proc_setup(SB)
 
 	CLD				// convention is D is always left cleared
 	CALL	runtime·check(SB)
 
-	//MOVL	16(SP), AX		// copy argc
-	//MOVL	AX, 0(SP)
-	//MOVQ	24(SP), AX		// copy argv
-	//MOVQ	AX, 8(SP)
 	MOVQ	$fakeargv(SB), AX
 	PUSHQ	AX
 	PUSHQ	$1
@@ -475,11 +463,6 @@ TEXT ·rflags(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT rrsp(SB), NOSPLIT, $0-8
-	MOVQ	SP, AX
-	MOVQ	AX, ret+0(FP)
-	RET
-
 TEXT ·Rdtsc(SB), NOSPLIT, $0-8
 	// rdtsc
 	BYTE	$0x0f
@@ -939,7 +922,7 @@ syscallreturn:
 	BYTE	$0x0f
 	BYTE	$0x35
 	// not reached; just to trick dead code analysis
-	CALL	_sysentry(SB)
+	CALL	·_sysentry(SB)
 	CALL	_userint(SB)
 
 // this should be a label since it is the bottom half of the Userrun_ function,
@@ -947,7 +930,7 @@ syscallreturn:
 // label. thus the function epilogue and offset to get the tf arg from Userrun_
 // are hand-coded.
 //_sysentry:
-TEXT _sysentry(SB), NOSPLIT, $0-0
+TEXT ·_sysentry(SB), NOSPLIT, $0-0
 	// save user state in fake trapframe
 	MOVQ	0x20(SP), R9
 	MOVQ	R10, TF_RSP(R9)

commit 624faa7efb6223d3632e7f3a65efbedab9698a2d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 14:37:55 2016 -0500

    convert LAPIC init code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 93bba7f6e6..f1dc36ac6c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -408,9 +408,23 @@ TEXT ·Wrmsr(SB), NOSPLIT, $0-16
 	WRMSR
 	RET
 
+// uint64 Inb(reg uint16)
+TEXT ·Inb(SB), NOSPLIT, $0-16
+	MOVW	reg+0(FP), DX
+	// inb (%dx), %al
+	BYTE	$0xec
+	// movzbq %al, %rax
+	BYTE $0x48
+	BYTE $0x0f
+	BYTE $0xb6
+	BYTE $0xc0
+	MOVQ	AX, ret+8(FP)
+	RET
+
+// void Outb(reg uint16, val uint8)
 TEXT runtime·Outb(SB), NOSPLIT, $0-16
-	MOVQ	reg+0(FP), DX
-	MOVQ	val+8(FP), AX
+	MOVW	reg+0(FP), DX
+	MOVB	val+2(FP), AX
 	// outb	%al, (%dx)
 	BYTE	$0xee
 	RET
@@ -430,14 +444,6 @@ TEXT runtime·Outl(SB), NOSPLIT, $0-16
 	BYTE	$0xef
 	RET
 
-//void outb(int64 port, int64 val)
-TEXT outb(SB), NOSPLIT, $0-16
-	MOVL	reg+0(FP), DX
-	MOVL	val+8(FP), AX
-	// outb	%al, (%dx)
-	BYTE	$0xee
-	RET
-
 TEXT runtime·Outsl(SB), NOSPLIT, $0-24
 	MOVQ	reg+0(FP), DX
 	MOVQ	ptr+8(FP), SI
@@ -463,22 +469,6 @@ TEXT runtime·Insl(SB), NOSPLIT, $0-24
 	BYTE	$0x6d
 	RET
 
-TEXT runtime·inb(SB), NOSPLIT, $0-0
-	JMP	inb(SB)
-
-//int64 inb(int64 port)
-TEXT inb(SB), NOSPLIT, $0-16
-	MOVL	reg+0(FP), DX
-	// inb	(%dx), %al
-	BYTE	$0xec
-	// movzbq %al, %rax
-	BYTE $0x48
-	BYTE $0x0f
-	BYTE $0xb6
-	BYTE $0xc0
-	MOVQ	AX, ret+8(FP)
-	RET
-
 TEXT ·rflags(SB), NOSPLIT, $0-8
 	PUSHFQ
 	POPQ	AX

commit fe3471e33f3fe3e430246c9855c46036355c5e4f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 11:14:30 2016 -0500

    finish fake syscalls: exit, sigaltstack, nanotime, futex, setitimer, write

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3f3ff36deb..93bba7f6e6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -803,7 +803,7 @@ TEXT gtest(SB), NOSPLIT, $0
 	MOVQ	$13, R14
 	MOVQ	$14, R15
 	PUSHQ	TRAP_YIELD
-	CALL	mktrap(SB)
+	CALL	·mktrap(SB)
 	ADDQ	$8, SP
 	CMPQ	AX, $1
 	JNE	badinko
@@ -837,8 +837,8 @@ TEXT gtest(SB), NOSPLIT, $0
 badinko:
 	INT	$3
 
-// void mktrap(uint64 intn)
-TEXT mktrap(SB), NOSPLIT, $0-8
+// void ·mktrap(uint64 intn)
+TEXT ·mktrap(SB), NOSPLIT, $0-8
 	PUSHQ	AX
 	PUSHQ	DX
 	PUSHFQ
@@ -1039,7 +1039,7 @@ TEXT ·fs_null(SB), NOSPLIT, $8-0
 	POPQ	FS
 	RET
 
-TEXT ·Gscpu(SB), NOSPLIT, $0-8
+TEXT ·_Gscpu(SB), NOSPLIT, $0-8
 	MOVQ	0(GS), AX
 	MOVQ	AX, ret+0(FP)
 	RET

commit c8bf8e0cb06d3ba231ba90071303ab38d27087fd
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 8 16:37:44 2016 -0500

    start converting scheduler code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 02b4df9576..3f3ff36deb 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -563,7 +563,7 @@ TEXT ·fxsave(SB), NOSPLIT, $0-8
 	BYTE	$0x00
 	RET
 
-TEXT fxrstor(SB), NOSPLIT, $0-8
+TEXT ·fxrstor(SB), NOSPLIT, $0-8
 	MOVQ	dst+0(FP), AX
 	// fxrstor	(%rax)
 	BYTE	$0x0f
@@ -571,7 +571,7 @@ TEXT fxrstor(SB), NOSPLIT, $0-8
 	BYTE	$0x08
 	RET
 
-TEXT cpu_halt(SB), NOSPLIT, $0-8
+TEXT ·cpu_halt(SB), NOSPLIT, $0-8
 	MOVQ	sp+0(FP), SP
 	STI
 hltagain:
@@ -736,7 +736,8 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	BYTE	$0xeb
 	BYTE	$0xfe
 
-TEXT trapret(SB), NOSPLIT, $0-16
+// void ·trapret(tf *uintptr, p_pmap uintptr)
+TEXT ·trapret(SB), NOSPLIT, $0-16
 	MOVQ	pmap+8(FP), BX
 	MOVQ	BX, CR3
 

commit 01398330b0a342dca59abcd1fcbd4f9172c52c5d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 8 15:19:32 2016 -0500

    convert fake clone

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a650421424..02b4df9576 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -161,7 +161,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	CALL	runtime·sc_setup(SB)
 
 	// save page table and first free address from bootloader.
-	MOVL	DI, kpmap(SB)
+	MOVL	DI, ·p_kpmap(SB)
 	MOVL	SI, ·pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 
@@ -171,6 +171,10 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	// create istack out of the given (operating system) stack.
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
+	// even though we only have one page of stack, leave stackguard far
+	// below. if the scheduler overflows its stack, we will get a pagefault
+	// which is probably easier to debug than panicking due to stack
+	// growing on scheduler.
 	LEAQ	(-64*1024+104)(SP), BX
 	//LEAQ	(-4*1024+104)(SP), BX
 	MOVQ	BX, g_stackguard0(DI)
@@ -201,11 +205,11 @@ h_nocpuinfo:
 	//CALL	AX
 
 	//// update stackguard after _cgo_init
-	//MOVQ	$runtime·g0(SB), CX
-	//MOVQ	(g_stack+stack_lo)(CX), AX
-	//ADDQ	$const_StackGuard, AX
-	//MOVQ	AX, g_stackguard0(CX)
-	//MOVQ	AX, g_stackguard1(CX)
+	MOVQ	$runtime·g0(SB), CX
+	MOVQ	(g_stack+stack_lo)(CX), AX
+	ADDQ	$const_StackGuard, AX
+	MOVQ	AX, g_stackguard0(CX)
+	MOVQ	AX, g_stackguard1(CX)
 
 	//CMPL	runtime·iswindows(SB), $0
 	//JEQ ok
@@ -223,9 +227,6 @@ h_needtls:
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
-	PUSHQ	$1
-	CALL	·fpuinit(SB)
-	POPQ	AX
 
 	// store through it, to make sure it works
 	get_tls(BX)
@@ -478,7 +479,7 @@ TEXT inb(SB), NOSPLIT, $0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT rflags(SB), NOSPLIT, $0-8
+TEXT ·rflags(SB), NOSPLIT, $0-8
 	PUSHFQ
 	POPQ	AX
 	MOVQ	AX, ret+0(FP)
@@ -577,6 +578,12 @@ hltagain:
 	HLT
 	JMP	hltagain
 
+// void ·clone_call(uintptr rip)
+TEXT ·clone_call(SB), NOSPLIT, $0-8
+	MOVQ	fn+0(FP), AX
+	CALL	AX
+	RET
+
 #define TRAP_YIELD      $49
 #define TRAP_SYSCALL    $64
 TEXT hack_yield(SB), NOSPLIT, $0-0

commit c25f8fd81340dac2232dda30103b50c15ee9a1b2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 16:58:16 2016 -0500

    convert/cleanup pmap, FPU, and physical memory code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b7d5f090fb..a650421424 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -131,14 +131,6 @@ TEXT fixcs(SB),NOSPLIT,$0
 	BYTE	$0x48
 	BYTE	$0xcb
 	MOVQ	$1, 0
-	//POPQ	DX
-	//MOVQ	$(3 << 3), AX
-	//SHLQ	$32, AX
-	//ORQ	DX, AX
-	//PUSHQ	AX
-	//// lret
-	//BYTE	$0xcb
-	//MOVQ	$1, 0
 
 TEXT runtime·deray(SB),NOSPLIT,$8
 	MOVQ	times+0(FP), CX
@@ -170,7 +162,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 
 	// save page table and first free address from bootloader.
 	MOVL	DI, kpmap(SB)
-	MOVL	SI, pgfirst(SB)
+	MOVL	SI, ·pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 
 	ANDQ	$~15, SP
@@ -231,7 +223,9 @@ h_needtls:
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
-	CALL	fpuinit(SB)
+	PUSHQ	$1
+	CALL	·fpuinit(SB)
+	POPQ	AX
 
 	// store through it, to make sure it works
 	get_tls(BX)
@@ -312,11 +306,11 @@ TEXT runtime·Cpuid(SB), NOSPLIT, $0-24
 	MOVL	DX, ret+20(FP)
 	RET
 
-TEXT finit(SB), NOSPLIT, $0-0
+TEXT ·finit(SB), NOSPLIT, $0-0
 	FINIT
 	RET
 
-TEXT rcr0(SB), NOSPLIT, $0-8
+TEXT ·rcr0(SB), NOSPLIT, $0-8
 	MOVQ	CR0, AX
 	MOVQ	AX, ret+0(FP)
 	RET
@@ -326,13 +320,13 @@ TEXT rcr2(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT rcr4(SB), NOSPLIT, $0-8
+TEXT ·rcr4(SB), NOSPLIT, $0-8
 	MOVQ	CR4, AX
 	MOVQ	AX, ret+0(FP)
 	RET
 
 TEXT ·Rcr4(SB), NOSPLIT, $0-0
-	JMP	rcr4(SB)
+	JMP	·rcr4(SB)
 
 TEXT tlbflush(SB), NOSPLIT, $0-0
 	MOVQ	CR3, AX
@@ -354,7 +348,7 @@ TEXT runtime·Invlpg(SB), $0-8
 	INVLPG	(AX)
 	RET
 
-TEXT invlpg(SB), NOSPLIT, $0-8
+TEXT ·invlpg(SB), NOSPLIT, $0-8
 	MOVQ	va+0(FP), AX
 	INVLPG	(AX)
 	RET
@@ -388,12 +382,12 @@ TEXT ·ltr(SB), NOSPLIT, $0-8
 	BYTE $0xd8
 	RET
 
-TEXT lcr0(SB), NOSPLIT, $0-8
+TEXT ·lcr0(SB), NOSPLIT, $0-8
 	MOVQ	val+0(FP), AX
 	MOVQ	AX, CR0
 	RET
 
-TEXT lcr4(SB), NOSPLIT, $0-8
+TEXT ·lcr4(SB), NOSPLIT, $0-8
 	MOVQ	val+0(FP), AX
 	MOVQ	AX, CR4
 	RET
@@ -560,7 +554,7 @@ TEXT ·htpause(SB), NOSPLIT, $0-0
 	PAUSE
 	RET
 
-TEXT fxsave(SB), NOSPLIT, $0-8
+TEXT ·fxsave(SB), NOSPLIT, $0-8
 	MOVQ	dst+0(FP), AX
 	// fxsave	(%rax)
 	BYTE	$0x0f

commit a462abcffb2d074d365b07930e844e62b852af99
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 12:27:01 2016 -0500

    convert and cleanup interrupt init code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9575c8d49c..b7d5f090fb 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -226,31 +226,11 @@ h_needtls:
 	//CMPL	runtime·issolaris(SB), $1
 	//JEQ ok
 
-	PUSHQ	AX
-	CALL	·segsetup(SB)
-
-	POPQ	AX
-	MOVQ	8(AX), DI
-	PUSHQ	DI
-	MOVQ	(AX), DI
-	PUSHQ	DI
-	// lgdt (%rsp)
-	BYTE	$0x0f
-	BYTE	$0x01
-	BYTE	$0x14
-	BYTE	$0x24
-	POPQ	AX
-	POPQ	AX
+	CALL	·seg_setup(SB)
 
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
-	// setup tls
-	LEAQ	runtime·tls0(SB), DI
-	PUSHQ	DI
-	CALL	·fs0init(SB)
-	POPQ	AX
-
 	CALL	fpuinit(SB)
 
 	// store through it, to make sure it works
@@ -276,7 +256,7 @@ h_ok:
 	// save m0 to g0->m
 	MOVQ	AX, g_m(CX)
 
-	CALL	int_setup(SB)
+	CALL	·int_setup(SB)
 	CALL	proc_setup(SB)
 
 	//MOVQ	CR0, AX
@@ -379,19 +359,25 @@ TEXT invlpg(SB), NOSPLIT, $0-8
 	INVLPG	(AX)
 	RET
 
-TEXT lidt(SB), NOSPLIT, $0-8
-	MOVQ	idtpd+0(FP), AX
-	MOVQ	8(AX), DI
-	PUSHQ	DI
-	MOVQ	(AX), DI
-	PUSHQ	DI
-	// lidt	(%rsp)
+// void lidt(pdesc_t);
+TEXT ·lidt(SB), NOSPLIT, $0-16
+	// lidtq 8(%rsp)
+	BYTE	$0x48
 	BYTE	$0x0f
 	BYTE	$0x01
-	BYTE	$0x1c
+	BYTE	$0x5c
 	BYTE	$0x24
-	POPQ	AX
-	POPQ	AX
+	BYTE	$0x08
+	RET
+
+// void lgdt(pdesc_t);
+TEXT ·lgdt(SB), NOSPLIT, $0-16
+	// lgdt 8(%rsp)
+	BYTE	$0x0f
+	BYTE	$0x01
+	BYTE	$0x54
+	BYTE	$0x24
+	BYTE	$0x08
 	RET
 
 TEXT ·ltr(SB), NOSPLIT, $0-8
@@ -643,52 +629,52 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	POPQ	AX;			\
 	RET
 
-IH_NOEC( 0,Xdz )
-IH_NOEC( 1,Xrz )
-IH_NOEC( 2,Xnmi )
-IH_NOEC( 3,Xbp )
-IH_NOEC( 4,Xov )
-IH_NOEC( 5,Xbnd )
-IH_NOEC( 6,Xuo )
-IH_NOEC( 7,Xnm )
-IH_EC  ( 8,Xdf )
-IH_NOEC( 9,Xrz2 )
-IH_EC  (10,Xtss )
-IH_EC  (11,Xsnp )
-IH_EC  (12,Xssf )
-IH_EC  (13,Xgp )
-IH_EC  (14,Xpf )
-IH_NOEC(15,Xrz3 )
-IH_NOEC(16,Xmf )
-IH_EC  (17,Xac )
-IH_NOEC(18,Xmc )
-IH_NOEC(19,Xfp )
-IH_NOEC(20,Xve )
-IH_NOEC(32,Xtimer )
-IH_NOEC(48,Xspur )
-IH_NOEC(49,Xyield )
-IH_NOEC(64,Xsyscall )
-IH_NOEC(70,Xtlbshoot )
-IH_NOEC(71,Xsigret )
-IH_NOEC(72,Xperfmask )
+IH_NOEC( 0,·Xdz )
+IH_NOEC( 1,·Xrz )
+IH_NOEC( 2,·Xnmi )
+IH_NOEC( 3,·Xbp )
+IH_NOEC( 4,·Xov )
+IH_NOEC( 5,·Xbnd )
+IH_NOEC( 6,·Xuo )
+IH_NOEC( 7,·Xnm )
+IH_EC  ( 8,·Xdf )
+IH_NOEC( 9,·Xrz2 )
+IH_EC  (10,·Xtss )
+IH_EC  (11,·Xsnp )
+IH_EC  (12,·Xssf )
+IH_EC  (13,·Xgp )
+IH_EC  (14,·Xpf )
+IH_NOEC(15,·Xrz3 )
+IH_NOEC(16,·Xmf )
+IH_EC  (17,·Xac )
+IH_NOEC(18,·Xmc )
+IH_NOEC(19,·Xfp )
+IH_NOEC(20,·Xve )
+IH_NOEC(32,·Xtimer )
+IH_NOEC(48,·Xspur )
+IH_NOEC(49,·Xyield )
+IH_NOEC(64,·Xsyscall )
+IH_NOEC(70,·Xtlbshoot )
+IH_NOEC(71,·Xsigret )
+IH_NOEC(72,·Xperfmask )
 
 // irqs
 // irq0 is Xtimer
-IH_IRQ( 1,Xirq1 )
-IH_IRQ( 2,Xirq2 )
-IH_IRQ( 3,Xirq3 )
-IH_IRQ( 4,Xirq4 )
-IH_IRQ( 5,Xirq5 )
-IH_IRQ( 6,Xirq6 )
-IH_IRQ( 7,Xirq7 )
-IH_IRQ( 8,Xirq8 )
-IH_IRQ( 9,Xirq9 )
-IH_IRQ(10,Xirq10 )
-IH_IRQ(11,Xirq11 )
-IH_IRQ(12,Xirq12 )
-IH_IRQ(13,Xirq13 )
-IH_IRQ(14,Xirq14 )
-IH_IRQ(15,Xirq15 )
+IH_IRQ( 1,·Xirq1 )
+IH_IRQ( 2,·Xirq2 )
+IH_IRQ( 3,·Xirq3 )
+IH_IRQ( 4,·Xirq4 )
+IH_IRQ( 5,·Xirq5 )
+IH_IRQ( 6,·Xirq6 )
+IH_IRQ( 7,·Xirq7 )
+IH_IRQ( 8,·Xirq8 )
+IH_IRQ( 9,·Xirq9 )
+IH_IRQ(10,·Xirq10 )
+IH_IRQ(11,·Xirq11 )
+IH_IRQ(12,·Xirq12 )
+IH_IRQ(13,·Xirq13 )
+IH_IRQ(14,·Xirq14 )
+IH_IRQ(15,·Xirq15 )
 
 #define IA32_FS_BASE		$0xc0000100UL
 #define IA32_SYSENTER_ESP	$0x175UL

commit 9bcf96fb6b3400d4994aab3079f01c0fb0162add
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 08:22:19 2016 -0500

    convert and cleanup segmentation code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e5b5638a91..9575c8d49c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -226,13 +226,9 @@ h_needtls:
 	//CMPL	runtime·issolaris(SB), $1
 	//JEQ ok
 
-	// setup tls
 	PUSHQ	AX
-	LEAQ	runtime·tls0(SB), DI
-	PUSHQ	DI
-	CALL	segsetup(SB)
+	CALL	·segsetup(SB)
 
-	POPQ	AX
 	POPQ	AX
 	MOVQ	8(AX), DI
 	PUSHQ	DI
@@ -245,11 +241,16 @@ h_needtls:
 	BYTE	$0x24
 	POPQ	AX
 	POPQ	AX
-	POPQ	AX
 
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
+	// setup tls
+	LEAQ	runtime·tls0(SB), DI
+	PUSHQ	DI
+	CALL	·fs0init(SB)
+	POPQ	AX
+
 	CALL	fpuinit(SB)
 
 	// store through it, to make sure it works
@@ -393,7 +394,7 @@ TEXT lidt(SB), NOSPLIT, $0-8
 	POPQ	AX
 	RET
 
-TEXT ltr(SB), NOSPLIT, $0-8
+TEXT ·ltr(SB), NOSPLIT, $0-8
 	MOVQ	seg+0(FP), AX
 	// ltr	%ax
 	BYTE $0x0f
@@ -569,7 +570,7 @@ TEXT getret(SB), NOSPLIT, $0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT htpause(SB), NOSPLIT, $0-0
+TEXT ·htpause(SB), NOSPLIT, $0-0
 	PAUSE
 	RET
 
@@ -1038,13 +1039,13 @@ TEXT old_sysentry(SB), NOSPLIT, $0
 	CALL	sysentry(SB)
 	INT	$3
 
-TEXT gs_null(SB), NOSPLIT, $8-0
+TEXT ·gs_null(SB), NOSPLIT, $8-0
 	XORQ	AX, AX
 	PUSHQ	AX
 	POPQ	GS
 	RET
 
-TEXT fs_null(SB), NOSPLIT, $8-0
+TEXT ·fs_null(SB), NOSPLIT, $8-0
 	XORQ	AX, AX
 	PUSHQ	AX
 	POPQ	FS

commit 3b52f875747da9139455aff1a73b0f19e6ed712f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 4 16:45:19 2016 -0500

    start the probably long and terrible road to merging with 1.5
    
    will then merge with 1.6 if all goes well. 1.4 -> 1.5 will probably be more
    work than 1.5 -> 1.6 since i need to convert all my C code to go code.
    fortunately, conversion to go code has a few benefits.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 47e1934ced..e5b5638a91 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -331,12 +331,6 @@ TEXT runtime·Cpuid(SB), NOSPLIT, $0-24
 	MOVL	DX, ret+20(FP)
 	RET
 
-TEXT runtime·atomic_dec(SB), NOSPLIT, $0-8
-	MOVQ	addr+0(FP), AX
-	LOCK
-	DECQ	(AX)
-	RET
-
 TEXT finit(SB), NOSPLIT, $0-0
 	FINIT
 	RET
@@ -514,7 +508,7 @@ TEXT rrsp(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT runtime·Rdtsc(SB), NOSPLIT, $0-8
+TEXT ·Rdtsc(SB), NOSPLIT, $0-8
 	// rdtsc
 	BYTE	$0x0f
 	BYTE	$0x31
@@ -522,22 +516,22 @@ TEXT runtime·Rdtsc(SB), NOSPLIT, $0-8
 	MOVL	DX, ret+4(FP)
 	RET
 
-TEXT cli(SB), NOSPLIT, $0-0
+TEXT ·cli(SB), NOSPLIT, $0-0
 	CLI
 	RET
 
-TEXT sti(SB), NOSPLIT, $0-0
+TEXT ·sti(SB), NOSPLIT, $0-0
 	STI
 	RET
 
-TEXT runtime·Pushcli(SB), NOSPLIT, $0-8
+TEXT ·Pushcli(SB), NOSPLIT, $0-8
 	PUSHFQ
 	POPQ	AX
 	MOVQ	AX, ret+0(FP)
 	CLI
 	RET
 
-TEXT runtime·Popcli(SB), NOSPLIT, $0-8
+TEXT ·Popcli(SB), NOSPLIT, $0-8
 	MOVQ	fl+0(FP), AX
 	PUSHQ	AX
 	POPFQ
@@ -1056,7 +1050,7 @@ TEXT fs_null(SB), NOSPLIT, $8-0
 	POPQ	FS
 	RET
 
-TEXT runtime·Gscpu(SB), NOSPLIT, $0-8
+TEXT ·Gscpu(SB), NOSPLIT, $0-8
 	MOVQ	0(GS), AX
 	MOVQ	AX, ret+0(FP)
 	RET

commit 5fea2ccc77eb50a9704fa04b7c61755fe34e1d95
Author: Brad Fitzpatrick <bradfitz@golang.org>
Date:   Tue Mar 1 23:21:55 2016 +0000

    all: single space after period.
    
    The tree's pretty inconsistent about single space vs double space
    after a period in documentation. Make it consistently a single space,
    per earlier decisions. This means contributors won't be confused by
    misleading precedence.
    
    This CL doesn't use go/doc to parse. It only addresses // comments.
    It was generated with:
    
    $ perl -i -npe 's,^(\s*// .+[a-z]\.)  +([A-Z]),$1 $2,' $(git grep -l -E '^\s*//(.+\.)  +([A-Z])')
    $ go test go/doc -update
    
    Change-Id: Iccdb99c37c797ef1f804a94b22ba5ee4b500c4f7
    Reviewed-on: https://go-review.googlesource.com/20022
    Reviewed-by: Rob Pike <r@golang.org>
    Reviewed-by: Dave Day <djd@golang.org>
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ac4630c833..b4df1d80d7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -203,7 +203,7 @@ TEXT runtime·gogo(SB), NOSPLIT, $0-8
 
 // func mcall(fn func(*g))
 // Switch to m->g0's stack, call fn(g).
-// Fn must never return.  It should gogo(&g->sched)
+// Fn must never return. It should gogo(&g->sched)
 // to keep running g.
 TEXT runtime·mcall(SB), NOSPLIT, $0-8
 	MOVQ	fn+0(FP), DI
@@ -237,7 +237,7 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 	RET
 
 // systemstack_switch is a dummy routine that systemstack leaves at the bottom
-// of the G stack.  We need to distinguish the routine that
+// of the G stack. We need to distinguish the routine that
 // lives at the bottom of the G stack from the one that lives
 // at the top of the system stack because the one at the top of
 // the system stack terminates the stack walk (see topofstack()).
@@ -268,7 +268,7 @@ TEXT runtime·systemstack(SB), NOSPLIT, $0-8
 	CALL	AX
 
 switch:
-	// save our state in g->sched.  Pretend to
+	// save our state in g->sched. Pretend to
 	// be systemstack_switch if the G stack is scanned.
 	MOVQ	$runtime·systemstack_switch(SB), SI
 	MOVQ	SI, (g_sched+gobuf_pc)(AX)
@@ -716,7 +716,7 @@ havem:
 	CALL	runtime·cgocallbackg(SB)
 	MOVQ	0(SP), R8
 
-	// Compute the size of the frame again.  FP and SP have
+	// Compute the size of the frame again. FP and SP have
 	// completely different values here than they did above,
 	// but only their difference matters.
 	LEAQ	fv+0(FP), AX
@@ -909,7 +909,7 @@ final1:
 	RET
 
 endofpage:
-	// address ends in 1111xxxx.  Might be up against
+	// address ends in 1111xxxx. Might be up against
 	// a page boundary, so load ending at last byte.
 	// Then shift bytes down using pshufb.
 	MOVOU	-32(AX)(CX*1), X1
@@ -1232,7 +1232,7 @@ TEXT ·checkASM(SB),NOSPLIT,$0-1
 	SETEQ	ret+0(FP)
 	RET
 
-// these are arguments to pshufb.  They move data down from
+// these are arguments to pshufb. They move data down from
 // the high bytes of the register to the low bytes of the register.
 // index is how many bytes to move.
 DATA shifts<>+0x00(SB)/8, $0x0000000000000000
@@ -1412,7 +1412,7 @@ small:
 	MOVQ	(SI), SI
 	JMP	si_finish
 si_high:
-	// address ends in 11111xxx.  Load up to bytes we want, move to correct position.
+	// address ends in 11111xxx. Load up to bytes we want, move to correct position.
 	MOVQ	-8(SI)(BX*1), SI
 	SHRQ	CX, SI
 si_finish:
@@ -1872,7 +1872,7 @@ sseloopentry:
 	CMPQ	DI, AX
 	JB	sseloop
 
-	// Search the last 16-byte chunk.  This chunk may overlap with the
+	// Search the last 16-byte chunk. This chunk may overlap with the
 	// chunks we've already searched, but that's ok.
 	MOVQ	AX, DI
 	MOVOU	(AX), X1

commit 3c5f27f4d00edcb9515b7029905f46e079486ce0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Feb 26 17:16:00 2016 -0500

    fix more types

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2369e7a057..47e1934ced 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -356,6 +356,9 @@ TEXT rcr4(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
+TEXT ·Rcr4(SB), NOSPLIT, $0-0
+	JMP	rcr4(SB)
+
 TEXT tlbflush(SB), NOSPLIT, $0-0
 	MOVQ	CR3, AX
 	MOVQ	AX, CR3

commit 687abca1ea828dd4745d50c351f3b73ccd4d09be
Author: Keith Randall <khr@golang.org>
Date:   Fri Jan 15 18:17:09 2016 -0800

    runtime: avoid using REP prefix for IndexByte
    
    REP-prefixed instructions have a large startup cost.
    Avoid them like the plague.
    
    benchmark                  old ns/op     new ns/op     delta
    BenchmarkIndexByte10-8     22.4          5.34          -76.16%
    
    Fixes #13983
    
    Change-Id: I857e956e240fc9681d053f2584ccf24c1b272bb3
    Reviewed-on: https://go-review.googlesource.com/18703
    Reviewed-by: Minux Ma <minux@golang.org>
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 98a8e839ed..ac4630c833 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1838,80 +1838,98 @@ TEXT strings·IndexByte(SB),NOSPLIT,$0-32
 //   AL: byte sought
 //   R8: address to put result
 TEXT runtime·indexbytebody(SB),NOSPLIT,$0
-	MOVQ SI, DI
-
-	CMPQ BX, $16
-	JLT small
-
-	CMPQ BX, $32
-	JA avx2
-no_avx2:
-	// round up to first 16-byte boundary
-	TESTQ $15, SI
-	JZ aligned
-	MOVQ SI, CX
-	ANDQ $~15, CX
-	ADDQ $16, CX
-
-	// search the beginning
-	SUBQ SI, CX
-	REPN; SCASB
-	JZ success
-
-// DI is 16-byte aligned; get ready to search using SSE instructions
-aligned:
-	// round down to last 16-byte boundary
-	MOVQ BX, R11
-	ADDQ SI, R11
-	ANDQ $~15, R11
-
-	// shuffle X0 around so that each byte contains c
+	// Shuffle X0 around so that each byte contains
+	// the character we're looking for.
 	MOVD AX, X0
 	PUNPCKLBW X0, X0
 	PUNPCKLBW X0, X0
 	PSHUFL $0, X0, X0
-	JMP condition
+	
+	CMPQ BX, $16
+	JLT small
+
+	MOVQ SI, DI
 
+	CMPQ BX, $32
+	JA avx2
 sse:
-	// move the next 16-byte chunk of the buffer into X1
-	MOVO (DI), X1
-	// compare bytes in X0 to X1
-	PCMPEQB X0, X1
-	// take the top bit of each byte in X1 and put the result in DX
+	LEAQ	-16(SI)(BX*1), AX	// AX = address of last 16 bytes
+	JMP	sseloopentry
+	
+sseloop:
+	// Move the next 16-byte chunk of the data into X1.
+	MOVOU	(DI), X1
+	// Compare bytes in X0 to X1.
+	PCMPEQB	X0, X1
+	// Take the top bit of each byte in X1 and put the result in DX.
 	PMOVMSKB X1, DX
-	TESTL DX, DX
-	JNZ ssesuccess
-	ADDQ $16, DI
+	// Find first set bit, if any.
+	BSFL	DX, DX
+	JNZ	ssesuccess
+	// Advance to next block.
+	ADDQ	$16, DI
+sseloopentry:
+	CMPQ	DI, AX
+	JB	sseloop
 
-condition:
-	CMPQ DI, R11
-	JLT sse
-
-	// search the end
-	MOVQ SI, CX
-	ADDQ BX, CX
-	SUBQ R11, CX
-	// if CX == 0, the zero flag will be set and we'll end up
-	// returning a false success
-	JZ failure
-	REPN; SCASB
-	JZ success
+	// Search the last 16-byte chunk.  This chunk may overlap with the
+	// chunks we've already searched, but that's ok.
+	MOVQ	AX, DI
+	MOVOU	(AX), X1
+	PCMPEQB	X0, X1
+	PMOVMSKB X1, DX
+	BSFL	DX, DX
+	JNZ	ssesuccess
 
 failure:
 	MOVQ $-1, (R8)
 	RET
 
+// We've found a chunk containing the byte.
+// The chunk was loaded from DI.
+// The index of the matching byte in the chunk is DX.
+// The start of the data is SI.
+ssesuccess:
+	SUBQ SI, DI	// Compute offset of chunk within data.
+	ADDQ DX, DI	// Add offset of byte within chunk.
+	MOVQ DI, (R8)
+	RET
+
 // handle for lengths < 16
 small:
-	MOVQ BX, CX
-	REPN; SCASB
-	JZ success
-	MOVQ $-1, (R8)
+	TESTQ	BX, BX
+	JEQ	failure
+
+	// Check if we'll load across a page boundary.
+	LEAQ	16(SI), AX
+	TESTW	$0xff0, AX
+	JEQ	endofpage
+
+	MOVOU	(SI), X1 // Load data
+	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
+	PMOVMSKB X1, DX	// Move result bits to integer register.
+	BSFL	DX, DX	// Find first set bit.
+	JZ	failure	// No set bit, failure.
+	CMPL	DX, BX
+	JAE	failure	// Match is past end of data.
+	MOVQ	DX, (R8)
+	RET
+
+endofpage:
+	MOVOU	-16(SI)(BX*1), X1	// Load data into the high end of X1.
+	PCMPEQB	X0, X1	// Compare target byte with each byte in data.
+	PMOVMSKB X1, DX	// Move result bits to integer register.
+	MOVL	BX, CX
+	SHLL	CX, DX
+	SHRL	$16, DX	// Shift desired bits down to bottom of register.
+	BSFL	DX, DX	// Find first set bit.
+	JZ	failure	// No set bit, failure.
+	MOVQ	DX, (R8)
 	RET
 
 avx2:
 	CMPB   runtime·support_avx2(SB), $1
-	JNE no_avx2
+	JNE sse
 	MOVD AX, X0
 	LEAQ -32(SI)(BX*1), R11
 	VPBROADCASTB  X0, Y1
@@ -1941,22 +1959,6 @@ avx2success:
 	VZEROUPPER
 	RET
 
-// we've found the chunk containing the byte
-// now just figure out which specific byte it is
-ssesuccess:
-	// get the index of the least significant set bit
-	BSFW DX, DX
-	SUBQ SI, DI
-	ADDQ DI, DX
-	MOVQ DX, (R8)
-	RET
-
-success:
-	SUBQ SI, DI
-	SUBL $1, DI
-	MOVQ DI, (R8)
-	RET
-
 TEXT bytes·Equal(SB),NOSPLIT,$0-49
 	MOVQ	a_len+8(FP), BX
 	MOVQ	b_len+32(FP), CX

commit bd70bd9cb2f458b23222083a3a11190f080af7fd
Author: Keith Randall <khr@golang.org>
Date:   Mon Feb 22 13:20:38 2016 -0800

    runtime: unify memeq and memequal
    
    They do the same thing, except memequal also has the short-circuit
    check if the two pointers are equal.
    
    A) We might as well always do the short-circuit check, it is only 2 instructions.
    B) The extra function call (memequal->memeq) is expensive.
    
    benchmark                 old ns/op     new ns/op     delta
    BenchmarkArrayEqual-8     8.56          5.31          -37.97%
    
    No noticeable affect on the former memeq user (maps).
    
    Fixes #14302
    
    Change-Id: I85d1ada59ed11e64dd6c54667f79d32cc5f81948
    Reviewed-on: https://go-review.googlesource.com/19843
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 5094812a05..98a8e839ed 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1269,12 +1269,18 @@ DATA shifts<>+0xf0(SB)/8, $0x0807060504030201
 DATA shifts<>+0xf8(SB)/8, $0xff0f0e0d0c0b0a09
 GLOBL shifts<>(SB),RODATA,$256
 
-TEXT runtime·memeq(SB),NOSPLIT,$0-25
+// memequal(p, q unsafe.Pointer, size uintptr) bool
+TEXT runtime·memequal(SB),NOSPLIT,$0-25
 	MOVQ	a+0(FP), SI
 	MOVQ	b+8(FP), DI
+	CMPQ	SI, DI
+	JEQ	eq
 	MOVQ	size+16(FP), BX
 	LEAQ	ret+24(FP), AX
 	JMP	runtime·memeqbody(SB)
+eq:
+	MOVB	$1, ret+24(FP)
+	RET
 
 // memequal_varlen(a, b unsafe.Pointer) bool
 TEXT runtime·memequal_varlen(SB),NOSPLIT,$0-17

commit 96fa737ec3f75661d98cb3a2b00eb8cf6f4cdeed
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 10 09:33:43 2016 -0500

    performance profiling skeleton via NMI and performance counters
    
    and an IPI to enable/disable the counters on all available cores.
    
    should have done this a long time ago; preliminary profiles look promising!

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0ca06eeeca..2369e7a057 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -672,6 +672,7 @@ IH_NOEC(49,Xyield )
 IH_NOEC(64,Xsyscall )
 IH_NOEC(70,Xtlbshoot )
 IH_NOEC(71,Xsigret )
+IH_NOEC(72,Xperfmask )
 
 // irqs
 // irq0 is Xtimer

commit 8d881b811d8212ffd1d43e296f2a1c1bf78198ab
Author: Russ Cox <rsc@golang.org>
Date:   Fri Jan 22 22:25:15 2016 -0500

    cmd/asm: correct, complete newly added AVX instructions
    
    Use the standard names, for discoverability.
    Use the standard register arguments, for correctness.
    Implement all possible arguments, for completeness.
    Enable the corresponding tests now that everything is standard.
    Update the uses in package runtime.
    
    Fixes #14068.
    
    Change-Id: I8e1af9a41e7d02d98c2a82af3d4cdb3e9204824f
    Reviewed-on: https://go-review.googlesource.com/18852
    Run-TryBot: Russ Cox <rsc@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rob Pike <r@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cac032c370..5094812a05 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1350,14 +1350,14 @@ hugeloop:
 hugeloop_avx2:
 	CMPQ	BX, $64
 	JB	bigloop_avx2
-	MOVHDU	(SI), X0
-	MOVHDU	(DI), X1
-	MOVHDU	32(SI), X2
-	MOVHDU	32(DI), X3
-	VPCMPEQB	X1, X0, X4
-	VPCMPEQB	X2, X3, X5
-	VPAND	X4, X5, X6
-	VPMOVMSKB X6, DX
+	VMOVDQU	(SI), Y0
+	VMOVDQU	(DI), Y1
+	VMOVDQU	32(SI), Y2
+	VMOVDQU	32(DI), Y3
+	VPCMPEQB	Y1, Y0, Y4
+	VPCMPEQB	Y2, Y3, Y5
+	VPAND	Y4, Y5, Y6
+	VPMOVMSKB Y6, DX
 	ADDQ	$64, SI
 	ADDQ	$64, DI
 	SUBQ	$64, BX
@@ -1614,16 +1614,16 @@ big_loop:
 	// Compare 64-bytes per loop iteration.
 	// Loop is unrolled and uses AVX2.
 big_loop_avx2:
-	MOVHDU	(SI), X2
-	MOVHDU	(DI), X3
-	MOVHDU	32(SI), X4
-	MOVHDU	32(DI), X5
-	VPCMPEQB X2, X3, X0
-	VPMOVMSKB X0, AX
+	VMOVDQU	(SI), Y2
+	VMOVDQU	(DI), Y3
+	VMOVDQU	32(SI), Y4
+	VMOVDQU	32(DI), Y5
+	VPCMPEQB Y2, Y3, Y0
+	VPMOVMSKB Y0, AX
 	XORL	$0xffffffff, AX
 	JNE	diff32_avx2
-	VPCMPEQB X4, X5, X6
-	VPMOVMSKB X6, AX
+	VPCMPEQB Y4, Y5, Y6
+	VPMOVMSKB Y6, AX
 	XORL	$0xffffffff, AX
 	JNE	diff64_avx2
 
@@ -1908,26 +1908,26 @@ avx2:
 	JNE no_avx2
 	MOVD AX, X0
 	LEAQ -32(SI)(BX*1), R11
-	VPBROADCASTB  X0, X1
+	VPBROADCASTB  X0, Y1
 avx2_loop:
-	MOVHDU (DI), X2
-	VPCMPEQB X1, X2, X3
-	VPTEST X3, X3
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPTEST Y3, Y3
 	JNZ avx2success
 	ADDQ $32, DI
 	CMPQ DI, R11
 	JLT avx2_loop
 	MOVQ R11, DI
-	MOVHDU (DI), X2
-	VPCMPEQB X1, X2, X3
-	VPTEST X3, X3
+	VMOVDQU (DI), Y2
+	VPCMPEQB Y1, Y2, Y3
+	VPTEST Y3, Y3
 	JNZ avx2success
 	VZEROUPPER
 	MOVQ $-1, (R8)
 	RET
 
 avx2success:
-	VPMOVMSKB X3, DX
+	VPMOVMSKB Y3, DX
 	BSFL DX, DX
 	SUBQ SI, DI
 	ADDQ DI, DX

commit 1d1f2fb4c6e4da4a88a0ab8a0b43822d411a23ea
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Wed Jan 13 16:43:22 2016 +0300

    cmd/internal/obj/x86: add new instructions, cleanup.
    
    Add several instructions that were used via BYTE and use them.
    Instructions added: PEXTRB, PEXTRD, PEXTRQ, PINSRB, XGETBV, POPCNT.
    
    Change-Id: I5a80cd390dc01f3555dbbe856a475f74b5e6df65
    Reviewed-on: https://go-review.googlesource.com/18593
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6ebe0dc8e6..cac032c370 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -56,7 +56,7 @@ notintel:
 	JNE     noavx
 	MOVL    $0, CX
 	// For XGETBV, OSXSAVE bit is required and sufficient
-	BYTE $0x0F; BYTE $0x01; BYTE $0xD0
+	XGETBV
 	ANDL    $6, AX
 	CMPL    AX, $6 // Check for OS support of YMM registers
 	JNE     noavx
@@ -822,10 +822,10 @@ TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	CMPB	runtime·lfenceBeforeRdtsc(SB), $1
 	JNE	mfence
-	BYTE	$0x0f; BYTE $0xae; BYTE $0xe8 // LFENCE
+	LFENCE
 	JMP	done
 mfence:
-	BYTE	$0x0f; BYTE $0xae; BYTE $0xf0 // MFENCE
+	MFENCE
 done:
 	RDTSC
 	SHLQ	$32, DX

commit bb6fb929d6e5c2e401f4e3ebe2b7505845970e4b
Author: Austin Clements <austin@google.com>
Date:   Wed Nov 18 16:48:22 2015 -0500

    runtime: fix sanity check in stackBarrier
    
    stackBarrier on amd64 sanity checks that it's unwinding the correct
    entry in the stack barrier array. However, this check is wrong in two
    ways that make it unlikely to catch anything, right or wrong:
    
    1) It checks that savedLRPtr == SP, but, in fact, it should be that
       savedLRPtr+8 == SP because the RET that returned to stackBarrier
       popped the saved LR. However, we didn't notice this check was wrong
       because,
    
    2) the sense of the conditional branch is also wrong.
    
    Fix both of these.
    
    Change-Id: I38ba1f652b0168b5b2c11b81637656241262af7c
    Reviewed-on: https://go-review.googlesource.com/17039
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 705238cb6d..6ebe0dc8e6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -378,8 +378,9 @@ TEXT runtime·stackBarrier(SB),NOSPLIT,$0
 	MOVQ	stkbar_savedLRPtr(DX)(BX*1), R8
 	MOVQ	stkbar_savedLRVal(DX)(BX*1), BX
 	// Assert that we're popping the right saved LR.
+	ADDQ	$8, R8
 	CMPQ	R8, SP
-	JNE	2(PC)
+	JEQ	2(PC)
 	MOVL	$0, 0
 	// Record that this stack barrier was hit.
 	ADDQ	$1, g_stkbarPos(CX)

commit 3583a44ed2ae27495626caaa431b197187dcf01b
Author: Shenghou Ma <minux@golang.org>
Date:   Thu Sep 3 02:44:26 2015 -0400

    runtime: check that masks and shifts are correct aligned
    
    We need a runtime check because the original issue is encountered
    when running cross compiled windows program from linux. It's better
    to give a meaningful crash message earlier than to segfault later.
    
    The added test should not impose any measurable overhead to Go
    programs.
    
    For #12415.
    
    Change-Id: Ib4a24ef560c09c0585b351d62eefd157b6b7f04c
    Reviewed-on: https://go-review.googlesource.com/14207
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Minux Ma <minux@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7f14f61e2a..705238cb6d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1222,6 +1222,15 @@ DATA masks<>+0xf0(SB)/8, $0xffffffffffffffff
 DATA masks<>+0xf8(SB)/8, $0x00ffffffffffffff
 GLOBL masks<>(SB),RODATA,$256
 
+TEXT ·checkASM(SB),NOSPLIT,$0-1
+	// check that masks<>(SB) and shifts<>(SB) are aligned to 16-byte
+	MOVQ	$masks<>(SB), AX
+	MOVQ	$shifts<>(SB), BX
+	ORQ	BX, AX
+	TESTQ	$15, AX
+	SETEQ	ret+0(FP)
+	RET
+
 // these are arguments to pshufb.  They move data down from
 // the high bytes of the register to the low bytes of the register.
 // index is how many bytes to move.

commit 3af29fb8585f4dff0e845c00b0109d94d85ff6ed
Author: Russ Cox <rsc@golang.org>
Date:   Thu Nov 19 15:51:39 2015 -0500

    runtime: make asmcgocall work without a g
    
    Solaris needs to make system calls without a g,
    and Solaris uses asmcgocall to make system calls.
    I know, I know.
    
    I hope this makes CL 16915, fixing #12277, work on Solaris.
    
    Change-Id: If988dfd37f418b302da9c7096f598e5113ecea87
    Reviewed-on: https://go-review.googlesource.com/17072
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Reviewed-by: Aram Hăvărneanu <aram@mgk.ro>
    Run-TryBot: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4e5e8f7512..7f14f61e2a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -550,6 +550,8 @@ TEXT ·asmcgocall(SB),NOSPLIT,$0-20
 	// come in on the m->g0 stack already.
 	get_tls(CX)
 	MOVQ	g(CX), R8
+	CMPQ	R8, $0
+	JEQ	nosave
 	MOVQ	g_m(R8), R8
 	MOVQ	m_g0(R8), SI
 	MOVQ	g(CX), DI
@@ -559,11 +561,11 @@ TEXT ·asmcgocall(SB),NOSPLIT,$0-20
 	CMPQ	SI, DI
 	JEQ	nosave
 	
+	// Switch to system stack.
 	MOVQ	m_g0(R8), SI
 	CALL	gosave<>(SB)
 	MOVQ	SI, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(SI), SP
-nosave:
 
 	// Now on a scheduling stack (a pthread-created stack).
 	// Make sure we have enough room for 4 stack-backed fast-call
@@ -589,6 +591,29 @@ nosave:
 	MOVL	AX, ret+16(FP)
 	RET
 
+nosave:
+	// Running on a system stack, perhaps even without a g.
+	// Having no g can happen during thread creation or thread teardown
+	// (see needm/dropm on Solaris, for example).
+	// This code is like the above sequence but without saving/restoring g
+	// and without worrying about the stack moving out from under us
+	// (because we're on a system stack, not a goroutine stack).
+	// The above code could be used directly if already on a system stack,
+	// but then the only path through this code would be a rare case on Solaris.
+	// Using this code for all "already on system stack" calls exercises it more,
+	// which should help keep it correct.
+	SUBQ	$64, SP
+	ANDQ	$~15, SP
+	MOVQ	$0, 48(SP)		// where above code stores g, in case someone looks during debugging
+	MOVQ	DX, 40(SP)	// save original stack pointer
+	MOVQ	BX, DI		// DI = first argument in AMD64 ABI
+	MOVQ	BX, CX		// CX = first argument in Win64
+	CALL	AX
+	MOVQ	40(SP), SI	// restore original stack pointer
+	MOVQ	SI, SP
+	MOVL	AX, ret+16(FP)
+	RET
+
 // cgocallback(void (*fn)(void*), void *frame, uintptr framesize)
 // Turn the fn into a Go func (by taking its address) and call
 // cgocallback_gofunc.

commit 7bb38f6e470995d54a8bac3a67f997efc1f60c69
Author: Matthew Dempsky <mdempsky@google.com>
Date:   Thu Nov 12 15:35:50 2015 -0800

    runtime: replace tls0 with m0.tls
    
    We're allocating TLS storage for m0 anyway, so might as well use it.
    
    Change-Id: I7dc20bbea5320c8ab8a367f18a9540706751e771
    Reviewed-on: https://go-review.googlesource.com/16890
    Run-TryBot: Matthew Dempsky <mdempsky@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Michael Hudson-Doyle <michael.hudson@canonical.com>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2f8940a678..4e5e8f7512 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -104,13 +104,13 @@ needtls:
 	JMP ok
 #endif
 
-	LEAQ	runtime·tls0(SB), DI
+	LEAQ	runtime·m0+m_tls(SB), DI
 	CALL	runtime·settls(SB)
 
 	// store through it, to make sure it works
 	get_tls(BX)
 	MOVQ	$0x123, g(BX)
-	MOVQ	runtime·tls0(SB), AX
+	MOVQ	runtime·m0+m_tls(SB), AX
 	CMPQ	AX, $0x123
 	JEQ 2(PC)
 	MOVL	AX, 0	// abort

commit 67faca7d9c54b367aee5fdeef2d5dd609fcf99d0
Author: Michael Matloob <matloob@golang.org>
Date:   Mon Nov 2 14:09:24 2015 -0500

    runtime: break atomics out into package runtime/internal/atomic
    
    This change breaks out most of the atomics functions in the runtime
    into package runtime/internal/atomic. It adds some basic support
    in the toolchain for runtime packages, and also modifies linux/arm
    atomics to remove the dependency on the runtime's mutex. The mutexes
    have been replaced with spinlocks.
    
    all trybots are happy!
    In addition to the trybots, I've tested on the darwin/arm64 builder,
    on the darwin/arm builder, and on a ppc64le machine.
    
    Change-Id: I6698c8e3cf3834f55ce5824059f44d00dc8e3c2f
    Reviewed-on: https://go-review.googlesource.com/14204
    Run-TryBot: Michael Matloob <matloob@golang.org>
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 68b342d4db..2f8940a678 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -495,111 +495,6 @@ CALLFN(·call268435456, 268435456)
 CALLFN(·call536870912, 536870912)
 CALLFN(·call1073741824, 1073741824)
 
-// bool cas(int32 *val, int32 old, int32 new)
-// Atomically:
-//	if(*val == old){
-//		*val = new;
-//		return 1;
-//	} else
-//		return 0;
-TEXT runtime·cas(SB), NOSPLIT, $0-17
-	MOVQ	ptr+0(FP), BX
-	MOVL	old+8(FP), AX
-	MOVL	new+12(FP), CX
-	LOCK
-	CMPXCHGL	CX, 0(BX)
-	SETEQ	ret+16(FP)
-	RET
-
-// bool	runtime·cas64(uint64 *val, uint64 old, uint64 new)
-// Atomically:
-//	if(*val == *old){
-//		*val = new;
-//		return 1;
-//	} else {
-//		return 0;
-//	}
-TEXT runtime·cas64(SB), NOSPLIT, $0-25
-	MOVQ	ptr+0(FP), BX
-	MOVQ	old+8(FP), AX
-	MOVQ	new+16(FP), CX
-	LOCK
-	CMPXCHGQ	CX, 0(BX)
-	SETEQ	ret+24(FP)
-	RET
-	
-TEXT runtime·casuintptr(SB), NOSPLIT, $0-25
-	JMP	runtime·cas64(SB)
-
-TEXT runtime·atomicloaduintptr(SB), NOSPLIT, $0-16
-	JMP	runtime·atomicload64(SB)
-
-TEXT runtime·atomicloaduint(SB), NOSPLIT, $0-16
-	JMP	runtime·atomicload64(SB)
-
-TEXT runtime·atomicstoreuintptr(SB), NOSPLIT, $0-16
-	JMP	runtime·atomicstore64(SB)
-
-// bool casp(void **val, void *old, void *new)
-// Atomically:
-//	if(*val == old){
-//		*val = new;
-//		return 1;
-//	} else
-//		return 0;
-TEXT runtime·casp1(SB), NOSPLIT, $0-25
-	MOVQ	ptr+0(FP), BX
-	MOVQ	old+8(FP), AX
-	MOVQ	new+16(FP), CX
-	LOCK
-	CMPXCHGQ	CX, 0(BX)
-	SETEQ	ret+24(FP)
-	RET
-
-// uint32 xadd(uint32 volatile *val, int32 delta)
-// Atomically:
-//	*val += delta;
-//	return *val;
-TEXT runtime·xadd(SB), NOSPLIT, $0-20
-	MOVQ	ptr+0(FP), BX
-	MOVL	delta+8(FP), AX
-	MOVL	AX, CX
-	LOCK
-	XADDL	AX, 0(BX)
-	ADDL	CX, AX
-	MOVL	AX, ret+16(FP)
-	RET
-
-TEXT runtime·xadd64(SB), NOSPLIT, $0-24
-	MOVQ	ptr+0(FP), BX
-	MOVQ	delta+8(FP), AX
-	MOVQ	AX, CX
-	LOCK
-	XADDQ	AX, 0(BX)
-	ADDQ	CX, AX
-	MOVQ	AX, ret+16(FP)
-	RET
-
-TEXT runtime·xadduintptr(SB), NOSPLIT, $0-24
-	JMP	runtime·xadd64(SB)
-
-TEXT runtime·xchg(SB), NOSPLIT, $0-20
-	MOVQ	ptr+0(FP), BX
-	MOVL	new+8(FP), AX
-	XCHGL	AX, 0(BX)
-	MOVL	AX, ret+16(FP)
-	RET
-
-TEXT runtime·xchg64(SB), NOSPLIT, $0-24
-	MOVQ	ptr+0(FP), BX
-	MOVQ	new+8(FP), AX
-	XCHGQ	AX, 0(BX)
-	MOVQ	AX, ret+16(FP)
-	RET
-
-TEXT runtime·xchguintptr(SB), NOSPLIT, $0-24
-	JMP	runtime·xchg64(SB)
-
 TEXT runtime·procyield(SB),NOSPLIT,$0-0
 	MOVL	cycles+0(FP), AX
 again:
@@ -608,39 +503,6 @@ again:
 	JNZ	again
 	RET
 
-TEXT runtime·atomicstorep1(SB), NOSPLIT, $0-16
-	MOVQ	ptr+0(FP), BX
-	MOVQ	val+8(FP), AX
-	XCHGQ	AX, 0(BX)
-	RET
-
-TEXT runtime·atomicstore(SB), NOSPLIT, $0-12
-	MOVQ	ptr+0(FP), BX
-	MOVL	val+8(FP), AX
-	XCHGL	AX, 0(BX)
-	RET
-
-TEXT runtime·atomicstore64(SB), NOSPLIT, $0-16
-	MOVQ	ptr+0(FP), BX
-	MOVQ	val+8(FP), AX
-	XCHGQ	AX, 0(BX)
-	RET
-
-// void	runtime·atomicor8(byte volatile*, byte);
-TEXT runtime·atomicor8(SB), NOSPLIT, $0-9
-	MOVQ	ptr+0(FP), AX
-	MOVB	val+8(FP), BX
-	LOCK
-	ORB	BX, (AX)
-	RET
-
-// void	runtime·atomicand8(byte volatile*, byte);
-TEXT runtime·atomicand8(SB), NOSPLIT, $0-9
-	MOVQ	ptr+0(FP), AX
-	MOVB	val+8(FP), BX
-	LOCK
-	ANDB	BX, (AX)
-	RET
 
 TEXT ·publicationBarrier(SB),NOSPLIT,$0-0
 	// Stores are already ordered on x86, so this is just a

commit 321a40721bbfcb9bcf6113d4e8afd1bc030f1d8f
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Thu Oct 29 18:52:22 2015 +0300

    runtime: optimize indexbytebody on amd64
    
    Use avx2 to compare 32 bytes per iteration.
    Results (haswell):
    
    name                    old time/op    new time/op     delta
    IndexByte32-6             15.5ns ± 0%     14.7ns ± 5%   -4.87%        (p=0.000 n=16+20)
    IndexByte4K-6              360ns ± 0%      183ns ± 0%  -49.17%        (p=0.000 n=19+20)
    IndexByte4M-6              384µs ± 0%      256µs ± 1%  -33.41%        (p=0.000 n=20+20)
    IndexByte64M-6            6.20ms ± 0%     4.18ms ± 1%  -32.52%        (p=0.000 n=19+20)
    IndexBytePortable32-6     73.4ns ± 5%     75.8ns ± 3%   +3.35%        (p=0.000 n=20+19)
    IndexBytePortable4K-6     5.15µs ± 0%     5.15µs ± 0%     ~     (all samples are equal)
    IndexBytePortable4M-6     5.26ms ± 0%     5.25ms ± 0%   -0.12%        (p=0.000 n=20+18)
    IndexBytePortable64M-6    84.1ms ± 0%     84.1ms ± 0%   -0.08%        (p=0.012 n=18+20)
    Index32-6                  352ns ± 0%      352ns ± 0%     ~     (all samples are equal)
    Index4K-6                 53.8µs ± 0%     53.8µs ± 0%   -0.03%        (p=0.000 n=16+18)
    Index4M-6                 55.4ms ± 0%     55.4ms ± 0%     ~           (p=0.149 n=20+19)
    Index64M-6                 886ms ± 0%      886ms ± 0%     ~           (p=0.108 n=20+20)
    IndexEasy32-6             80.3ns ± 0%     80.1ns ± 0%   -0.21%        (p=0.000 n=20+20)
    IndexEasy4K-6              426ns ± 0%      215ns ± 0%  -49.53%        (p=0.000 n=20+20)
    IndexEasy4M-6              388µs ± 0%      262µs ± 1%  -32.42%        (p=0.000 n=18+20)
    IndexEasy64M-6            6.20ms ± 0%     4.19ms ± 1%  -32.47%        (p=0.000 n=18+20)
    
    name                    old speed      new speed       delta
    IndexByte32-6           2.06GB/s ± 1%   2.17GB/s ± 5%   +5.19%        (p=0.000 n=18+20)
    IndexByte4K-6           11.4GB/s ± 0%   22.3GB/s ± 0%  +96.45%        (p=0.000 n=17+20)
    IndexByte4M-6           10.9GB/s ± 0%   16.4GB/s ± 1%  +50.17%        (p=0.000 n=20+20)
    IndexByte64M-6          10.8GB/s ± 0%   16.0GB/s ± 1%  +48.19%        (p=0.000 n=19+20)
    IndexBytePortable32-6    436MB/s ± 5%    422MB/s ± 3%   -3.27%        (p=0.000 n=20+19)
    IndexBytePortable4K-6    795MB/s ± 0%    795MB/s ± 0%     ~           (p=0.940 n=17+18)
    IndexBytePortable4M-6    798MB/s ± 0%    799MB/s ± 0%   +0.12%        (p=0.000 n=20+18)
    IndexBytePortable64M-6   798MB/s ± 0%    798MB/s ± 0%   +0.08%        (p=0.011 n=18+20)
    Index32-6               90.9MB/s ± 0%   90.9MB/s ± 0%   -0.00%        (p=0.025 n=20+20)
    Index4K-6               76.1MB/s ± 0%   76.1MB/s ± 0%   +0.03%        (p=0.000 n=14+15)
    Index4M-6               75.7MB/s ± 0%   75.7MB/s ± 0%     ~           (p=0.076 n=20+19)
    Index64M-6              75.7MB/s ± 0%   75.7MB/s ± 0%     ~           (p=0.456 n=20+17)
    IndexEasy32-6            399MB/s ± 0%    399MB/s ± 0%   +0.20%        (p=0.000 n=20+19)
    IndexEasy4K-6           9.60GB/s ± 0%  19.02GB/s ± 0%  +98.19%        (p=0.000 n=20+20)
    IndexEasy4M-6           10.8GB/s ± 0%   16.0GB/s ± 1%  +47.98%        (p=0.000 n=18+20)
    IndexEasy64M-6          10.8GB/s ± 0%   16.0GB/s ± 1%  +48.08%        (p=0.000 n=18+20)
    
    Change-Id: I46075921dde9f3580a89544c0b3a2d8c9181ebc4
    Reviewed-on: https://go-review.googlesource.com/16484
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    Reviewed-by: Klaus Post <klauspost@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8401accbcd..68b342d4db 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1940,6 +1940,9 @@ TEXT runtime·indexbytebody(SB),NOSPLIT,$0
 	CMPQ BX, $16
 	JLT small
 
+	CMPQ BX, $32
+	JA avx2
+no_avx2:
 	// round up to first 16-byte boundary
 	TESTQ $15, SI
 	JZ aligned
@@ -2003,6 +2006,38 @@ small:
 	MOVQ $-1, (R8)
 	RET
 
+avx2:
+	CMPB   runtime·support_avx2(SB), $1
+	JNE no_avx2
+	MOVD AX, X0
+	LEAQ -32(SI)(BX*1), R11
+	VPBROADCASTB  X0, X1
+avx2_loop:
+	MOVHDU (DI), X2
+	VPCMPEQB X1, X2, X3
+	VPTEST X3, X3
+	JNZ avx2success
+	ADDQ $32, DI
+	CMPQ DI, R11
+	JLT avx2_loop
+	MOVQ R11, DI
+	MOVHDU (DI), X2
+	VPCMPEQB X1, X2, X3
+	VPTEST X3, X3
+	JNZ avx2success
+	VZEROUPPER
+	MOVQ $-1, (R8)
+	RET
+
+avx2success:
+	VPMOVMSKB X3, DX
+	BSFL DX, DX
+	SUBQ SI, DI
+	ADDQ DI, DX
+	MOVQ DX, (R8)
+	VZEROUPPER
+	RET
+
 // we've found the chunk containing the byte
 // now just figure out which specific byte it is
 ssesuccess:

commit 967564be7eef0575235e838839c7847da7723378
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Thu Oct 29 17:17:05 2015 +0300

    runtime: optimize string comparison on amd64
    
    Use AVX2 if possible.
    Results below (haswell):
    
    name                            old time/op    new time/op     delta
    CompareStringEqual-6              8.77ns ± 0%     8.63ns ± 1%   -1.58%        (p=0.000 n=20+19)
    CompareStringIdentical-6          5.02ns ± 0%     5.02ns ± 0%     ~     (all samples are equal)
    CompareStringSameLength-6         7.51ns ± 0%     7.51ns ± 0%     ~     (all samples are equal)
    CompareStringDifferentLength-6    1.56ns ± 0%     1.56ns ± 0%     ~     (all samples are equal)
    CompareStringBigUnaligned-6        124µs ± 1%      105µs ± 5%  -14.99%        (p=0.000 n=20+18)
    CompareStringBig-6                 112µs ± 1%      103µs ± 0%   -7.87%        (p=0.000 n=20+17)
    
    name                            old speed      new speed       delta
    CompareStringBigUnaligned-6     8.48GB/s ± 1%   9.98GB/s ± 5%  +17.67%        (p=0.000 n=20+18)
    CompareStringBig-6              9.37GB/s ± 1%  10.17GB/s ± 0%   +8.54%        (p=0.000 n=20+17)
    
    Change-Id: I1c949626dd2aaf9f633e3c888a9df71c82eed7e1
    Reviewed-on: https://go-review.googlesource.com/16481
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Klaus Post <klauspost@gmail.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2ba3d3d106..8401accbcd 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1416,6 +1416,10 @@ eq:
 TEXT runtime·memeqbody(SB),NOSPLIT,$0-0
 	CMPQ	BX, $8
 	JB	small
+	CMPQ	BX, $64
+	JB	bigloop
+	CMPB    runtime·support_avx2(SB), $1
+	JE	hugeloop_avx2
 	
 	// 64 bytes at a time using xmm registers
 hugeloop:
@@ -1445,6 +1449,30 @@ hugeloop:
 	MOVB	$0, (AX)
 	RET
 
+	// 64 bytes at a time using ymm registers
+hugeloop_avx2:
+	CMPQ	BX, $64
+	JB	bigloop_avx2
+	MOVHDU	(SI), X0
+	MOVHDU	(DI), X1
+	MOVHDU	32(SI), X2
+	MOVHDU	32(DI), X3
+	VPCMPEQB	X1, X0, X4
+	VPCMPEQB	X2, X3, X5
+	VPAND	X4, X5, X6
+	VPMOVMSKB X6, DX
+	ADDQ	$64, SI
+	ADDQ	$64, DI
+	SUBQ	$64, BX
+	CMPL	DX, $0xffffffff
+	JEQ	hugeloop_avx2
+	VZEROUPPER
+	MOVB	$0, (AX)
+	RET
+
+bigloop_avx2:
+	VZEROUPPER
+
 	// 8 bytes at a time using 64-bit register
 bigloop:
 	CMPQ	BX, $8

commit 95333aea53e1476587e29a55e3e4f34ccf61ce6a
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Wed Oct 28 18:05:05 2015 +0300

    strings: add asm version of Index() for short strings on amd64
    
    Currently we have special case for 1-byte strings,
    This extends this to strings shorter than 32 bytes on amd64.
    Results (broadwell):
    
    name                 old time/op  new time/op  delta
    IndexRune-4          57.4ns ± 0%  57.5ns ± 0%   +0.10%        (p=0.000 n=20+19)
    IndexRuneFastPath-4  20.4ns ± 0%  20.4ns ± 0%     ~     (all samples are equal)
    Index-4              21.0ns ± 0%  21.8ns ± 0%   +3.81%        (p=0.000 n=20+20)
    LastIndex-4          7.07ns ± 1%  6.98ns ± 0%   -1.21%        (p=0.000 n=20+16)
    IndexByte-4          18.3ns ± 0%  18.3ns ± 0%     ~     (all samples are equal)
    IndexHard1-4         1.46ms ± 0%  0.39ms ± 0%  -73.06%        (p=0.000 n=16+16)
    IndexHard2-4         1.46ms ± 0%  0.30ms ± 0%  -79.55%        (p=0.000 n=18+18)
    IndexHard3-4         1.46ms ± 0%  0.66ms ± 0%  -54.68%        (p=0.000 n=19+19)
    LastIndexHard1-4     1.46ms ± 0%  1.46ms ± 0%   -0.01%        (p=0.036 n=18+20)
    LastIndexHard2-4     1.46ms ± 0%  1.46ms ± 0%     ~           (p=0.588 n=19+19)
    LastIndexHard3-4     1.46ms ± 0%  1.46ms ± 0%     ~           (p=0.283 n=17+20)
    IndexTorture-4       11.1µs ± 0%  11.1µs ± 0%   +0.01%        (p=0.000 n=18+17)
    
    Change-Id: I892781549f558f698be4e41f9f568e3d0611efb5
    Reviewed-on: https://go-review.googlesource.com/16430
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 33d641e612..2ba3d3d106 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1725,6 +1725,168 @@ big_loop_avx2_exit:
 	JMP loop
 
 
+// TODO: Also use this in bytes.Index
+TEXT strings·indexShortStr(SB),NOSPLIT,$0-40
+	MOVQ s+0(FP), DI
+	MOVQ s_len+8(FP), CX
+	MOVQ c+16(FP), AX
+	MOVQ c_len+24(FP), BX
+	CMPQ BX, CX
+	JA fail
+	CMPQ BX, $2
+	JA   _3_or_more
+	MOVW (AX), AX
+	LEAQ -1(DI)(CX*1), CX
+loop2:
+	MOVW (DI), SI
+	CMPW SI,AX
+	JZ success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop2
+	JMP fail
+_3_or_more:
+	CMPQ BX, $3
+	JA   _4_or_more
+	MOVW 1(AX), DX
+	MOVW (AX), AX
+	LEAQ -2(DI)(CX*1), CX
+loop3:
+	MOVW (DI), SI
+	CMPW SI,AX
+	JZ   partial_success3
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop3
+	JMP fail
+partial_success3:
+	MOVW 1(DI), SI
+	CMPW SI,DX
+	JZ success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop3
+	JMP fail
+_4_or_more:
+	CMPQ BX, $4
+	JA   _5_or_more
+	MOVL (AX), AX
+	LEAQ -3(DI)(CX*1), CX
+loop4:
+	MOVL (DI), SI
+	CMPL SI,AX
+	JZ   success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop4
+	JMP fail
+_5_or_more:
+	CMPQ BX, $7
+	JA   _8_or_more
+	LEAQ 1(DI)(CX*1), CX
+	SUBQ BX, CX
+	MOVL -4(AX)(BX*1), DX
+	MOVL (AX), AX
+loop5to7:
+	MOVL (DI), SI
+	CMPL SI,AX
+	JZ   partial_success5to7
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop5to7
+	JMP fail
+partial_success5to7:
+	MOVL -4(BX)(DI*1), SI
+	CMPL SI,DX
+	JZ success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop5to7
+	JMP fail
+_8_or_more:
+	CMPQ BX, $8
+	JA   _9_or_more
+	MOVQ (AX), AX
+	LEAQ -7(DI)(CX*1), CX
+loop8:
+	MOVQ (DI), SI
+	CMPQ SI,AX
+	JZ   success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop8
+	JMP fail
+_9_or_more:
+	CMPQ BX, $16
+	JA   _16_or_more
+	LEAQ 1(DI)(CX*1), CX
+	SUBQ BX, CX
+	MOVQ -8(AX)(BX*1), DX
+	MOVQ (AX), AX
+loop9to15:
+	MOVQ (DI), SI
+	CMPQ SI,AX
+	JZ   partial_success9to15
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop9to15
+	JMP fail
+partial_success9to15:
+	MOVQ -8(BX)(DI*1), SI
+	CMPQ SI,DX
+	JZ success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop9to15
+	JMP fail
+_16_or_more:
+	CMPQ BX, $16
+	JA   _17_to_31
+	MOVOU (AX), X1
+	LEAQ -15(DI)(CX*1), CX
+loop16:
+	MOVOU (DI), X2
+	PCMPEQB X1, X2
+	PMOVMSKB X2, SI
+	CMPQ  SI, $0xffff
+	JE   success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop16
+	JMP fail
+_17_to_31:
+	LEAQ 1(DI)(CX*1), CX
+	SUBQ BX, CX
+	MOVOU -16(AX)(BX*1), X0
+	MOVOU (AX), X1
+loop17to31:
+	MOVOU (DI), X2
+	PCMPEQB X1,X2
+	PMOVMSKB X2, SI
+	CMPQ  SI, $0xffff
+	JE   partial_success17to31
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop17to31
+	JMP fail
+partial_success17to31:
+	MOVOU -16(BX)(DI*1), X3
+	PCMPEQB X0, X3
+	PMOVMSKB X3, SI
+	CMPQ  SI, $0xffff
+	JE success
+	ADDQ $1,DI
+	CMPQ DI,CX
+	JB loop17to31
+fail:
+	MOVQ $-1, ret+32(FP)
+	RET
+success:
+	SUBQ s+0(FP), DI
+	MOVQ DI, ret+32(FP)
+	RET
+
+
 TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX

commit 0e23ca41d99c82d301badf1b762888e2c69e6c57
Author: Ilya Tocar <ilya.tocar@intel.com>
Date:   Wed Oct 28 23:20:26 2015 +0300

    bytes: speed up Compare() on amd64
    
    Use AVX2 if available.
    Results (haswell), below:
    
    name                           old time/op    new time/op     delta
    BytesCompare1-6                  11.4ns ± 0%     11.4ns ± 0%     ~     (all samples are equal)
    BytesCompare2-6                  11.4ns ± 0%     11.4ns ± 0%     ~     (all samples are equal)
    BytesCompare4-6                  11.4ns ± 0%     11.4ns ± 0%     ~     (all samples are equal)
    BytesCompare8-6                  9.29ns ± 2%     8.76ns ± 0%   -5.72%        (p=0.000 n=16+17)
    BytesCompare16-6                 9.29ns ± 2%     9.20ns ± 0%   -1.02%        (p=0.000 n=20+16)
    BytesCompare32-6                 11.4ns ± 1%     11.4ns ± 0%     ~           (p=0.191 n=20+20)
    BytesCompare64-6                 14.4ns ± 0%     13.1ns ± 0%   -8.68%        (p=0.000 n=20+20)
    BytesCompare128-6                20.2ns ± 0%     18.5ns ± 0%   -8.27%        (p=0.000 n=16+20)
    BytesCompare256-6                29.3ns ± 0%     24.5ns ± 0%  -16.38%        (p=0.000 n=16+16)
    BytesCompare512-6                46.8ns ± 0%     37.1ns ± 0%  -20.78%        (p=0.000 n=18+16)
    BytesCompare1024-6               82.9ns ± 0%     62.3ns ± 0%  -24.86%        (p=0.000 n=20+14)
    BytesCompare2048-6                155ns ± 0%      112ns ± 0%  -27.74%        (p=0.000 n=20+20)
    CompareBytesEqual-6              10.1ns ± 1%     10.0ns ± 1%     ~           (p=0.527 n=20+20)
    CompareBytesToNil-6              10.0ns ± 2%      9.4ns ± 0%   -6.57%        (p=0.000 n=20+17)
    CompareBytesEmpty-6              8.76ns ± 0%     8.76ns ± 0%     ~     (all samples are equal)
    CompareBytesIdentical-6          8.76ns ± 0%     8.76ns ± 0%     ~     (all samples are equal)
    CompareBytesSameLength-6         10.6ns ± 1%     10.6ns ± 1%     ~           (p=0.240 n=20+20)
    CompareBytesDifferentLength-6    10.6ns ± 0%     10.6ns ± 1%     ~           (p=1.000 n=20+20)
    CompareBytesBigUnaligned-6        132±s ± 1%      105±s ± 1%  -20.61%        (p=0.000 n=20+18)
    CompareBytesBig-6                 125±s ± 1%      105±s ± 1%  -16.31%        (p=0.000 n=20+20)
    CompareBytesBigIdentical-6       8.13ns ± 0%     8.13ns ± 0%     ~     (all samples are equal)
    
    name                           old speed      new speed       delta
    CompareBytesBigUnaligned-6     7.94GB/s ± 1%  10.01GB/s ± 1%  +25.96%        (p=0.000 n=20+18)
    CompareBytesBig-6              8.38GB/s ± 1%  10.01GB/s ± 1%  +19.48%        (p=0.000 n=20+20)
    CompareBytesBigIdentical-6      129TB/s ± 0%    129TB/s ± 0%   +0.01%        (p=0.003 n=17+19)
    
    Change-Id: I820f31bab4582dd4204b146bb077c0d2f24cd8f5
    Reviewed-on: https://go-review.googlesource.com/16434
    Run-TryBot: Ilya Tocar <ilya.tocar@intel.com>
    Reviewed-by: Klaus Post <klauspost@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 454789c509..33d641e612 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -42,11 +42,37 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	JNE	notintel
 	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
 notintel:
+	// Do nothing.
 
 	MOVQ	$1, AX
 	CPUID
 	MOVL	CX, runtime·cpuid_ecx(SB)
 	MOVL	DX, runtime·cpuid_edx(SB)
+	// Detect AVX and AVX2 as per 14.7.1  Detection of AVX2 chapter of [1]
+	// [1] 64-ia-32-architectures-software-developer-manual-325462.pdf
+	// http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-manual-325462.pdf
+	ANDL    $0x18000000, CX // check for OSXSAVE and AVX bits
+	CMPL    CX, $0x18000000
+	JNE     noavx
+	MOVL    $0, CX
+	// For XGETBV, OSXSAVE bit is required and sufficient
+	BYTE $0x0F; BYTE $0x01; BYTE $0xD0
+	ANDL    $6, AX
+	CMPL    AX, $6 // Check for OS support of YMM registers
+	JNE     noavx
+	MOVB    $1, runtime·support_avx(SB)
+	MOVL    $7, AX
+	MOVL    $0, CX
+	CPUID
+	ANDL    $0x20, BX // check for AVX2 bit
+	CMPL    BX, $0x20
+	JNE     noavx2
+	MOVB    $1, runtime·support_avx2(SB)
+	JMP     nocpuinfo
+noavx:
+	MOVB    $0, runtime·support_avx(SB)
+noavx2:
+	MOVB    $0, runtime·support_avx2(SB)
 nocpuinfo:	
 	
 	// if there is an _cgo_init, call it.
@@ -1508,7 +1534,10 @@ TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
 	JB	small
 
 	CMPQ	R8, $63
-	JA	big_loop
+	JBE	loop
+	CMPB    runtime·support_avx2(SB), $1
+	JEQ     big_loop_avx2
+	JMP	big_loop
 loop:
 	CMPQ	R8, $16
 	JBE	_0through16
@@ -1657,6 +1686,45 @@ big_loop:
 	JBE	loop
 	JMP	big_loop
 
+	// Compare 64-bytes per loop iteration.
+	// Loop is unrolled and uses AVX2.
+big_loop_avx2:
+	MOVHDU	(SI), X2
+	MOVHDU	(DI), X3
+	MOVHDU	32(SI), X4
+	MOVHDU	32(DI), X5
+	VPCMPEQB X2, X3, X0
+	VPMOVMSKB X0, AX
+	XORL	$0xffffffff, AX
+	JNE	diff32_avx2
+	VPCMPEQB X4, X5, X6
+	VPMOVMSKB X6, AX
+	XORL	$0xffffffff, AX
+	JNE	diff64_avx2
+
+	ADDQ	$64, SI
+	ADDQ	$64, DI
+	SUBQ	$64, R8
+	CMPQ	R8, $64
+	JB	big_loop_avx2_exit
+	JMP	big_loop_avx2
+
+	// Avoid AVX->SSE transition penalty and search first 32 bytes of 64 byte chunk.
+diff32_avx2:
+	VZEROUPPER
+	JMP diff16
+
+	// Same as diff32_avx2, but for last 32 bytes.
+diff64_avx2:
+	VZEROUPPER
+	JMP diff48
+
+	// For <64 bytes remainder jump to normal loop.
+big_loop_avx2_exit:
+	VZEROUPPER
+	JMP loop
+
+
 TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX

commit 8ee0fd862357aade3f58cdb41467408105c9e865
Author: Matthew Dempsky <mdempsky@google.com>
Date:   Tue Jun 9 15:24:38 2015 -0700

    runtime: replace is{plan9,solaris,windows} with GOOS tests
    
    Change-Id: I27589395f547c5837dc7536a0ab5bc7cc23a4ff6
    Reviewed-on: https://go-review.googlesource.com/10872
    Run-TryBot: Matthew Dempsky <mdempsky@google.com>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 39602ec7dc..454789c509 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -65,15 +65,18 @@ nocpuinfo:
 	MOVQ	AX, g_stackguard0(CX)
 	MOVQ	AX, g_stackguard1(CX)
 
-	CMPL	runtime·iswindows(SB), $0
-	JEQ ok
+#ifndef GOOS_windows
+	JMP ok
+#endif
 needtls:
+#ifdef GOOS_plan9
 	// skip TLS setup on Plan 9
-	CMPL	runtime·isplan9(SB), $1
-	JEQ ok
+	JMP ok
+#endif
+#ifdef GOOS_solaris
 	// skip TLS setup on Solaris
-	CMPL	runtime·issolaris(SB), $1
-	JEQ ok
+	JMP ok
+#endif
 
 	LEAQ	runtime·tls0(SB), DI
 	CALL	runtime·settls(SB)

commit 91059de095703ebc4ce6b8bad7a0a40dedeef7dc
Author: Keith Randall <khr@golang.org>
Date:   Mon Aug 31 16:26:12 2015 -0700

    runtime: make aeshash more DOS-proof
    
    Improve the aeshash implementation to make it harder to engineer collisions.
    
    1) Scramble the seed before xoring with the input string.  This
       makes it harder to cancel known portions of the seed (like the size)
       because it mixes the per-table seed into those other parts.
    
    2) Use table-dependent seeds for all stripes when hashing >16 byte strings.
    
    For small strings this change uses 4 aesenc ops instead of 3, so it
    is somewhat slower.  The first two can run in parallel, though, so
    it isn't 33% slower.
    
    benchmark                            old ns/op     new ns/op     delta
    BenchmarkHash64-12                   10.2          11.2          +9.80%
    BenchmarkHash16-12                   5.71          6.13          +7.36%
    BenchmarkHash5-12                    6.64          7.01          +5.57%
    BenchmarkHashBytesSpeed-12           30.3          31.9          +5.28%
    BenchmarkHash65536-12                2785          2882          +3.48%
    BenchmarkHash1024-12                 53.6          55.4          +3.36%
    BenchmarkHashStringArraySpeed-12     54.9          56.5          +2.91%
    BenchmarkHashStringSpeed-12          18.7          19.2          +2.67%
    BenchmarkHashInt32Speed-12           14.8          15.1          +2.03%
    BenchmarkHashInt64Speed-12           14.5          14.5          +0.00%
    
    Change-Id: I59ea124b5cb92b1c7e8584008257347f9049996c
    Reviewed-on: https://go-review.googlesource.com/14124
    Reviewed-by: jcd . <jcd@golang.org>
    Run-TryBot: Keith Randall <khr@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4020bdfbfc..39602ec7dc 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -951,10 +951,14 @@ TEXT runtime·aeshashstr(SB),NOSPLIT,$0-24
 // CX: length
 // DX: address to put return value
 TEXT runtime·aeshashbody(SB),NOSPLIT,$0-0
-	MOVQ	h+8(FP), X6	// seed to low 64 bits of xmm6
-	PINSRQ	$1, CX, X6	// size to high 64 bits of xmm6
-	PSHUFHW	$0, X6, X6	// replace size with its low 2 bytes repeated 4 times
-	MOVO	runtime·aeskeysched(SB), X7
+	// Fill an SSE register with our seeds.
+	MOVQ	h+8(FP), X0			// 64 bits of per-table hash seed
+	PINSRW	$4, CX, X0			// 16 bits of length
+	PSHUFHW $0, X0, X0			// repeat length 4 times total
+	MOVO	X0, X1				// save unscrambled seed
+	PXOR	runtime·aeskeysched(SB), X0	// xor in per-process seed
+	AESENC	X0, X0				// scramble seed
+
 	CMPQ	CX, $16
 	JB	aes0to15
 	JE	aes16
@@ -976,219 +980,275 @@ aes0to15:
 
 	// 16 bytes loaded at this address won't cross
 	// a page boundary, so we can load it directly.
-	MOVOU	-16(AX), X0
+	MOVOU	-16(AX), X1
 	ADDQ	CX, CX
 	MOVQ	$masks<>(SB), AX
-	PAND	(AX)(CX*8), X0
-
-	// scramble 3 times
-	AESENC	X6, X0
-	AESENC	X7, X0
-	AESENC	X7, X0
-	MOVQ	X0, (DX)
+	PAND	(AX)(CX*8), X1
+final1:
+	AESENC	X0, X1	// scramble input, xor in seed
+	AESENC	X1, X1  // scramble combo 2 times
+	AESENC	X1, X1
+	MOVQ	X1, (DX)
 	RET
 
 endofpage:
 	// address ends in 1111xxxx.  Might be up against
 	// a page boundary, so load ending at last byte.
 	// Then shift bytes down using pshufb.
-	MOVOU	-32(AX)(CX*1), X0
+	MOVOU	-32(AX)(CX*1), X1
 	ADDQ	CX, CX
 	MOVQ	$shifts<>(SB), AX
-	PSHUFB	(AX)(CX*8), X0
-	AESENC	X6, X0
-	AESENC	X7, X0
-	AESENC	X7, X0
-	MOVQ	X0, (DX)
-	RET
+	PSHUFB	(AX)(CX*8), X1
+	JMP	final1
 
 aes0:
 	// Return scrambled input seed
-	AESENC	X7, X6
-	AESENC	X7, X6
-	MOVQ	X6, (DX)
+	AESENC	X0, X0
+	MOVQ	X0, (DX)
 	RET
 
 aes16:
-	MOVOU	(AX), X0
-	AESENC	X6, X0
-	AESENC	X7, X0
-	AESENC	X7, X0
-	MOVQ	X0, (DX)
-	RET
+	MOVOU	(AX), X1
+	JMP	final1
 
 aes17to32:
+	// make second starting seed
+	PXOR	runtime·aeskeysched+16(SB), X1
+	AESENC	X1, X1
+	
 	// load data to be hashed
-	MOVOU	(AX), X0
-	MOVOU	-16(AX)(CX*1), X1
+	MOVOU	(AX), X2
+	MOVOU	-16(AX)(CX*1), X3
 
 	// scramble 3 times
-	AESENC	X6, X0
-	AESENC	runtime·aeskeysched+16(SB), X1
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X0
-	AESENC	X7, X1
+	AESENC	X0, X2
+	AESENC	X1, X3
+	AESENC	X2, X2
+	AESENC	X3, X3
+	AESENC	X2, X2
+	AESENC	X3, X3
 
 	// combine results
-	PXOR	X1, X0
-	MOVQ	X0, (DX)
+	PXOR	X3, X2
+	MOVQ	X2, (DX)
 	RET
 
 aes33to64:
-	MOVOU	(AX), X0
-	MOVOU	16(AX), X1
-	MOVOU	-32(AX)(CX*1), X2
-	MOVOU	-16(AX)(CX*1), X3
+	// make 3 more starting seeds
+	MOVO	X1, X2
+	MOVO	X1, X3
+	PXOR	runtime·aeskeysched+16(SB), X1
+	PXOR	runtime·aeskeysched+32(SB), X2
+	PXOR	runtime·aeskeysched+48(SB), X3
+	AESENC	X1, X1
+	AESENC	X2, X2
+	AESENC	X3, X3
 	
-	AESENC	X6, X0
-	AESENC	runtime·aeskeysched+16(SB), X1
-	AESENC	runtime·aeskeysched+32(SB), X2
-	AESENC	runtime·aeskeysched+48(SB), X3
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-
-	PXOR	X2, X0
-	PXOR	X3, X1
-	PXOR	X1, X0
-	MOVQ	X0, (DX)
+	MOVOU	(AX), X4
+	MOVOU	16(AX), X5
+	MOVOU	-32(AX)(CX*1), X6
+	MOVOU	-16(AX)(CX*1), X7
+	
+	AESENC	X0, X4
+	AESENC	X1, X5
+	AESENC	X2, X6
+	AESENC	X3, X7
+	
+	AESENC	X4, X4
+	AESENC	X5, X5
+	AESENC	X6, X6
+	AESENC	X7, X7
+	
+	AESENC	X4, X4
+	AESENC	X5, X5
+	AESENC	X6, X6
+	AESENC	X7, X7
+
+	PXOR	X6, X4
+	PXOR	X7, X5
+	PXOR	X5, X4
+	MOVQ	X4, (DX)
 	RET
 
 aes65to128:
-	MOVOU	(AX), X0
-	MOVOU	16(AX), X1
-	MOVOU	32(AX), X2
-	MOVOU	48(AX), X3
-	MOVOU	-64(AX)(CX*1), X4
-	MOVOU	-48(AX)(CX*1), X5
-	MOVOU	-32(AX)(CX*1), X8
-	MOVOU	-16(AX)(CX*1), X9
+	// make 7 more starting seeds
+	MOVO	X1, X2
+	MOVO	X1, X3
+	MOVO	X1, X4
+	MOVO	X1, X5
+	MOVO	X1, X6
+	MOVO	X1, X7
+	PXOR	runtime·aeskeysched+16(SB), X1
+	PXOR	runtime·aeskeysched+32(SB), X2
+	PXOR	runtime·aeskeysched+48(SB), X3
+	PXOR	runtime·aeskeysched+64(SB), X4
+	PXOR	runtime·aeskeysched+80(SB), X5
+	PXOR	runtime·aeskeysched+96(SB), X6
+	PXOR	runtime·aeskeysched+112(SB), X7
+	AESENC	X1, X1
+	AESENC	X2, X2
+	AESENC	X3, X3
+	AESENC	X4, X4
+	AESENC	X5, X5
+	AESENC	X6, X6
+	AESENC	X7, X7
+
+	// load data
+	MOVOU	(AX), X8
+	MOVOU	16(AX), X9
+	MOVOU	32(AX), X10
+	MOVOU	48(AX), X11
+	MOVOU	-64(AX)(CX*1), X12
+	MOVOU	-48(AX)(CX*1), X13
+	MOVOU	-32(AX)(CX*1), X14
+	MOVOU	-16(AX)(CX*1), X15
+
+	// scramble data, xor in seed
+	AESENC	X0, X8
+	AESENC	X1, X9
+	AESENC	X2, X10
+	AESENC	X3, X11
+	AESENC	X4, X12
+	AESENC	X5, X13
+	AESENC	X6, X14
+	AESENC	X7, X15
+
+	// scramble twice
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
 	
-	AESENC	X6, X0
-	AESENC	runtime·aeskeysched+16(SB), X1
-	AESENC	runtime·aeskeysched+32(SB), X2
-	AESENC	runtime·aeskeysched+48(SB), X3
-	AESENC	runtime·aeskeysched+64(SB), X4
-	AESENC	runtime·aeskeysched+80(SB), X5
-	AESENC	runtime·aeskeysched+96(SB), X8
-	AESENC	runtime·aeskeysched+112(SB), X9
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X4
-	AESENC	X7, X5
-	AESENC	X7, X8
-	AESENC	X7, X9
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X4
-	AESENC	X7, X5
-	AESENC	X7, X8
-	AESENC	X7, X9
-
-	PXOR	X4, X0
-	PXOR	X5, X1
-	PXOR	X8, X2
-	PXOR	X9, X3
-	PXOR	X2, X0
-	PXOR	X3, X1
-	PXOR	X1, X0
-	MOVQ	X0, (DX)
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
+
+	// combine results
+	PXOR	X12, X8
+	PXOR	X13, X9
+	PXOR	X14, X10
+	PXOR	X15, X11
+	PXOR	X10, X8
+	PXOR	X11, X9
+	PXOR	X9, X8
+	MOVQ	X8, (DX)
 	RET
 
 aes129plus:
+	// make 7 more starting seeds
+	MOVO	X1, X2
+	MOVO	X1, X3
+	MOVO	X1, X4
+	MOVO	X1, X5
+	MOVO	X1, X6
+	MOVO	X1, X7
+	PXOR	runtime·aeskeysched+16(SB), X1
+	PXOR	runtime·aeskeysched+32(SB), X2
+	PXOR	runtime·aeskeysched+48(SB), X3
+	PXOR	runtime·aeskeysched+64(SB), X4
+	PXOR	runtime·aeskeysched+80(SB), X5
+	PXOR	runtime·aeskeysched+96(SB), X6
+	PXOR	runtime·aeskeysched+112(SB), X7
+	AESENC	X1, X1
+	AESENC	X2, X2
+	AESENC	X3, X3
+	AESENC	X4, X4
+	AESENC	X5, X5
+	AESENC	X6, X6
+	AESENC	X7, X7
+	
 	// start with last (possibly overlapping) block
-	MOVOU	-128(AX)(CX*1), X0
-	MOVOU	-112(AX)(CX*1), X1
-	MOVOU	-96(AX)(CX*1), X2
-	MOVOU	-80(AX)(CX*1), X3
-	MOVOU	-64(AX)(CX*1), X4
-	MOVOU	-48(AX)(CX*1), X5
-	MOVOU	-32(AX)(CX*1), X8
-	MOVOU	-16(AX)(CX*1), X9
-
-	// scramble state once
-	AESENC	X6, X0
-	AESENC	runtime·aeskeysched+16(SB), X1
-	AESENC	runtime·aeskeysched+32(SB), X2
-	AESENC	runtime·aeskeysched+48(SB), X3
-	AESENC	runtime·aeskeysched+64(SB), X4
-	AESENC	runtime·aeskeysched+80(SB), X5
-	AESENC	runtime·aeskeysched+96(SB), X8
-	AESENC	runtime·aeskeysched+112(SB), X9
-
+	MOVOU	-128(AX)(CX*1), X8
+	MOVOU	-112(AX)(CX*1), X9
+	MOVOU	-96(AX)(CX*1), X10
+	MOVOU	-80(AX)(CX*1), X11
+	MOVOU	-64(AX)(CX*1), X12
+	MOVOU	-48(AX)(CX*1), X13
+	MOVOU	-32(AX)(CX*1), X14
+	MOVOU	-16(AX)(CX*1), X15
+
+	// scramble input once, xor in seed
+	AESENC	X0, X8
+	AESENC	X1, X9
+	AESENC	X2, X10
+	AESENC	X3, X11
+	AESENC	X4, X12
+	AESENC	X5, X13
+	AESENC	X6, X14
+	AESENC	X7, X15
+	
 	// compute number of remaining 128-byte blocks
 	DECQ	CX
 	SHRQ	$7, CX
 	
 aesloop:
 	// scramble state, xor in a block
-	MOVOU	(AX), X10
-	MOVOU	16(AX), X11
-	MOVOU	32(AX), X12
-	MOVOU	48(AX), X13
-	AESENC	X10, X0
-	AESENC	X11, X1
-	AESENC	X12, X2
-	AESENC	X13, X3
-	MOVOU	64(AX), X10
-	MOVOU	80(AX), X11
-	MOVOU	96(AX), X12
-	MOVOU	112(AX), X13
-	AESENC	X10, X4
-	AESENC	X11, X5
-	AESENC	X12, X8
-	AESENC	X13, X9
+	MOVOU	(AX), X0
+	MOVOU	16(AX), X1
+	MOVOU	32(AX), X2
+	MOVOU	48(AX), X3
+	AESENC	X0, X8
+	AESENC	X1, X9
+	AESENC	X2, X10
+	AESENC	X3, X11
+	MOVOU	64(AX), X4
+	MOVOU	80(AX), X5
+	MOVOU	96(AX), X6
+	MOVOU	112(AX), X7
+	AESENC	X4, X12
+	AESENC	X5, X13
+	AESENC	X6, X14
+	AESENC	X7, X15
 
 	// scramble state
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X4
-	AESENC	X7, X5
-	AESENC	X7, X8
-	AESENC	X7, X9
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
 
 	ADDQ	$128, AX
 	DECQ	CX
 	JNE	aesloop
 
 	// 2 more scrambles to finish
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X4
-	AESENC	X7, X5
-	AESENC	X7, X8
-	AESENC	X7, X9
-	AESENC	X7, X0
-	AESENC	X7, X1
-	AESENC	X7, X2
-	AESENC	X7, X3
-	AESENC	X7, X4
-	AESENC	X7, X5
-	AESENC	X7, X8
-	AESENC	X7, X9
-
-	PXOR	X4, X0
-	PXOR	X5, X1
-	PXOR	X8, X2
-	PXOR	X9, X3
-	PXOR	X2, X0
-	PXOR	X3, X1
-	PXOR	X1, X0
-	MOVQ	X0, (DX)
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
+	AESENC	X8, X8
+	AESENC	X9, X9
+	AESENC	X10, X10
+	AESENC	X11, X11
+	AESENC	X12, X12
+	AESENC	X13, X13
+	AESENC	X14, X14
+	AESENC	X15, X15
+
+	PXOR	X12, X8
+	PXOR	X13, X9
+	PXOR	X14, X10
+	PXOR	X15, X11
+	PXOR	X10, X8
+	PXOR	X11, X9
+	PXOR	X9, X8
+	MOVQ	X8, (DX)
 	RET
 	
 TEXT runtime·aeshash32(SB),NOSPLIT,$0-24

commit 936e9d809b4456cbd2ab2c2b4eed3f7ec30e3ae3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 2 18:03:35 2015 -0400

    fix FS segment bug; cleanup segment code
    
    also fix an amusing, ancient bug from when i apparently didn't yet know that
    the calling convention for both runtime C and Go code is to pass return values
    on the stack. my assembly code expected the return value in rax. magically,
    this code never broke because the compiler is apparently very likely to use rax
    to hold the return value in order to copy it to the caller's stack frame.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 56cefb51ca..0ca06eeeca 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -120,9 +120,6 @@ TEXT runtime·asminit(SB),NOSPLIT,$0-0
 	RET
 
 #define		CODESEG		1
-#define		DATASEG		2
-#define		FSSEG		3
-#define		GSSEG		4
 #define		UCSEG		5
 #define		UDSEG		6
 
@@ -230,10 +227,13 @@ h_needtls:
 	//JEQ ok
 
 	// setup tls
+	PUSHQ	AX
 	LEAQ	runtime·tls0(SB), DI
 	PUSHQ	DI
 	CALL	segsetup(SB)
 
+	POPQ	AX
+	POPQ	AX
 	MOVQ	8(AX), DI
 	PUSHQ	DI
 	MOVQ	(AX), DI
@@ -247,21 +247,6 @@ h_needtls:
 	POPQ	AX
 	POPQ	AX
 
-	MOVQ	$(FSSEG << 3), AX
-	PUSHQ	AX
-	POPQ	FS
-
-	MOVL	$(DATASEG << 3), AX
-	//MOVL	AX, ES
-	BYTE	$0x8e
-	BYTE	$0xd8
-	//MOVL	AX, DS
-	BYTE	$0x8e
-	BYTE	$0xc0
-	//MOVL	AX, SS
-	BYTE	$0x8e
-	BYTE	$0xd0
-
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
@@ -1061,6 +1046,12 @@ TEXT gs_null(SB), NOSPLIT, $8-0
 	POPQ	GS
 	RET
 
+TEXT fs_null(SB), NOSPLIT, $8-0
+	XORQ	AX, AX
+	PUSHQ	AX
+	POPQ	FS
+	RET
+
 TEXT runtime·Gscpu(SB), NOSPLIT, $0-8
 	MOVQ	0(GS), AX
 	MOVQ	AX, ret+0(FP)

commit 2b3cdc7f2138ec8d05d333e45f49c68267ce213f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 2 13:22:40 2015 -0400

    fix threads and SMP; save/restore less user state
    
    only getrusage(2) is broken now

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index acc549892f..56cefb51ca 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -926,6 +926,8 @@ TEXT mktrap(SB), NOSPLIT, $0-8
 
 #define TFREGS		17
 #define TF_SYSRSP	(8*0)
+#define TF_R13		(8*4)
+#define TF_R12		(8*5)
 #define TF_R8		(8*9)
 #define TF_RBP		(8*10)
 #define TF_RSI		(8*11)
@@ -962,15 +964,7 @@ syscallreturn:
 	POPQ	AX
 	POPQ	AX
 
-	// user dx/cx cannot be loaded directly those registers are used by
-	// sysexit. thus we use r10 and r11 for dx and cx respectively. this
-	// only matters for exec when we need to setup a call to _entry() with
-	// more than two arguments. we also clobber all other registers; maybe
-	// this is bad.
 	MOVQ	TF_RAX(R9), AX
-	MOVQ	TF_RDI(R9), DI
-	MOVQ	TF_RSI(R9), SI
-	MOVQ	TF_RDX(R9), R10
 	MOVQ	TF_RSP(R9), CX
 	MOVQ	TF_RIP(R9), DX
 	MOVQ	TF_RBP(R9), BP
@@ -996,12 +990,14 @@ TEXT _sysentry(SB), NOSPLIT, $0-0
 	MOVQ	0x20(SP), R9
 	MOVQ	R10, TF_RSP(R9)
 	MOVQ	R11, TF_RIP(R9)
+	// syscall args
 	MOVQ	AX,  TF_RAX(R9)
 	MOVQ	DI,  TF_RDI(R9)
 	MOVQ	SI,  TF_RSI(R9)
 	MOVQ	DX,  TF_RDX(R9)
 	MOVQ	CX,  TF_RCX(R9)
 	MOVQ	R8,  TF_R8(R9)
+	// kernel preserves rbp and rbx
 	MOVQ	BP,  TF_RBP(R9)
 	MOVQ	BX,  TF_RBX(R9)
 	// return val 1
@@ -1015,6 +1011,7 @@ TEXT _sysentry(SB), NOSPLIT, $0-0
 // exception is generated during user program execution.
 TEXT _userint(SB), NOSPLIT, $0-0
 	CLI
+	// user state is already saved by trap handler.
 	// AX holds the interrupt number, BX holds aux (cr2 for page fault)
 	MOVQ	AX, 0x30(SP)
 	MOVQ	BX, 0x38(SP)

commit 7b14bae3b1ede63d14cd972c27b40a2816838078
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Oct 1 11:47:43 2015 -0400

    delete code that is now unused, fix process termination
    
    the C scheduler no longer cares about pids or notifying. still need to fix
    threads, pmap freeing, and kernel profiling.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d6ed9aa1bf..acc549892f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -614,12 +614,6 @@ hltagain:
 	HLT
 	JMP	hltagain
 
-TEXT runtime·handle_int(SB), NOSPLIT, $0-0
-	PUSHQ	DX
-	CALL	_handle_int(SB)
-	POPQ	DX
-	RET
-
 #define TRAP_YIELD      $49
 #define TRAP_SYSCALL    $64
 TEXT hack_yield(SB), NOSPLIT, $0-0
@@ -945,8 +939,8 @@ TEXT mktrap(SB), NOSPLIT, $0-8
 
 // if you change the number of arguments, you must adjust the stack offsets in
 // _sysentry and _userint.
-// func Userrun_(tf *[24]int, fastret bool) (int, int)
-TEXT ·Userrun_(SB), NOSPLIT, $24-32
+// func _Userrun(tf *[24]int, fastret bool) (int, int)
+TEXT ·_Userrun(SB), NOSPLIT, $24-32
 	MOVQ	tf+0(FP), R9
 
 	// fastret or iret?

commit 3ad9816a4631fe8ffdb70106969aeaff89718350
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Sep 25 11:37:17 2015 -0400

    Userrun() for fast syscalls and better interface to user programs
    
    system call overhead is reduced 10x using this approach, putting Biscuit on par
    with Linux. i think the main reasons for this speedup are that, during
    syscalls, there is little state saved when switching to/from kernel mode and we
    now use a single go routine to handle all of a thread's CPU exceptions/syscalls
    instead of creating a new one for each event.
    
    Userrun() also makes the code for handling all user events (page faults, timer
    interrupts) clearer and exposes timer interrupts during user program execution
    to the kernel, allowing for finer grained control and time accounting.
    
    i still need to fix thread termination: since user programs are no longer
    managed via global state, it should be easier but i haven't done it yet.
    
    also, start converting C code to Go code. its painful because we have to have
    two defintions of every struct that is used by both kinds of code -- it is up
    to the programmer to make sure they are coherent. maybe this will help motivate
    me to convert more C to Go.
    
    cleanup trap() and trap circular buffer code since only IRQs/IPIs go through
    the circular buffer now. sadly, we can't handle IRQs using the new method
    because IRQs/IPIs are truly asynchronous, thus we need a way to inject them
    into the runtime regardless of whether the CPU is currently in user or kernel
    mode.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f70ce579a2..d6ed9aa1bf 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -429,15 +429,15 @@ TEXT lcr4(SB), NOSPLIT, $0-8
 	MOVQ	AX, CR4
 	RET
 
-TEXT rdmsr(SB), NOSPLIT, $0-16
+TEXT ·Rdmsr(SB), NOSPLIT, $0-16
 	MOVQ	reg+0(FP), CX
 	RDMSR
 	MOVL	DX, ret2+12(FP)
 	MOVL	AX, ret1+8(FP)
 	RET
 
-// void wrmsr(uint64 reg, uint64 val)
-TEXT wrmsr(SB), NOSPLIT, $0-16
+// void ·Wrmsr(uint64 reg, uint64 val)
+TEXT ·Wrmsr(SB), NOSPLIT, $0-16
 	MOVQ	reg+0(FP), CX
 	MOVL	vlo+8(FP), AX
 	MOVL	vhi+12(FP), DX
@@ -542,14 +542,14 @@ TEXT sti(SB), NOSPLIT, $0-0
 	STI
 	RET
 
-TEXT pushcli(SB), NOSPLIT, $0-8
+TEXT runtime·Pushcli(SB), NOSPLIT, $0-8
 	PUSHFQ
 	POPQ	AX
 	MOVQ	AX, ret+0(FP)
 	CLI
 	RET
 
-TEXT popcli(SB), NOSPLIT, $0-8
+TEXT runtime·Popcli(SB), NOSPLIT, $0-8
 	MOVQ	fl+0(FP), AX
 	PUSHQ	AX
 	POPFQ
@@ -621,6 +621,7 @@ TEXT runtime·handle_int(SB), NOSPLIT, $0-0
 	RET
 
 #define TRAP_YIELD      $49
+#define TRAP_SYSCALL    $64
 TEXT hack_yield(SB), NOSPLIT, $0-0
 	INT	TRAP_YIELD
 	RET
@@ -711,7 +712,9 @@ IH_IRQ(13,Xirq13 )
 IH_IRQ(14,Xirq14 )
 IH_IRQ(15,Xirq15 )
 
-#define IA32_FS_BASE   $0xc0000100UL
+#define IA32_FS_BASE		$0xc0000100UL
+#define IA32_SYSENTER_ESP	$0x175UL
+#define IA32_SYSENTER_EIP	$0x176UL
 
 TEXT wrfsb(SB), NOSPLIT, $0-8
 	get_tls(BX)
@@ -753,6 +756,13 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	ORQ	DX, AX
 	PUSHQ	AX
 
+	// save sysenter rsp
+	MOVQ	IA32_SYSENTER_ESP, CX
+	RDMSR
+	SHLQ	$32, DX
+	ORQ	DX, AX
+	PUSHQ	AX
+
 	MOVQ	SP, AX
 	PUSHQ	AX
 
@@ -773,6 +783,14 @@ TEXT _trapret(SB), NOSPLIT, $0-8
 				// threads[]
 	MOVQ	AX, SP
 
+	// restore sysenter esp
+	MOVQ	IA32_SYSENTER_ESP, CX
+	POPQ	AX
+	MOVQ	AX, DX
+	ANDQ	$((1 << 32) - 1), AX
+	SHRQ	$32, DX
+	WRMSR
+
 	// restore fsbase
 	MOVQ	IA32_FS_BASE, CX
 	POPQ	AX
@@ -912,7 +930,104 @@ TEXT mktrap(SB), NOSPLIT, $0-8
 
 	JMP	alltraps(SB)
 
-TEXT _sysentry(SB), NOSPLIT, $0
+#define TFREGS		17
+#define TF_SYSRSP	(8*0)
+#define TF_R8		(8*9)
+#define TF_RBP		(8*10)
+#define TF_RSI		(8*11)
+#define TF_RDI		(8*12)
+#define TF_RDX		(8*13)
+#define TF_RCX		(8*14)
+#define TF_RBX		(8*15)
+#define TF_RAX		(8*16)
+#define TF_RIP		(8*(TFREGS + 2))
+#define TF_RSP		(8*(TFREGS + 5))
+
+// if you change the number of arguments, you must adjust the stack offsets in
+// _sysentry and _userint.
+// func Userrun_(tf *[24]int, fastret bool) (int, int)
+TEXT ·Userrun_(SB), NOSPLIT, $24-32
+	MOVQ	tf+0(FP), R9
+
+	// fastret or iret?
+	MOVB	fastret+8(FP), AX
+	CMPB	AX, $0
+	JNE	syscallreturn
+	// do full state restore, make sure the SP we return with is correct
+	MOVQ	SP, TF_SYSRSP(R9)
+	PUSHQ	R9
+	CALL	_trapret(SB)
+	INT	$3
+
+syscallreturn:
+	// set SP MSRs manually
+	MOVQ	SP, AX
+	PUSHQ	AX
+	PUSHQ	IA32_SYSENTER_ESP
+	CALL	·Wrmsr(SB)
+	POPQ	AX
+	POPQ	AX
+
+	// user dx/cx cannot be loaded directly those registers are used by
+	// sysexit. thus we use r10 and r11 for dx and cx respectively. this
+	// only matters for exec when we need to setup a call to _entry() with
+	// more than two arguments. we also clobber all other registers; maybe
+	// this is bad.
+	MOVQ	TF_RAX(R9), AX
+	MOVQ	TF_RDI(R9), DI
+	MOVQ	TF_RSI(R9), SI
+	MOVQ	TF_RDX(R9), R10
+	MOVQ	TF_RSP(R9), CX
+	MOVQ	TF_RIP(R9), DX
+	MOVQ	TF_RBP(R9), BP
+	MOVQ	TF_RBX(R9), BX
+	// rcx contains rsp
+	// rdx contains rip
+	STI
+	// rex64 sysexit
+	BYTE	$0x48
+	BYTE	$0x0f
+	BYTE	$0x35
+	// not reached; just to trick dead code analysis
+	CALL	_sysentry(SB)
+	CALL	_userint(SB)
+
+// this should be a label since it is the bottom half of the Userrun_ function,
+// but i can't figure out how to get the plan9 assembler to let me use lea on a
+// label. thus the function epilogue and offset to get the tf arg from Userrun_
+// are hand-coded.
+//_sysentry:
+TEXT _sysentry(SB), NOSPLIT, $0-0
+	// save user state in fake trapframe
+	MOVQ	0x20(SP), R9
+	MOVQ	R10, TF_RSP(R9)
+	MOVQ	R11, TF_RIP(R9)
+	MOVQ	AX,  TF_RAX(R9)
+	MOVQ	DI,  TF_RDI(R9)
+	MOVQ	SI,  TF_RSI(R9)
+	MOVQ	DX,  TF_RDX(R9)
+	MOVQ	CX,  TF_RCX(R9)
+	MOVQ	R8,  TF_R8(R9)
+	MOVQ	BP,  TF_RBP(R9)
+	MOVQ	BX,  TF_RBX(R9)
+	// return val 1
+	MOVQ	TRAP_SYSCALL, 0x30(SP)
+	// return val 2
+	MOVQ	$0, 0x38(SP)
+	ADDQ	$0x18, SP
+	RET
+
+// this is the bottom half of _userrun() that is executed if a timer int or CPU
+// exception is generated during user program execution.
+TEXT _userint(SB), NOSPLIT, $0-0
+	CLI
+	// AX holds the interrupt number, BX holds aux (cr2 for page fault)
+	MOVQ	AX, 0x30(SP)
+	MOVQ	BX, 0x38(SP)
+	ADDQ	$0x18, SP
+	RET
+
+TEXT old_sysentry(SB), NOSPLIT, $0
 	// r10 contains return rsp, r11 contains return rip
 	PUSHQ	AX
 
@@ -939,7 +1054,6 @@ TEXT _sysentry(SB), NOSPLIT, $0
 	PUSHQ	$0
 
 	// interrupt number
-#define TRAP_SYSCALL    $64
 	PUSHQ	TRAP_SYSCALL
 
 	// and finally, restore rax
@@ -950,47 +1064,13 @@ TEXT _sysentry(SB), NOSPLIT, $0
 	CALL	sysentry(SB)
 	INT	$3
 
-// this is unused
-TEXT sysexitportal(SB), NOSPLIT, $0-56
-	MOVQ	pmap+24(FP), AX
-	MOVQ	AX, CR3
-
-	MOVQ	fsb+16(FP), AX
-
-	// restore fsbase
-	MOVQ	IA32_FS_BASE, CX
-	MOVQ	AX, DX
-	ANDQ	$((1 << 32) - 1), AX
-	SHRQ	$32, DX
-	WRMSR
-
-	MOVQ	rip+0(FP), DX
-	MOVQ	rsp+8(FP), CX
-	MOVQ	rax+32(FP), AX
-	MOVQ	rbp+40(FP), BP
-	MOVQ	rbx+48(FP), BX
-
-	//MOVQ	0x90(AX), DX
-	//MOVQ	0xa8(AX), CX
-	// stack cannot be used after STI since an int using the IST can come
-	// in and clobber our stack
-	STI
-
-	// rcx contains rsp
-	// rdx contains rip
-	// rex64 sysexit
-	BYTE	$0x48
-	BYTE	$0x0f
-	BYTE	$0x35
-	INT	$3
-
 TEXT gs_null(SB), NOSPLIT, $8-0
 	XORQ	AX, AX
 	PUSHQ	AX
 	POPQ	GS
 	RET
 
-TEXT gscpu(SB), NOSPLIT, $0-8
+TEXT runtime·Gscpu(SB), NOSPLIT, $0-8
 	MOVQ	0(GS), AX
 	MOVQ	AX, ret+0(FP)
 	RET

commit 731bdc51157fd7f685fb73c298e97922318ac453
Author: Keith Randall <khr@golang.org>
Date:   Tue Sep 1 12:53:15 2015 -0700

    runtime: fix aeshash of empty string
    
    Aeshash currently computes the hash of the empty string as
    hash("", seed) = seed.  This is bad because the hash of a compound
    object with empty strings in it doesn't include information about
    where those empty strings were.  For instance [2]string{"", "foo"}
    and [2]string{"foo", ""} might get the same hash.
    
    Fix this by returning a scrambled seed instead of the seed itself.
    With this fix, we can remove the scrambling done by the generated
    array hash routines.
    
    The test also rejects hash("", seed) = 0, if we ever thought
    it would be a good idea to try that.
    
    The fallback hash is already OK in this regard.
    
    Change-Id: Iaedbaa5be8d6a246dc7e9383d795000e0f562037
    Reviewed-on: https://go-review.googlesource.com/14129
    Reviewed-by: jcd . <jcd@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index dc975bebc2..4020bdfbfc 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1003,9 +1003,10 @@ endofpage:
 	RET
 
 aes0:
-	// return input seed
-	MOVQ	h+8(FP), AX
-	MOVQ	AX, (DX)
+	// Return scrambled input seed
+	AESENC	X7, X6
+	AESENC	X7, X6
+	MOVQ	X6, (DX)
 	RET
 
 aes16:

commit 77e528293bbb51a92d16a5e77a8d7920c96764bd
Author: Austin Clements <austin@google.com>
Date:   Wed Aug 26 15:06:43 2015 -0400

    runtime: check that stack barrier unwind is in sync
    
    Currently the stack barrier stub blindly unwinds the next stack
    barrier from the G's stack barrier array without checking that it's
    the right stack barrier. If through some bug the stack barrier array
    position gets out of sync with where we actually are on the stack,
    this could return to the wrong PC, which would lead to difficult to
    debug crashes. To address this, this commit adds a check to the amd64
    stack barrier stub that it's unwinding the correct stack barrier.
    
    Updates #12238.
    
    Change-Id: If824d95191d07e2512dc5dba0d9978cfd9f54e02
    Reviewed-on: https://go-review.googlesource.com/13948
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d165e08333..dc975bebc2 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -346,7 +346,12 @@ TEXT runtime·stackBarrier(SB),NOSPLIT,$0
 	MOVQ	(g_stkbar+slice_array)(CX), DX
 	MOVQ	g_stkbarPos(CX), BX
 	IMULQ	$stkbar__size, BX	// Too big for SIB.
+	MOVQ	stkbar_savedLRPtr(DX)(BX*1), R8
 	MOVQ	stkbar_savedLRVal(DX)(BX*1), BX
+	// Assert that we're popping the right saved LR.
+	CMPQ	R8, SP
+	JNE	2(PC)
+	MOVL	$0, 0
 	// Record that this stack barrier was hit.
 	ADDQ	$1, g_stkbarPos(CX)
 	// Jump to the original return PC.

commit d497eeb00540cebe5fb875570a06cc0083e8016b
Author: Michael Hudson-Doyle <michael.hudson@canonical.com>
Date:   Thu Aug 27 10:59:43 2015 +1200

    runtime: remove unused xchgp/xchgp1
    
    I noticed that they were unimplemented on arm64 but then that they were
    in fact not used at all.
    
    Change-Id: Iee579feda2a5e374fa571bcc8c89e4ef607d50f6
    Reviewed-on: https://go-review.googlesource.com/13951
    Run-TryBot: Minux Ma <minux@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ff2da3a858..d165e08333 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -563,13 +563,6 @@ TEXT runtime·xchg64(SB), NOSPLIT, $0-24
 	MOVQ	AX, ret+16(FP)
 	RET
 
-TEXT runtime·xchgp1(SB), NOSPLIT, $0-24
-	MOVQ	ptr+0(FP), BX
-	MOVQ	new+8(FP), AX
-	XCHGQ	AX, 0(BX)
-	MOVQ	AX, ret+16(FP)
-	RET
-
 TEXT runtime·xchguintptr(SB), NOSPLIT, $0-24
 	JMP	runtime·xchg64(SB)
 

commit 32add8d7c8433d87aca782ddcd79898922ac96b7
Author: Uttam C Pawar <uttam.c.pawar@intel.com>
Date:   Thu Jul 2 11:43:46 2015 -0700

    bytes: improve Compare function on amd64 for large byte arrays
    
    This patch contains only loop unrolling change for size > 63B
    
    Following are the performance numbers for various sizes on
    On Haswell based system: Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz.
    
    benchcmp go.head.8.25.15.txt go.head.8.25.15.opt.txt
    benchmark                       old ns/op     new ns/op     delta
    BenchmarkBytesCompare1-4        5.37          5.37          +0.00%
    BenchmarkBytesCompare2-4        5.37          5.38          +0.19%
    BenchmarkBytesCompare4-4        5.37          5.37          +0.00%
    BenchmarkBytesCompare8-4        4.42          4.38          -0.90%
    BenchmarkBytesCompare16-4       4.27          4.45          +4.22%
    BenchmarkBytesCompare32-4       5.30          5.36          +1.13%
    BenchmarkBytesCompare64-4       6.93          6.78          -2.16%
    BenchmarkBytesCompare128-4      10.3          9.50          -7.77%
    BenchmarkBytesCompare256-4      17.1          13.8          -19.30%
    BenchmarkBytesCompare512-4      31.3          22.1          -29.39%
    BenchmarkBytesCompare1024-4     62.5          39.0          -37.60%
    BenchmarkBytesCompare2048-4     112           73.2          -34.64%
    
    Change-Id: I4eeb1c22732fd62cbac97ba757b0d29f648d4ef1
    Reviewed-on: https://go-review.googlesource.com/11871
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3b4ca4d012..ff2da3a858 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1445,6 +1445,8 @@ TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
 	CMPQ	R8, $8
 	JB	small
 
+	CMPQ	R8, $63
+	JA	big_loop
 loop:
 	CMPQ	R8, $16
 	JBE	_0through16
@@ -1459,6 +1461,17 @@ loop:
 	SUBQ	$16, R8
 	JMP	loop
 	
+diff64:
+	ADDQ	$48, SI
+	ADDQ	$48, DI
+	JMP	diff16
+diff48:
+	ADDQ	$32, SI
+	ADDQ	$32, DI
+	JMP	diff16
+diff32:
+	ADDQ	$16, SI
+	ADDQ	$16, DI
 	// AX = bit mask of differences
 diff16:
 	BSFQ	AX, BX	// index of first byte that differs
@@ -1545,6 +1558,43 @@ allsame:
 	MOVQ	AX, (R9)
 	RET
 
+	// this works for >= 64 bytes of data.
+big_loop:
+	MOVOU	(SI), X0
+	MOVOU	(DI), X1
+	PCMPEQB X0, X1
+	PMOVMSKB X1, AX
+	XORQ	$0xffff, AX
+	JNE	diff16
+
+	MOVOU	16(SI), X0
+	MOVOU	16(DI), X1
+	PCMPEQB X0, X1
+	PMOVMSKB X1, AX
+	XORQ	$0xffff, AX
+	JNE	diff32
+
+	MOVOU	32(SI), X0
+	MOVOU	32(DI), X1
+	PCMPEQB X0, X1
+	PMOVMSKB X1, AX
+	XORQ	$0xffff, AX
+	JNE	diff48
+
+	MOVOU	48(SI), X0
+	MOVOU	48(DI), X1
+	PCMPEQB X0, X1
+	PMOVMSKB X1, AX
+	XORQ	$0xffff, AX
+	JNE	diff64
+
+	ADDQ	$64, SI
+	ADDQ	$64, DI
+	SUBQ	$64, R8
+	CMPQ	R8, $64
+	JBE	loop
+	JMP	big_loop
+
 TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX

commit 0779a7a243ba99b46cddb1307434ea2d13cd2eb2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Aug 10 17:36:02 2015 -0400

    use sysenter for syscalls
    
    mostly for fun. other things in biscuit are slow enough that this makes little
    difference for performance as of now.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 493001a390..f70ce579a2 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -123,6 +123,8 @@ TEXT runtime·asminit(SB),NOSPLIT,$0-0
 #define		DATASEG		2
 #define		FSSEG		3
 #define		GSSEG		4
+#define		UCSEG		5
+#define		UDSEG		6
 
 TEXT fixcs(SB),NOSPLIT,$0
 	POPQ	AX
@@ -911,19 +913,69 @@ TEXT mktrap(SB), NOSPLIT, $0-8
 	JMP	alltraps(SB)
 
 TEXT _sysentry(SB), NOSPLIT, $0
-	// rcx contains rsp, rdx contains ret addr
-	PUSHQ	CX
-	PUSHQ	DX
+	// r10 contains return rsp, r11 contains return rip
+	PUSHQ	AX
+
+	// build hardware trap frame
+
+	MOVQ	$((UDSEG << 3) | 3), AX
+	PUSHQ	AX
+
+	PUSHQ	R10
+
+#define		TF_FL_IF	$(1 << 9)
+	PUSHFQ
+	POPQ	AX
+	ORQ	TF_FL_IF, AX
+	PUSHQ	AX
+
+	MOVQ	$((UCSEG << 3) | 3), AX
+	PUSHQ	AX
+
+	// ret addr
+	PUSHQ	R11
+
+	// dummy error code
+	PUSHQ	$0
+
+	// interrupt number
+#define TRAP_SYSCALL    $64
+	PUSHQ	TRAP_SYSCALL
+
+	// and finally, restore rax
+	MOVQ	56(SP), AX
+	JMP	alltraps(SB)
+
+	INT	$3
 	CALL	sysentry(SB)
 	INT	$3
 
-TEXT sysexitportal(SB), NOSPLIT, $0-8
-	MOVQ	thread+0(FP), AX
-	MOVQ	0x90(AX), DX
-	MOVQ	0xa8(AX), CX
+// this is unused
+TEXT sysexitportal(SB), NOSPLIT, $0-56
+	MOVQ	pmap+24(FP), AX
+	MOVQ	AX, CR3
+
+	MOVQ	fsb+16(FP), AX
+
+	// restore fsbase
+	MOVQ	IA32_FS_BASE, CX
+	MOVQ	AX, DX
+	ANDQ	$((1 << 32) - 1), AX
+	SHRQ	$32, DX
+	WRMSR
+
+	MOVQ	rip+0(FP), DX
+	MOVQ	rsp+8(FP), CX
+	MOVQ	rax+32(FP), AX
+	MOVQ	rbp+40(FP), BP
+	MOVQ	rbx+48(FP), BX
+
+	//MOVQ	0x90(AX), DX
+	//MOVQ	0xa8(AX), CX
 	// stack cannot be used after STI since an int using the IST can come
 	// in and clobber our stack
 	STI
+
 	// rcx contains rsp
 	// rdx contains rip
 	// rex64 sysexit

commit 12f6359215edd94c1614eb0faf0a172e27d0b301
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Aug 10 13:16:16 2015 -0400

    use runtime memmove/memclr instead of homerolled ones
    
    the builtin ones are optimized to used SSE and increase performance of a hacked
    version of mailbench by 57%.
    
    i made my own when i first started biscuit because i wasn't sure what the
    built-in ones depended on and we need these very early in the boot process.
    
    thus, we must call fpuinit() earlier in boot process.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 487664ab06..493001a390 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -263,6 +263,7 @@ h_needtls:
 	// i cannot fix CS via far call to a label because i don't know how to
 	// call a label with plan9 compiler.
 	CALL	fixcs(SB)
+	CALL	fpuinit(SB)
 
 	// store through it, to make sure it works
 	get_tls(BX)
@@ -289,7 +290,6 @@ h_ok:
 
 	CALL	int_setup(SB)
 	CALL	proc_setup(SB)
-	CALL	fpuinit(SB)
 
 	//MOVQ	CR0, AX
 	//PUSHQ	AX

commit d85e8930054a70fe8b14ec13ed472befac44001c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Aug 6 16:43:10 2015 -0400

    don't use int instruction to yield
    
    build trap frame in software

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 377ef3889d..487664ab06 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -801,9 +801,113 @@ TEXT _trapret(SB), NOSPLIT, $0-8
 	BYTE	$0x48
 	BYTE	$0xcf
 
-TEXT goodbye(SB), NOSPLIT, $0-8
+TEXT gtest(SB), NOSPLIT, $0
+	MOVQ	$1, AX
+	MOVQ	$2, BX
+	MOVQ	$3, CX
+	MOVQ	$4, DX
+	MOVQ	$5, DI
+	MOVQ	$6, SI
+	MOVQ	$7, R8
+	MOVQ	$8, R9
+	MOVQ	$9, R10
+	MOVQ	$10, R11
+	MOVQ	$11, R12
+	MOVQ	$12, R13
+	MOVQ	$13, R14
+	MOVQ	$14, R15
+	PUSHQ	TRAP_YIELD
+	CALL	mktrap(SB)
+	ADDQ	$8, SP
+	CMPQ	AX, $1
+	JNE	badinko
+	CMPQ	BX, $2
+	JNE	badinko
+	CMPQ	CX, $3
+	JNE	badinko
+	CMPQ	DX, $4
+	JNE	badinko
+	CMPQ	DI, $5
+	JNE	badinko
+	CMPQ	SI, $6
+	JNE	badinko
+	CMPQ	R8, $7
+	JNE	badinko
+	CMPQ	R9, $8
+	JNE	badinko
+	CMPQ	R10, $9
+	JNE	badinko
+	CMPQ	R11, $10
+	JNE	badinko
+	CMPQ	R12, $11
+	JNE	badinko
+	CMPQ	R13, $12
+	JNE	badinko
+	CMPQ	R14, $13
+	JNE	badinko
+	CMPQ	R15, $14
+	JNE	badinko
+	RET
+badinko:
+	INT	$3
+
+// void mktrap(uint64 intn)
+TEXT mktrap(SB), NOSPLIT, $0-8
+	PUSHQ	AX
+	PUSHQ	DX
+	PUSHFQ
+	POPQ	AX
+
 	CLI
-	MOVQ	tfptr+0(FP), R15
+
+	// do hardware trap frame; get CPU's interrupt stack
+	MOVQ	16(GS), DX
+
+	// save rflags first
+	MOVQ	AX, -24(DX)
+
+	XORQ	AX, AX
+	// mov %ss, %ax
+	BYTE	$0x66
+	BYTE	$0x8c
+	BYTE	$0xd0
+	MOVQ	AX, -8(DX)
+
+	MOVQ	SP, AX
+	// get rid of our pushes and ret addr, return there directly
+	ADDQ	$24, AX
+	MOVQ	AX, -16(DX)
+
+	XORQ	AX, AX
+	// mov %cs, %ax
+	BYTE	$0x66
+	BYTE	$0x8c
+	BYTE	$0xc8
+	MOVQ	AX, -32(DX)
+
+	// ret addr
+	MOVQ	ret+-8(FP), AX
+	MOVQ	AX, -40(DX)
+
+	// dummy error code
+	MOVQ	$0, -48(DX)
+
+	// interrupt number
+	MOVQ	intn+0(FP), AX
+	MOVQ	AX, -56(DX)
+
+	// and finally, restore rax and rdx
+	POPQ	AX
+	MOVQ	AX, -64(DX)
+
+	POPQ	AX
+	MOVQ	AX, -72(DX)
+
+	LEAQ	-72(DX), SP
+
+	POPQ	AX
+	POPQ	DX
+
 	JMP	alltraps(SB)
 
 TEXT _sysentry(SB), NOSPLIT, $0

commit 557416733ff81970f56e3e6a66e97de156981291
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Aug 6 14:41:34 2015 -0400

    use %gs in kernel mode to keep per-cpu state

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 55640c9bf9..377ef3889d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -122,6 +122,7 @@ TEXT runtime·asminit(SB),NOSPLIT,$0-0
 #define		CODESEG		1
 #define		DATASEG		2
 #define		FSSEG		3
+#define		GSSEG		4
 
 TEXT fixcs(SB),NOSPLIT,$0
 	POPQ	AX
@@ -827,6 +828,17 @@ TEXT sysexitportal(SB), NOSPLIT, $0-8
 	BYTE	$0x35
 	INT	$3
 
+TEXT gs_null(SB), NOSPLIT, $8-0
+	XORQ	AX, AX
+	PUSHQ	AX
+	POPQ	GS
+	RET
+
+TEXT gscpu(SB), NOSPLIT, $0-8
+	MOVQ	0(GS), AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
 /*
  *  go-routine
  */

commit 2f77267ebb7f01d46d2ca6850fdf684f9139fe4b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Aug 6 10:27:37 2015 -0400

    sysenter/sysexit
    
    not using it for syscalls yet

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 25e1b37c3a..55640c9bf9 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -800,6 +800,33 @@ TEXT _trapret(SB), NOSPLIT, $0-8
 	BYTE	$0x48
 	BYTE	$0xcf
 
+TEXT goodbye(SB), NOSPLIT, $0-8
+	CLI
+	MOVQ	tfptr+0(FP), R15
+	JMP	alltraps(SB)
+
+TEXT _sysentry(SB), NOSPLIT, $0
+	// rcx contains rsp, rdx contains ret addr
+	PUSHQ	CX
+	PUSHQ	DX
+	CALL	sysentry(SB)
+	INT	$3
+
+TEXT sysexitportal(SB), NOSPLIT, $0-8
+	MOVQ	thread+0(FP), AX
+	MOVQ	0x90(AX), DX
+	MOVQ	0xa8(AX), CX
+	// stack cannot be used after STI since an int using the IST can come
+	// in and clobber our stack
+	STI
+	// rcx contains rsp
+	// rdx contains rip
+	// rex64 sysexit
+	BYTE	$0x48
+	BYTE	$0x0f
+	BYTE	$0x35
+	INT	$3
+
 /*
  *  go-routine
  */

commit 0a194c889f673cfbc0af7672f98c17ee0f2e987a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jul 17 17:45:05 2015 -0400

    use two yield functions
    
    so i can distinguish between callers easier during profiling.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b6bcb20e42..25e1b37c3a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -622,6 +622,14 @@ TEXT hack_yield(SB), NOSPLIT, $0-0
 	INT	TRAP_YIELD
 	RET
 
+TEXT fut_hack_yield(SB), NOSPLIT, $0-0
+	INT	TRAP_YIELD
+	RET
+
+TEXT find_hack_yield(SB), NOSPLIT, $0-0
+	INT	TRAP_YIELD
+	RET
+
 #define IH_NOEC(num, fn)		\
 TEXT fn(SB), NOSPLIT, $0-0;		\
 	PUSHQ	$0;			\

commit 27e911a554f2db3369928a45b5cf0fe2dd8bcd7b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Jul 13 12:49:11 2015 -0400

    make go profiler work!
    
    required simulating signals and setitimer (and thus an accurate way to count
    virtual time).
    
    also, make TLB shootdowns always save/restore context to reduce complexity.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cf7b74fe73..b6bcb20e42 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -680,6 +680,7 @@ IH_NOEC(48,Xspur )
 IH_NOEC(49,Xyield )
 IH_NOEC(64,Xsyscall )
 IH_NOEC(70,Xtlbshoot )
+IH_NOEC(71,Xsigret )
 
 // irqs
 // irq0 is Xtimer
@@ -751,12 +752,16 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 
 TEXT trapret(SB), NOSPLIT, $0-16
 	MOVQ	pmap+8(FP), BX
+	MOVQ	BX, CR3
+
+	JMP	_trapret(SB)
+	INT	$3
+
+TEXT _trapret(SB), NOSPLIT, $0-8
 	MOVQ	tf+0(FP), AX	// tf is not on the callers stack frame, but in
 				// threads[]
 	MOVQ	AX, SP
 
-	MOVQ	BX, CR3
-
 	// restore fsbase
 	MOVQ	IA32_FS_BASE, CX
 	POPQ	AX

commit 4188417ffa117ea3ca927335a72d8f5bf536c69f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jul 7 13:07:13 2015 -0400

    TLB shootdowns with test
    
    i learned that gcc will not warn if it fails to align data as requested.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e737796b62..cf7b74fe73 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -343,6 +343,12 @@ TEXT runtime·Cpuid(SB), NOSPLIT, $0-24
 	MOVL	DX, ret+20(FP)
 	RET
 
+TEXT runtime·atomic_dec(SB), NOSPLIT, $0-8
+	MOVQ	addr+0(FP), AX
+	LOCK
+	DECQ	(AX)
+	RET
+
 TEXT finit(SB), NOSPLIT, $0-0
 	FINIT
 	RET
@@ -626,7 +632,7 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	POPQ	AX;			\
 	POPQ	AX;			\
 	RET
-// pops are to silence plan9 warnings
+// pops are to silence plan9 assembler warnings
 
 #define IH_IRQ(num, fn)			\
 TEXT fn(SB), NOSPLIT, $0-0;		\
@@ -673,6 +679,7 @@ IH_NOEC(32,Xtimer )
 IH_NOEC(48,Xspur )
 IH_NOEC(49,Xyield )
 IH_NOEC(64,Xsyscall )
+IH_NOEC(70,Xtlbshoot )
 
 // irqs
 // irq0 is Xtimer

commit 069222d403218b151bde2cf8faf1570d69a91997
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jun 30 23:06:20 2015 -0400

    separate code for timer interrupts and yielding
    
    since i now use the timer interrupt to keep track of wall clock time, we need
    to differentiate yields from clock ticks since only clock ticks should
    increment the uptime counter. the runtime yields a lot due to futexs.
    
    it is important that the yield interrupt use an IST since it is called from go
    routines.
    
    however, i do not yet initialize the uptime ticks -- i start at 0. thus the
    runtime thinks it is dec 31st, 1969. i don't care about this yet.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 255acab23c..e737796b62 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -288,7 +288,6 @@ h_ok:
 
 	CALL	int_setup(SB)
 	CALL	proc_setup(SB)
-
 	CALL	fpuinit(SB)
 
 	//MOVQ	CR0, AX
@@ -612,9 +611,9 @@ TEXT runtime·handle_int(SB), NOSPLIT, $0-0
 	POPQ	DX
 	RET
 
-#define TRAP_TIMER      $32
+#define TRAP_YIELD      $49
 TEXT hack_yield(SB), NOSPLIT, $0-0
-	INT	TRAP_TIMER
+	INT	TRAP_YIELD
 	RET
 
 #define IH_NOEC(num, fn)		\
@@ -672,6 +671,7 @@ IH_NOEC(19,Xfp )
 IH_NOEC(20,Xve )
 IH_NOEC(32,Xtimer )
 IH_NOEC(48,Xspur )
+IH_NOEC(49,Xyield )
 IH_NOEC(64,Xsyscall )
 
 // irqs

commit f5d494bbdf945f2662eb4da45cdb75de2b7d43d4
Author: Austin Clements <austin@google.com>
Date:   Mon Jun 15 12:30:23 2015 -0400

    runtime: ensure GC sees type-safe memory on weak machines
    
    Currently its possible for the garbage collector to observe
    uninitialized memory or stale heap bitmap bits on weakly ordered
    architectures such as ARM and PPC. On such architectures, the stores
    that zero newly allocated memory and initialize its heap bitmap may
    move after a store in user code that makes the allocated object
    observable by the garbage collector.
    
    To fix this, add a "publication barrier" (also known as an "export
    barrier") before returning from mallocgc. This is a store/store
    barrier that ensures any write done by user code that makes the
    returned object observable to the garbage collector will be ordered
    after the initialization performed by mallocgc. No barrier is
    necessary on the reading side because of the data dependency between
    loading the pointer and loading the contents of the object.
    
    Fixes one of the issues raised in #9984.
    
    Change-Id: Ia3d96ad9c5fc7f4d342f5e05ec0ceae700cd17c8
    Reviewed-on: https://go-review.googlesource.com/11083
    Reviewed-by: Rick Hudson <rlh@golang.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Minux Ma <minux@golang.org>
    Reviewed-by: Martin Capitanio <capnm9@gmail.com>
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 13cca8e460..3b4ca4d012 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -615,6 +615,11 @@ TEXT runtime·atomicand8(SB), NOSPLIT, $0-9
 	ANDB	BX, (AX)
 	RET
 
+TEXT ·publicationBarrier(SB),NOSPLIT,$0-0
+	// Stores are already ordered on x86, so this is just a
+	// compile barrier.
+	RET
+
 // void jmpdefer(fn, sp);
 // called from deferreturn.
 // 1. pop the caller

commit 9d968cb47b240f47b40809639a0a532e752f6a08
Author: Alex Brainman <alex.brainman@gmail.com>
Date:   Mon Apr 27 17:32:23 2015 +1000

    runtime: rename cgocall_errno and asmcgocall_errno into cgocall and asmcgocall
    
    Change-Id: I5917bea8bb35b0e725dcc56a68f3a70137cfc180
    Reviewed-on: https://go-review.googlesource.com/9387
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 639ab340fe..13cca8e460 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -641,19 +641,14 @@ TEXT gosave<>(SB),NOSPLIT,$0
 	MOVQ	BP, (g_sched+gobuf_bp)(R8)
 	RET
 
-// asmcgocall(void(*fn)(void*), void *arg)
+// func asmcgocall(fn, arg unsafe.Pointer) int32
 // Call fn(arg) on the scheduler stack,
 // aligned appropriately for the gcc ABI.
-// See cgocall.c for more details.
-TEXT ·asmcgocall_errno(SB),NOSPLIT,$0-20
+// See cgocall.go for more details.
+TEXT ·asmcgocall(SB),NOSPLIT,$0-20
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX
-	CALL	asmcgocall<>(SB)
-	MOVL	AX, ret+16(FP)
-	RET
 
-// asmcgocall common code. fn in AX, arg in BX. returns errno in AX.
-TEXT asmcgocall<>(SB),NOSPLIT,$0-0
 	MOVQ	SP, DX
 
 	// Figure out if we need to switch to m->g0 stack.
@@ -696,6 +691,8 @@ nosave:
 	SUBQ	40(SP), SI
 	MOVQ	DI, g(CX)
 	MOVQ	SI, SP
+
+	MOVL	AX, ret+16(FP)
 	RET
 
 // cgocallback(void (*fn)(void*), void *frame, uintptr framesize)
@@ -713,7 +710,7 @@ TEXT runtime·cgocallback(SB),NOSPLIT,$24-24
 	RET
 
 // cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize)
-// See cgocall.c for more details.
+// See cgocall.go for more details.
 TEXT ·cgocallback_gofunc(SB),NOSPLIT,$8-24
 	NO_LOCAL_POINTERS
 

commit 2858b7384351129ee502da8c51e9625e55d88367
Author: Alex Brainman <alex.brainman@gmail.com>
Date:   Wed Jun 17 16:48:02 2015 +1000

    runtime: remove cgocall and asmcgocall
    
    In preparation for rename of cgocall_errno into cgocall and
    asmcgocall_errno into asmcgocall in the fllowinng CL.
    rsc requested CL 9387 to be split into two parts. This is first part.
    
    Change-Id: I7434f0e4b44dd37017540695834bfcb1eebf0b2f
    Reviewed-on: https://go-review.googlesource.com/11166
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d43e660cb4..639ab340fe 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -645,12 +645,6 @@ TEXT gosave<>(SB),NOSPLIT,$0
 // Call fn(arg) on the scheduler stack,
 // aligned appropriately for the gcc ABI.
 // See cgocall.c for more details.
-TEXT ·asmcgocall(SB),NOSPLIT,$0-16
-	MOVQ	fn+0(FP), AX
-	MOVQ	arg+8(FP), BX
-	CALL	asmcgocall<>(SB)
-	RET
-
 TEXT ·asmcgocall_errno(SB),NOSPLIT,$0-20
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX

commit faa7a7e8ae824c78e78b272604f89e834ade6695
Author: Austin Clements <austin@google.com>
Date:   Wed May 20 16:30:49 2015 -0400

    runtime: implement GC stack barriers
    
    This commit implements stack barriers to minimize the amount of
    stack re-scanning that must be done during mark termination.
    
    Currently the GC scans stacks of active goroutines twice during every
    GC cycle: once at the beginning during root discovery and once at the
    end during mark termination. The second scan happens while the world
    is stopped and guarantees that we've seen all of the roots (since
    there are no write barriers on writes to local stack
    variables). However, this means pause time is proportional to stack
    size. In particularly recursive programs, this can drive pause time up
    past our 10ms goal (e.g., it takes about 150ms to scan a 50MB heap).
    
    Re-scanning the entire stack is rarely necessary, especially for large
    stacks, because usually most of the frames on the stack were not
    active between the first and second scans and hence any changes to
    these frames (via non-escaping pointers passed down the stack) were
    tracked by write barriers.
    
    To efficiently track how far a stack has been unwound since the first
    scan (and, hence, how much needs to be re-scanned), this commit
    introduces stack barriers. During the first scan, at exponentially
    spaced points in each stack, the scan overwrites return PCs with the
    PC of the stack barrier function. When "returned" to, the stack
    barrier function records how far the stack has unwound and jumps to
    the original return PC for that point in the stack. Then the second
    scan only needs to proceed as far as the lowest barrier that hasn't
    been hit.
    
    For deeply recursive programs, this substantially reduces mark
    termination time (and hence pause time). For the goscheme example
    linked in issue #10898, prior to this change, mark termination times
    were typically between 100 and 500ms; with this change, mark
    termination times are typically between 10 and 20ms. As a result of
    the reduced stack scanning work, this reduces overall execution time
    of the goscheme example by 20%.
    
    Fixes #10898.
    
    The effect of this on programs that are not deeply recursive is
    minimal:
    
    name                   old time/op    new time/op    delta
    BinaryTree17              3.16s ± 2%     3.26s ± 1%  +3.31%  (p=0.000 n=19+19)
    Fannkuch11                2.42s ± 1%     2.48s ± 1%  +2.24%  (p=0.000 n=17+19)
    FmtFprintfEmpty          50.0ns ± 3%    49.8ns ± 1%    ~     (p=0.534 n=20+19)
    FmtFprintfString          173ns ± 0%     175ns ± 0%  +1.49%  (p=0.000 n=16+19)
    FmtFprintfInt             170ns ± 1%     175ns ± 1%  +2.97%  (p=0.000 n=20+19)
    FmtFprintfIntInt          288ns ± 0%     295ns ± 0%  +2.73%  (p=0.000 n=16+19)
    FmtFprintfPrefixedInt     242ns ± 1%     252ns ± 1%  +4.13%  (p=0.000 n=18+18)
    FmtFprintfFloat           324ns ± 0%     323ns ± 0%  -0.36%  (p=0.000 n=20+19)
    FmtManyArgs              1.14µs ± 0%    1.12µs ± 1%  -1.01%  (p=0.000 n=18+19)
    GobDecode                8.88ms ± 1%    8.87ms ± 0%    ~     (p=0.480 n=19+18)
    GobEncode                6.80ms ± 1%    6.85ms ± 0%  +0.82%  (p=0.000 n=20+18)
    Gzip                      363ms ± 1%     363ms ± 1%    ~     (p=0.077 n=18+20)
    Gunzip                   90.6ms ± 0%    90.0ms ± 1%  -0.71%  (p=0.000 n=17+18)
    HTTPClientServer         51.5µs ± 1%    50.8µs ± 1%  -1.32%  (p=0.000 n=18+18)
    JSONEncode               17.0ms ± 0%    17.1ms ± 0%  +0.40%  (p=0.000 n=18+17)
    JSONDecode               61.8ms ± 0%    63.8ms ± 1%  +3.11%  (p=0.000 n=18+17)
    Mandelbrot200            3.84ms ± 0%    3.84ms ± 1%    ~     (p=0.583 n=19+19)
    GoParse                  3.71ms ± 1%    3.72ms ± 1%    ~     (p=0.159 n=18+19)
    RegexpMatchEasy0_32       100ns ± 0%     100ns ± 1%  -0.19%  (p=0.033 n=17+19)
    RegexpMatchEasy0_1K       342ns ± 1%     331ns ± 0%  -3.41%  (p=0.000 n=19+19)
    RegexpMatchEasy1_32      82.5ns ± 0%    81.7ns ± 0%  -0.98%  (p=0.000 n=18+18)
    RegexpMatchEasy1_1K       505ns ± 0%     494ns ± 1%  -2.16%  (p=0.000 n=18+18)
    RegexpMatchMedium_32      137ns ± 1%     137ns ± 1%  -0.24%  (p=0.048 n=20+18)
    RegexpMatchMedium_1K     41.6µs ± 0%    41.3µs ± 1%  -0.57%  (p=0.004 n=18+20)
    RegexpMatchHard_32       2.11µs ± 0%    2.11µs ± 1%  +0.20%  (p=0.037 n=17+19)
    RegexpMatchHard_1K       63.9µs ± 2%    63.3µs ± 0%  -0.99%  (p=0.000 n=20+17)
    Revcomp                   560ms ± 1%     522ms ± 0%  -6.87%  (p=0.000 n=18+16)
    Template                 75.0ms ± 0%    75.1ms ± 1%  +0.18%  (p=0.013 n=18+19)
    TimeParse                 358ns ± 1%     364ns ± 0%  +1.74%  (p=0.000 n=20+15)
    TimeFormat                360ns ± 0%     372ns ± 0%  +3.55%  (p=0.000 n=20+18)
    
    Change-Id: If8a9bfae6c128d15a4f405e02bcfa50129df82a2
    Reviewed-on: https://go-review.googlesource.com/10314
    Reviewed-by: Russ Cox <rsc@golang.org>
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0f9aeb8f37..d43e660cb4 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -336,6 +336,22 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 	MOVL	$0, DX
 	JMP	runtime·morestack(SB)
 
+TEXT runtime·stackBarrier(SB),NOSPLIT,$0
+	// We came here via a RET to an overwritten return PC.
+	// AX may be live. Other registers are available.
+
+	// Get the original return PC, g.stkbar[g.stkbarPos].savedLRVal.
+	get_tls(CX)
+	MOVQ	g(CX), CX
+	MOVQ	(g_stkbar+slice_array)(CX), DX
+	MOVQ	g_stkbarPos(CX), BX
+	IMULQ	$stkbar__size, BX	// Too big for SIB.
+	MOVQ	stkbar_savedLRVal(DX)(BX*1), BX
+	// Record that this stack barrier was hit.
+	ADDQ	$1, g_stkbarPos(CX)
+	// Jump to the original return PC.
+	JMP	BX
+
 // reflectcall: call a function with the given argument list
 // func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
 // we don't have variable-sized frames, so we use a small number
@@ -860,17 +876,31 @@ TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
 	INT	$3
 	RET
 
-TEXT runtime·getcallerpc(SB),NOSPLIT,$0-16
+TEXT runtime·getcallerpc(SB),NOSPLIT,$8-16
 	MOVQ	argp+0(FP),AX		// addr of first arg
 	MOVQ	-8(AX),AX		// get calling pc
+	CMPQ	AX, runtime·stackBarrierPC(SB)
+	JNE	nobar
+	// Get original return PC.
+	CALL	runtime·nextBarrierPC(SB)
+	MOVQ	0(SP), AX
+nobar:
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT runtime·setcallerpc(SB),NOSPLIT,$0-16
+TEXT runtime·setcallerpc(SB),NOSPLIT,$8-16
 	MOVQ	argp+0(FP),AX		// addr of first arg
 	MOVQ	pc+8(FP), BX
+	MOVQ	-8(AX), CX
+	CMPQ	CX, runtime·stackBarrierPC(SB)
+	JEQ	setbar
 	MOVQ	BX, -8(AX)		// set calling pc
 	RET
+setbar:
+	// Set the stack barrier return PC.
+	MOVQ	BX, 0(SP)
+	CALL	runtime·setNextBarrierPC(SB)
+	RET
 
 TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
 	MOVQ	argp+0(FP), AX

commit be0cb9224b68d5be4e03fd35396d2c2f0755adad
Author: Michael Hudson-Doyle <michael.hudson@canonical.com>
Date:   Tue May 12 11:59:14 2015 +1200

    runtime: fix addmoduledata to follow the platform ABI
    
    addmoduledata is called from a .init_array function and need to follow the
    platform ABI. It contains accesses to global data which are rewritten to use
    R15 by the assembler, and as R15 is callee-save we need to save it.
    
    Change-Id: I03893efb1576aed4f102f2465421f256f3bb0f30
    Reviewed-on: https://go-review.googlesource.com/9941
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 36353d108f..0f9aeb8f37 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1693,8 +1693,10 @@ TEXT runtime·prefetchnta(SB),NOSPLIT,$0-8
 	RET
 
 // This is called from .init_array and follows the platform, not Go, ABI.
-TEXT runtime·addmoduledata(SB),NOSPLIT,$0-8
+TEXT runtime·addmoduledata(SB),NOSPLIT,$0-0
+	PUSHQ	R15 // The access to global variables below implicitly uses R15, which is callee-save
 	MOVQ	runtime·lastmoduledatap(SB), AX
 	MOVQ	DI, moduledata_next(AX)
 	MOVQ	DI, runtime·lastmoduledatap(SB)
+	POPQ	R15
 	RET

commit 17fd1cf934bc4439b674fc34a0befe79994441e2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon May 4 22:06:24 2015 -0400

    ulimit for memory, fix open
    
    open with O_CREAT should fail if the file already exists only if O_EXCL is also
    given.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d3c7602d3c..255acab23c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -170,7 +170,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 
 	// save page table and first free address from bootloader.
 	MOVL	DI, kpmap(SB)
-	MOVL	SI, runtime·Pgfirst(SB)
+	MOVL	SI, pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 
 	ANDQ	$~15, SP

commit c526f3ac1099fef117a385d0336860cacde6b257
Author: Keith Randall <khr@golang.org>
Date:   Tue Apr 21 14:22:41 2015 -0700

    runtime: tail call into memeq/cmp body implementations
    
    There's no need to call/ret to the body implementation.
    It can write the result to the right place.  Just jump to
    it and have it return to our caller.
    
    Old:
      call body implementation
      compute result
      put result in a register
      return
      write register to result location
      return
    
    New:
      load address of result location into a register
      jump to body implementation
      compute result
      write result to passed-in address
      return
    
    It's a bit tricky on 386 because there is no free register
    with which to pass the result location.  Free up a register
    by keeping around blen-alen instead of both alen and blen.
    
    Change-Id: If2cf0682a5bf1cc592bdda7c126ed4eee8944fba
    Reviewed-on: https://go-review.googlesource.com/9202
    Reviewed-by: Josh Bleecher Snyder <josharian@gmail.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 02e25f7402..36353d108f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1262,9 +1262,8 @@ TEXT runtime·memeq(SB),NOSPLIT,$0-25
 	MOVQ	a+0(FP), SI
 	MOVQ	b+8(FP), DI
 	MOVQ	size+16(FP), BX
-	CALL	runtime·memeqbody(SB)
-	MOVB	AX, ret+24(FP)
-	RET
+	LEAQ	ret+24(FP), AX
+	JMP	runtime·memeqbody(SB)
 
 // memequal_varlen(a, b unsafe.Pointer) bool
 TEXT runtime·memequal_varlen(SB),NOSPLIT,$0-17
@@ -1273,9 +1272,8 @@ TEXT runtime·memequal_varlen(SB),NOSPLIT,$0-17
 	CMPQ	SI, DI
 	JEQ	eq
 	MOVQ	8(DX), BX    // compiler stores size at offset 8 in the closure
-	CALL	runtime·memeqbody(SB)
-	MOVB	AX, ret+16(FP)
-	RET
+	LEAQ	ret+16(FP), AX
+	JMP	runtime·memeqbody(SB)
 eq:
 	MOVB	$1, ret+16(FP)
 	RET
@@ -1291,9 +1289,8 @@ TEXT runtime·eqstring(SB),NOSPLIT,$0-33
 	CMPQ	SI, DI
 	JEQ	eq
 	MOVQ	s1len+8(FP), BX
-	CALL	runtime·memeqbody(SB)
-	MOVB	AX, v+32(FP)
-	RET
+	LEAQ	v+32(FP), AX
+	JMP	runtime·memeqbody(SB)
 eq:
 	MOVB	$1, v+32(FP)
 	RET
@@ -1301,9 +1298,8 @@ eq:
 // a in SI
 // b in DI
 // count in BX
+// address of result byte in AX
 TEXT runtime·memeqbody(SB),NOSPLIT,$0-0
-	XORQ	AX, AX
-
 	CMPQ	BX, $8
 	JB	small
 	
@@ -1332,6 +1328,7 @@ hugeloop:
 	SUBQ	$64, BX
 	CMPL	DX, $0xffff
 	JEQ	hugeloop
+	MOVB	$0, (AX)
 	RET
 
 	// 8 bytes at a time using 64-bit register
@@ -1345,6 +1342,7 @@ bigloop:
 	SUBQ	$8, BX
 	CMPQ	CX, DX
 	JEQ	bigloop
+	MOVB	$0, (AX)
 	RET
 
 	// remaining 0-8 bytes
@@ -1352,7 +1350,7 @@ leftover:
 	MOVQ	-8(SI)(BX*1), CX
 	MOVQ	-8(DI)(BX*1), DX
 	CMPQ	CX, DX
-	SETEQ	AX
+	SETEQ	(AX)
 	RET
 
 small:
@@ -1387,7 +1385,7 @@ di_finish:
 	SUBQ	SI, DI
 	SHLQ	CX, DI
 equal:
-	SETEQ	AX
+	SETEQ	(AX)
 	RET
 
 TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
@@ -1395,26 +1393,23 @@ TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
 	MOVQ	s1_len+8(FP), BX
 	MOVQ	s2_base+16(FP), DI
 	MOVQ	s2_len+24(FP), DX
-	CALL	runtime·cmpbody(SB)
-	MOVQ	AX, ret+32(FP)
-	RET
+	LEAQ	ret+32(FP), R9
+	JMP	runtime·cmpbody(SB)
 
 TEXT bytes·Compare(SB),NOSPLIT,$0-56
 	MOVQ	s1+0(FP), SI
 	MOVQ	s1+8(FP), BX
 	MOVQ	s2+24(FP), DI
 	MOVQ	s2+32(FP), DX
-	CALL	runtime·cmpbody(SB)
-	MOVQ	AX, res+48(FP)
-	RET
+	LEAQ	res+48(FP), R9
+	JMP	runtime·cmpbody(SB)
 
 // input:
 //   SI = a
 //   DI = b
 //   BX = alen
 //   DX = blen
-// output:
-//   AX = 1/0/-1
+//   R9 = address of output word (stores -1/0/1 here)
 TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
 	CMPQ	SI, DI
 	JEQ	allsame
@@ -1446,6 +1441,7 @@ diff16:
 	CMPB	CX, (DI)(BX*1)
 	SETHI	AX
 	LEAQ	-1(AX*2), AX	// convert 1/0 to +1/-1
+	MOVQ	AX, (R9)
 	RET
 
 	// 0 through 16 bytes left, alen>=8, blen>=8
@@ -1471,6 +1467,7 @@ diff8:
 	SHRQ	CX, AX	// move a's bit to bottom
 	ANDQ	$1, AX	// mask bit
 	LEAQ	-1(AX*2), AX // 1/0 => +1/-1
+	MOVQ	AX, (R9)
 	RET
 
 	// 0-7 bytes in common
@@ -1509,6 +1506,7 @@ di_finish:
 	SHRQ	CX, SI	// move a's bit to bottom
 	ANDQ	$1, SI	// mask bit
 	LEAQ	-1(SI*2), AX // 1/0 => +1/-1
+	MOVQ	AX, (R9)
 	RET
 
 allsame:
@@ -1518,30 +1516,28 @@ allsame:
 	SETGT	AX	// 1 if alen > blen
 	SETEQ	CX	// 1 if alen == blen
 	LEAQ	-1(CX)(AX*2), AX	// 1,0,-1 result
+	MOVQ	AX, (R9)
 	RET
 
 TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX
 	MOVB c+24(FP), AL
-	CALL runtime·indexbytebody(SB)
-	MOVQ AX, ret+32(FP)
-	RET
+	LEAQ ret+32(FP), R8
+	JMP  runtime·indexbytebody(SB)
 
 TEXT strings·IndexByte(SB),NOSPLIT,$0-32
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX
 	MOVB c+16(FP), AL
-	CALL runtime·indexbytebody(SB)
-	MOVQ AX, ret+24(FP)
-	RET
+	LEAQ ret+24(FP), R8
+	JMP  runtime·indexbytebody(SB)
 
 // input:
 //   SI: data
 //   BX: data len
 //   AL: byte sought
-// output:
-//   AX
+//   R8: address to put result
 TEXT runtime·indexbytebody(SB),NOSPLIT,$0
 	MOVQ SI, DI
 
@@ -1600,7 +1596,7 @@ condition:
 	JZ success
 
 failure:
-	MOVQ $-1, AX
+	MOVQ $-1, (R8)
 	RET
 
 // handle for lengths < 16
@@ -1608,7 +1604,7 @@ small:
 	MOVQ BX, CX
 	REPN; SCASB
 	JZ success
-	MOVQ $-1, AX
+	MOVQ $-1, (R8)
 	RET
 
 // we've found the chunk containing the byte
@@ -1618,26 +1614,26 @@ ssesuccess:
 	BSFW DX, DX
 	SUBQ SI, DI
 	ADDQ DI, DX
-	MOVQ DX, AX
+	MOVQ DX, (R8)
 	RET
 
 success:
 	SUBQ SI, DI
 	SUBL $1, DI
-	MOVQ DI, AX
+	MOVQ DI, (R8)
 	RET
 
 TEXT bytes·Equal(SB),NOSPLIT,$0-49
 	MOVQ	a_len+8(FP), BX
 	MOVQ	b_len+32(FP), CX
-	XORQ	AX, AX
 	CMPQ	BX, CX
 	JNE	eqret
 	MOVQ	a+0(FP), SI
 	MOVQ	b+24(FP), DI
-	CALL	runtime·memeqbody(SB)
+	LEAQ	ret+48(FP), AX
+	JMP	runtime·memeqbody(SB)
 eqret:
-	MOVB	AX, ret+48(FP)
+	MOVB	$0, ret+48(FP)
 	RET
 
 TEXT runtime·fastrand1(SB), NOSPLIT, $0-4

commit 6ad33be2d9d6b24aa741b3007a4bcd52db222c41
Author: Srdjan Petrovic <spetrovic@google.com>
Date:   Thu Apr 16 14:32:18 2015 -0700

    runtime: implement xadduintptr and update system mstats using it
    
    The motivation is that sysAlloc/Free() currently aren't safe to be
    called without a valid G, because arm's xadd64() uses locks that require
    a valid G.
    
    The solution here was proposed by Dmitry Vyukov: use xadduintptr()
    instead of xadd64(), until arm can support xadd64 on all of its
    architectures (not a trivial task for arm).
    
    Change-Id: I250252079357ea2e4360e1235958b1c22051498f
    Reviewed-on: https://go-review.googlesource.com/9002
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 468763f095..02e25f7402 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -530,6 +530,9 @@ TEXT runtime·xadd64(SB), NOSPLIT, $0-24
 	MOVQ	AX, ret+16(FP)
 	RET
 
+TEXT runtime·xadduintptr(SB), NOSPLIT, $0-24
+	JMP	runtime·xadd64(SB)
+
 TEXT runtime·xchg(SB), NOSPLIT, $0-20
 	MOVQ	ptr+0(FP), BX
 	MOVL	new+8(FP), AX

commit f616af23e0977cda63a0771d726d1529e70f17a4
Author: Michael Hudson-Doyle <michael.hudson@canonical.com>
Date:   Wed Apr 1 14:17:43 2015 +1300

    cmd/6l: call runtime.addmoduledata from .init_array
    
    Change-Id: I09e84161d106960a69972f5fc845a1e40c28e58f
    Reviewed-on: https://go-review.googlesource.com/8331
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0e5389fbd7..468763f095 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1692,3 +1692,10 @@ TEXT runtime·prefetchnta(SB),NOSPLIT,$0-8
 	MOVQ	addr+0(FP), AX
 	PREFETCHNTA	(AX)
 	RET
+
+// This is called from .init_array and follows the platform, not Go, ABI.
+TEXT runtime·addmoduledata(SB),NOSPLIT,$0-8
+	MOVQ	runtime·lastmoduledatap(SB), AX
+	MOVQ	DI, moduledata_next(AX)
+	MOVQ	DI, runtime·lastmoduledatap(SB)
+	RET

commit 92c826b1b2473e743964d3478b73a9c39a579abf
Author: Russ Cox <rsc@golang.org>
Date:   Fri Apr 3 12:23:28 2015 -0400

    cmd/internal/gc: inline runtime.getg
    
    This more closely restores what the old C runtime did.
    (In C, g was an 'extern register' with the same effective
    implementation as in this CL.)
    
    On a late 2012 MacBookPro10,2, best of 5 old vs best of 5 new:
    
    benchmark                          old ns/op      new ns/op      delta
    BenchmarkBinaryTree17              4981312777     4463426605     -10.40%
    BenchmarkFannkuch11                3046495712     3006819428     -1.30%
    BenchmarkFmtFprintfEmpty           89.3           79.8           -10.64%
    BenchmarkFmtFprintfString          284            262            -7.75%
    BenchmarkFmtFprintfInt             282            262            -7.09%
    BenchmarkFmtFprintfIntInt          480            448            -6.67%
    BenchmarkFmtFprintfPrefixedInt     382            358            -6.28%
    BenchmarkFmtFprintfFloat           529            486            -8.13%
    BenchmarkFmtManyArgs               1849           1773           -4.11%
    BenchmarkGobDecode                 12835963       11794385       -8.11%
    BenchmarkGobEncode                 10527170       10288422       -2.27%
    BenchmarkGzip                      436109569      438422516      +0.53%
    BenchmarkGunzip                    110121663      109843648      -0.25%
    BenchmarkHTTPClientServer          81930          85446          +4.29%
    BenchmarkJSONEncode                24638574       24280603       -1.45%
    BenchmarkJSONDecode                93022423       85753546       -7.81%
    BenchmarkMandelbrot200             4703899        4735407        +0.67%
    BenchmarkGoParse                   5319853        5086843        -4.38%
    BenchmarkRegexpMatchEasy0_32       151            151            +0.00%
    BenchmarkRegexpMatchEasy0_1K       452            453            +0.22%
    BenchmarkRegexpMatchEasy1_32       131            132            +0.76%
    BenchmarkRegexpMatchEasy1_1K       761            722            -5.12%
    BenchmarkRegexpMatchMedium_32      228            224            -1.75%
    BenchmarkRegexpMatchMedium_1K      63751          64296          +0.85%
    BenchmarkRegexpMatchHard_32        3188           3238           +1.57%
    BenchmarkRegexpMatchHard_1K        95396          96756          +1.43%
    BenchmarkRevcomp                   661587262      687107364      +3.86%
    BenchmarkTemplate                  108312598      104008540      -3.97%
    BenchmarkTimeParse                 453            459            +1.32%
    BenchmarkTimeFormat                475            441            -7.16%
    
    The garbage benchmark from the benchmarks subrepo gets 2.6% faster as well.
    
    Change-Id: I320aeda332db81012688b26ffab23f6581c59cfa
    Reviewed-on: https://go-review.googlesource.com/8460
    Reviewed-by: Rick Hudson <rlh@golang.org>
    Run-TryBot: Rick Hudson <rlh@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 946e151110..0e5389fbd7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1673,12 +1673,6 @@ TEXT runtime·goexit(SB),NOSPLIT,$0-0
 	// traceback from goexit1 must hit code range of goexit
 	BYTE	$0x90	// NOP
 
-TEXT runtime·getg(SB),NOSPLIT,$0-8
-	get_tls(CX)
-	MOVQ	g(CX), AX
-	MOVQ	AX, ret+0(FP)
-	RET
-
 TEXT runtime·prefetcht0(SB),NOSPLIT,$0-8
 	MOVQ	addr+0(FP), AX
 	PREFETCHT0	(AX)

commit ad3600945aaf168ba12ae82c9f66c3c60836ba87
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Tue Mar 31 09:19:10 2015 -0700

    runtime: auto-generate duff routines
    
    This makes it easier to experiment with alternative implementations.
    
    While we're here, update the comments.
    
    No functional changes. Passes toolstash -cmp.
    
    Change-Id: I428535754908f0fdd7cc36c214ddb6e1e60f376e
    Reviewed-on: https://go-review.googlesource.com/8310
    Reviewed-by: Keith Randall <khr@golang.org>
    Run-TryBot: Josh Bleecher Snyder <josharian@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index fdea05366e..946e151110 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1637,801 +1637,6 @@ eqret:
 	MOVB	AX, ret+48(FP)
 	RET
 
-// A Duff's device for zeroing memory.
-// The compiler jumps to computed addresses within
-// this routine to zero chunks of memory.  Do not
-// change this code without also changing the code
-// in ../../cmd/6g/ggen.c:clearfat.
-// AX: zero
-// DI: ptr to memory to be zeroed
-// DI is updated as a side effect.
-TEXT runtime·duffzero(SB), NOSPLIT, $0-0
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	STOSQ
-	RET
-
-// A Duff's device for copying memory.
-// The compiler jumps to computed addresses within
-// this routine to copy chunks of memory.  Source
-// and destination must not overlap.  Do not
-// change this code without also changing the code
-// in ../../cmd/6g/cgen.c:sgen.
-// SI: ptr to source memory
-// DI: ptr to destination memory
-// SI and DI are updated as a side effect.
-
-// NOTE: this is equivalent to a sequence of MOVSQ but
-// for some reason that is 3.5x slower than this code.
-// The STOSQ above seem fine, though.
-TEXT runtime·duffcopy(SB), NOSPLIT, $0-0
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	MOVQ	(SI),CX
-	ADDQ	$8,SI
-	MOVQ	CX,(DI)
-	ADDQ	$8,DI
-
-	RET
-
 TEXT runtime·fastrand1(SB), NOSPLIT, $0-4
 	get_tls(CX)
 	MOVQ	g(CX), AX

commit f78dc1dac1890ff58c6c82b1097620bf171ec149
Author: Michael Hudson-Doyle <michael.hudson@canonical.com>
Date:   Sun Mar 29 23:38:20 2015 +0000

    runtime: rename ·main·f to ·mainPC to avoid duplicate symbol
    
    runtime·main·f is normalized by the linker to runtime.main.f, as is
    the compiler-generated symbol runtime.main·f.  Change the former to
    runtime·mainPC instead.
    
    Fixes issue #9934
    
    Change-Id: I656a6fa6422d45385fa2cc55bd036c6affa1abfe
    Reviewed-on: https://go-review.googlesource.com/8234
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3039358d23..fdea05366e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -109,7 +109,7 @@ ok:
 	CALL	runtime·schedinit(SB)
 
 	// create a new goroutine to start program
-	MOVQ	$runtime·main·f(SB), AX		// entry
+	MOVQ	$runtime·mainPC(SB), AX		// entry
 	PUSHQ	AX
 	PUSHQ	$0			// arg size
 	CALL	runtime·newproc(SB)
@@ -122,8 +122,8 @@ ok:
 	MOVL	$0xf1, 0xf1  // crash
 	RET
 
-DATA	runtime·main·f+0(SB)/8,$runtime·main(SB)
-GLOBL	runtime·main·f(SB),RODATA,$8
+DATA	runtime·mainPC+0(SB)/8,$runtime·main(SB)
+GLOBL	runtime·mainPC(SB),RODATA,$8
 
 TEXT runtime·breakpoint(SB),NOSPLIT,$0-0
 	BYTE	$0xcc

commit 631d6a33bf2889c5c648555ab993687a48f9c287
Author: Russ Cox <rsc@golang.org>
Date:   Thu Mar 19 19:42:16 2015 -0400

    runtime: implement atomicand8 atomically
    
    We're skating on thin ice, and things are finally starting to melt around here.
    (I want to avoid the debugging session that will happen when someone
    uses atomicand8 expecting it to be atomic with respect to other operations.)
    
    Change-Id: I254f1582be4eb1f2d7fbba05335a91c6bf0c7f02
    Reviewed-on: https://go-review.googlesource.com/7861
    Reviewed-by: Minux Ma <minux@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 09fbb51337..3039358d23 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -588,6 +588,14 @@ TEXT runtime·atomicor8(SB), NOSPLIT, $0-9
 	ORB	BX, (AX)
 	RET
 
+// void	runtime·atomicand8(byte volatile*, byte);
+TEXT runtime·atomicand8(SB), NOSPLIT, $0-9
+	MOVQ	ptr+0(FP), AX
+	MOVB	val+8(FP), BX
+	LOCK
+	ANDB	BX, (AX)
+	RET
+
 // void jmpdefer(fn, sp);
 // called from deferreturn.
 // 1. pop the caller

commit 9f5d74ba51e450ba0fe0c643dee2a36e1ad0bf24
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 19 22:17:53 2015 -0400

    checkpoint code for event-driven CPU exception handling instead of polling
    
    insert 4 instructions into the go function call prologue to check for
    interrupts and call the interrupt handler if necessary. be careful not to
    clobber the context or TLS registers.
    
    i just realized this approach has a big problem though: what if there are no
    runnable go routines? thus we really need to start/wakeup a goroutine if there
    aren't any runnable goroutines.
    
    starting a goroutine from interrupt context is difficult though (i tried it
    earlier today) since we need to manipulate run queues in interrupt context --
    obviously interrupt code cannot attempt to acquire the mutex protecting the run
    queues.
    
    need to figure something out.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b5b860da17..d3c7602d3c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -606,6 +606,12 @@ hltagain:
 	HLT
 	JMP	hltagain
 
+TEXT runtime·handle_int(SB), NOSPLIT, $0-0
+	PUSHQ	DX
+	CALL	_handle_int(SB)
+	POPQ	DX
+	RET
+
 #define TRAP_TIMER      $32
 TEXT hack_yield(SB), NOSPLIT, $0-0
 	INT	TRAP_TIMER

commit 7e612ff4f61aded306b539a1c42237c99e19a6d0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 18:34:30 2015 -0400

    cycle counting

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 051434f78d..b5b860da17 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -518,6 +518,14 @@ TEXT rrsp(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
+TEXT runtime·Rdtsc(SB), NOSPLIT, $0-8
+	// rdtsc
+	BYTE	$0x0f
+	BYTE	$0x31
+	MOVL	AX, ret+0(FP)
+	MOVL	DX, ret+4(FP)
+	RET
+
 TEXT cli(SB), NOSPLIT, $0-0
 	CLI
 	RET

commit 24e6d973dc265c98e43bc4bd58f80c19fc12b50f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 14:02:28 2015 -0400

    Outw, Rflags

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3c420711c4..051434f78d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -443,6 +443,14 @@ TEXT runtime·Outb(SB), NOSPLIT, $0-16
 	BYTE	$0xee
 	RET
 
+TEXT runtime·Outw(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), DX
+	MOVQ	val+8(FP), AX
+	// outw	%ax, (%dx)
+	BYTE	$0x66
+	BYTE	$0xef
+	RET
+
 TEXT runtime·Outl(SB), NOSPLIT, $0-16
 	MOVQ	reg+0(FP), DX
 	MOVQ	val+8(FP), AX

commit 8b53244a81f4200f4a3044b70f0057e00274c63f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 14 01:04:08 2015 -0400

    don't take printing lock while interrupts are enabled
    
    otherwise can't use printing in interrupt handler

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8b363470bc..3c420711c4 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -500,10 +500,9 @@ TEXT inb(SB), NOSPLIT, $0-16
 	RET
 
 TEXT rflags(SB), NOSPLIT, $0-8
-	// pushf
-	BYTE	$0x9c
+	PUSHFQ
 	POPQ	AX
-	MOVQ	AX, ret+8(FP)
+	MOVQ	AX, ret+0(FP)
 	RET
 
 TEXT rrsp(SB), NOSPLIT, $0-8
@@ -519,6 +518,19 @@ TEXT sti(SB), NOSPLIT, $0-0
 	STI
 	RET
 
+TEXT pushcli(SB), NOSPLIT, $0-8
+	PUSHFQ
+	POPQ	AX
+	MOVQ	AX, ret+0(FP)
+	CLI
+	RET
+
+TEXT popcli(SB), NOSPLIT, $0-8
+	MOVQ	fl+0(FP), AX
+	PUSHQ	AX
+	POPFQ
+	RET
+
 TEXT ·Sgdt(SB), NOSPLIT, $0-8
 	MOVQ	ptr+0(FP), AX
 	// sgdtl (%rax)

commit c9ac55d98df5dda3b9d387cede4ba3925ef96201
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 13 17:38:22 2015 -0400

    PCI config space reads
    
    necessary to clear disk interrupts for PCI-native IDE.
    
    fix panic newlines too.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9f69aa9ee7..8b363470bc 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -442,7 +442,14 @@ TEXT runtime·Outb(SB), NOSPLIT, $0-16
 	// outb	%al, (%dx)
 	BYTE	$0xee
 	RET
-	
+
+TEXT runtime·Outl(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), DX
+	MOVQ	val+8(FP), AX
+	// outl	%eax, (%dx)
+	BYTE	$0xef
+	RET
+
 //void outb(int64 port, int64 val)
 TEXT outb(SB), NOSPLIT, $0-16
 	MOVL	reg+0(FP), DX
@@ -460,6 +467,13 @@ TEXT runtime·Outsl(SB), NOSPLIT, $0-24
 	BYTE	$0x6f
 	RET
 
+TEXT runtime·Inl(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), DX
+	// inl	(%dx), %eax
+	BYTE	$0xed
+	MOVQ	AX, ret+8(FP)
+	RET
+
 TEXT runtime·Insl(SB), NOSPLIT, $0-24
 	MOVQ	reg+0(FP), DX
 	MOVQ	ptr+8(FP), DI

commit d048b0d057860b97eb416153f2c5b9510e1a52db
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 9 09:11:39 2015 -0400

    send 8259a EOI on disk interrupt
    
    biscuit now seems to be fully working on at least one computer.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f629f53dc0..9f69aa9ee7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -436,8 +436,12 @@ TEXT wrmsr(SB), NOSPLIT, $0-16
 	WRMSR
 	RET
 
-TEXT runtime·outb(SB), NOSPLIT, $0-0
-	JMP outb(SB)
+TEXT runtime·Outb(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), DX
+	MOVQ	val+8(FP), AX
+	// outb	%al, (%dx)
+	BYTE	$0xee
+	RET
 	
 //void outb(int64 port, int64 val)
 TEXT outb(SB), NOSPLIT, $0-16

commit 8686c9080ffc0f472c9a672b99c6a7c881bc868a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 7 16:58:34 2015 -0500

    use 2MB pages for direct map if CPU doesn't support 1GB pages
    
    my test hardware sadly does not.
    
    biscuit now boots on real hardware!

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d60576c201..f629f53dc0 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -332,6 +332,18 @@ h_ok:
 	RET
 
 
+TEXT runtime·Cpuid(SB), NOSPLIT, $0-24
+	XORQ	AX, AX
+	XORQ	CX, CX
+	MOVL	eax+0(FP), AX
+	MOVL	ecx+4(FP), CX
+	CPUID
+	MOVL	AX, ret+8(FP)
+	MOVL	BX, ret+12(FP)
+	MOVL	CX, ret+16(FP)
+	MOVL	DX, ret+20(FP)
+	RET
+
 TEXT finit(SB), NOSPLIT, $0-0
 	FINIT
 	RET

commit 3b00197017ad8ec903448fe203202602b0a466df
Author: Shenghou Ma <minux@golang.org>
Date:   Sat Mar 7 00:18:16 2015 -0500

    runtime: add argument sizes for asm functions for bytes, strings
    
    Also fixed a stack corruption bug for nacl/amd64p32.
    
    Change-Id: I64b821b16999c296a159137d971af3870053c621
    Signed-off-by: Shenghou Ma <minux@golang.org>
    Reviewed-on: https://go-review.googlesource.com/7073
    Reviewed-by: Dave Cheney <dave@cheney.net>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 139b5059cb..09fbb51337 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1509,7 +1509,7 @@ allsame:
 	LEAQ	-1(CX)(AX*2), AX	// 1,0,-1 result
 	RET
 
-TEXT bytes·IndexByte(SB),NOSPLIT,$0
+TEXT bytes·IndexByte(SB),NOSPLIT,$0-40
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX
 	MOVB c+24(FP), AL
@@ -1517,7 +1517,7 @@ TEXT bytes·IndexByte(SB),NOSPLIT,$0
 	MOVQ AX, ret+32(FP)
 	RET
 
-TEXT strings·IndexByte(SB),NOSPLIT,$0
+TEXT strings·IndexByte(SB),NOSPLIT,$0-32
 	MOVQ s+0(FP), SI
 	MOVQ s_len+8(FP), BX
 	MOVB c+16(FP), AL

commit 894024f478bff096871e20b5ea7bceb64c70d33f
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Fri Feb 20 20:07:02 2015 +0300

    runtime: fix traceback from goexit1
    
    We used to not call traceback from goexit1.
    But now tracer does it and crashes on amd64p32:
    
    runtime: unexpected return pc for runtime.getg called from 0x108a4240
    goroutine 18 [runnable, locked to thread]:
    runtime.traceGoEnd()
        src/runtime/trace.go:758 fp=0x10818fe0 sp=0x10818fdc
    runtime.goexit1()
        src/runtime/proc1.go:1540 +0x20 fp=0x10818fe8 sp=0x10818fe0
    runtime.getg(0x0)
        src/runtime/asm_386.s:2414 fp=0x10818fec sp=0x10818fe8
    created by runtime/pprof_test.TestTraceStress
        src/runtime/pprof/trace_test.go:123 +0x500
    
    Return PC from goexit1 points right after goexit (+0x6).
    It happens to work most of the time somehow.
    
    This change fixes traceback from goexit1 by adding an additional NOP to goexit.
    
    Fixes #9931
    
    Change-Id: Ied25240a181b0a2d7bc98127b3ed9068e9a1a13e
    Reviewed-on: https://go-review.googlesource.com/5460
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 68bf38464e..139b5059cb 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2457,6 +2457,8 @@ TEXT _cgo_topofstack(SB),NOSPLIT,$0
 TEXT runtime·goexit(SB),NOSPLIT,$0-0
 	BYTE	$0x90	// NOP
 	CALL	runtime·goexit1(SB)	// does not return
+	// traceback from goexit1 must hit code range of goexit
+	BYTE	$0x90	// NOP
 
 TEXT runtime·getg(SB),NOSPLIT,$0-8
 	get_tls(CX)

commit 7abdc90fe372b39946f46884d06f953f4f675316
Author: Matthew Dempsky <mdempsky@google.com>
Date:   Wed Feb 25 17:28:52 2015 +0900

    runtime: remove gogetcallerpc and gogetcallersp functions
    
    Package runtime's Go code was converted to directly call getcallerpc
    and getcallersp in https://golang.org/cl/138740043, but the assembly
    implementations were not removed.
    
    Change-Id: Ib2eaee674d594cbbe799925aae648af782a01c83
    Reviewed-on: https://go-review.googlesource.com/5901
    Run-TryBot: Matthew Dempsky <mdempsky@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d3f88037ff..68bf38464e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -855,12 +855,6 @@ TEXT runtime·getcallerpc(SB),NOSPLIT,$0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-TEXT runtime·gogetcallerpc(SB),NOSPLIT,$0-16
-	MOVQ	p+0(FP),AX		// addr of first arg
-	MOVQ	-8(AX),AX		// get calling pc
-	MOVQ	AX,ret+8(FP)
-	RET
-
 TEXT runtime·setcallerpc(SB),NOSPLIT,$0-16
 	MOVQ	argp+0(FP),AX		// addr of first arg
 	MOVQ	pc+8(FP), BX
@@ -872,12 +866,6 @@ TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-// func gogetcallersp(p unsafe.Pointer) uintptr
-TEXT runtime·gogetcallersp(SB),NOSPLIT,$0-16
-	MOVQ	p+0(FP),AX		// addr of first arg
-	MOVQ	AX, ret+8(FP)
-	RET
-
 // func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	CMPB	runtime·lfenceBeforeRdtsc(SB), $1

commit de50bad12134ad79c897637649cb8b16c4824d1a
Merge: 7cec2157b8 48469a2c86
Author: Russ Cox <rsc@golang.org>
Date:   Mon Feb 23 10:15:35 2015 -0500

    [dev.cc] all: merge master (48469a2) into dev.cc
    
    Change-Id: I10f7950d173b302151f2a31daebce297b4306ebe

commit 6e70fddec0e1d4a43ffb450f555dde82ff313397
Author: Dmitry Vyukov <dvyukov@google.com>
Date:   Tue Feb 17 14:25:49 2015 +0300

    runtime: fix cputicks on x86
    
    See the following issue for context:
    https://github.com/golang/go/issues/9729#issuecomment-74648287
    In short, RDTSC can produce skewed results without preceding LFENCE/MFENCE.
    Information on this matter is very scrappy in the internet.
    But this is what linux kernel does (see rdtsc_barrier).
    It also fixes the test program on my machine.
    
    Update #9729
    
    Change-Id: I3c1ffbf129fdfdd388bd5b7911b392b319248e68
    Reviewed-on: https://go-review.googlesource.com/5033
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f6c1c5f6e6..270fdc1823 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -30,6 +30,19 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	CPUID
 	CMPQ	AX, $0
 	JE	nocpuinfo
+
+	// Figure out how to serialize RDTSC.
+	// On Intel processors LFENCE is enough. AMD requires MFENCE.
+	// Don't know about the rest, so let's do MFENCE.
+	CMPL	BX, $0x756E6547  // "Genu"
+	JNE	notintel
+	CMPL	DX, $0x49656E69  // "ineI"
+	JNE	notintel
+	CMPL	CX, $0x6C65746E  // "ntel"
+	JNE	notintel
+	MOVB	$1, runtime·lfenceBeforeRdtsc(SB)
+notintel:
+
 	MOVQ	$1, AX
 	CPUID
 	MOVL	CX, runtime·cpuid_ecx(SB)
@@ -865,8 +878,15 @@ TEXT runtime·gogetcallersp(SB),NOSPLIT,$0-16
 	MOVQ	AX, ret+8(FP)
 	RET
 
-// int64 runtime·cputicks(void)
+// func cputicks() int64
 TEXT runtime·cputicks(SB),NOSPLIT,$0-0
+	CMPB	runtime·lfenceBeforeRdtsc(SB), $1
+	JNE	mfence
+	BYTE	$0x0f; BYTE $0xae; BYTE $0xe8 // LFENCE
+	JMP	done
+mfence:
+	BYTE	$0x0f; BYTE $0xae; BYTE $0xf0 // MFENCE
+done:
 	RDTSC
 	SHLQ	$32, DX
 	ADDQ	DX, AX

commit c21f1d5ef30ff52cb42fca146a9c7161dfee5c3c
Author: Rob Pike <r@golang.org>
Date:   Thu Feb 19 13:44:06 2015 -0800

    [dev.cc] runtime,syscall: quiet some more vet errors
    
    Fix many incorrect FP references and a few other details.
    
    Some errors remain, especially in vlop, but fixing them requires semantics. For another day.
    
    Change-Id: Ib769fb519b465e79fc08d004a51acc5644e8b259
    Reviewed-on: https://go-review.googlesource.com/5288
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f6c1c5f6e6..1ac4b78a4d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -756,7 +756,7 @@ havem:
 	MOVQ	BX, -8(DI)
 	// Compute the size of the frame, including return PC and, if
 	// GOEXPERIMENT=framepointer, the saved based pointer
-	LEAQ	x+0(FP), AX
+	LEAQ	fv+0(FP), AX
 	SUBQ	SP, AX
 	SUBQ	AX, DI
 	MOVQ	DI, SP
@@ -768,7 +768,7 @@ havem:
 	// Compute the size of the frame again.  FP and SP have
 	// completely different values here than they did above,
 	// but only their difference matters.
-	LEAQ	x+0(FP), AX
+	LEAQ	fv+0(FP), AX
 	SUBQ	SP, AX
 
 	// Restore g->sched (== m->curg->sched) from saved values.

commit 5c0351af5230d7b8b13635ee0402a5359372ba18
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Feb 15 20:47:30 2015 -0500

    ide disk driver and ide server go-routine daemon
    
    kernel code submits requests for ide reads/writes through a channel. its very
    straight-forward! we get kernel thread sleeping/wakeup and safe queue access
    for free via go channels. see ide_test() for example code demonstrating how to
    use the ide daemon.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0ca68693aa..d60576c201 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -435,6 +435,24 @@ TEXT outb(SB), NOSPLIT, $0-16
 	BYTE	$0xee
 	RET
 
+TEXT runtime·Outsl(SB), NOSPLIT, $0-24
+	MOVQ	reg+0(FP), DX
+	MOVQ	ptr+8(FP), SI
+	MOVQ	len+16(FP), CX
+	// repnz outsl (%rsi), (%dx)
+	BYTE	$0xf2
+	BYTE	$0x6f
+	RET
+
+TEXT runtime·Insl(SB), NOSPLIT, $0-24
+	MOVQ	reg+0(FP), DX
+	MOVQ	ptr+8(FP), DI
+	MOVQ	len+16(FP), CX
+	// repnz insl (%dx), (%rdi)
+	BYTE	$0xf2
+	BYTE	$0x6d
+	RET
+
 TEXT runtime·inb(SB), NOSPLIT, $0-0
 	JMP	inb(SB)
 

commit 6cdc5a371a01a03ea35586b020388e0d6536f414
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Feb 13 15:03:36 2015 -0500

    irqs
    
    prep for ata pio driver

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f18661432a..0ca68693aa 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -545,13 +545,12 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	POPQ	AX;			\
 	POPQ	AX;			\
 	RET
+// pops are to silence plan9 warnings
 
-#define IH_NOEC_LOOP(num, fn)		\
+#define IH_IRQ(num, fn)			\
 TEXT fn(SB), NOSPLIT, $0-0;		\
 	PUSHQ	$0;			\
-	PUSHQ	$num;			\
-	BYTE	$0xeb;			\
-	BYTE	$0xfe;			\
+	PUSHQ	$(32 + num);		\
 	JMP	alltraps(SB);		\
 	BYTE	$0xeb;			\
 	BYTE	$0xfe;			\
@@ -590,9 +589,27 @@ IH_NOEC(18,Xmc )
 IH_NOEC(19,Xfp )
 IH_NOEC(20,Xve )
 IH_NOEC(32,Xtimer )
-IH_NOEC(47,Xspur )
+IH_NOEC(48,Xspur )
 IH_NOEC(64,Xsyscall )
 
+// irqs
+// irq0 is Xtimer
+IH_IRQ( 1,Xirq1 )
+IH_IRQ( 2,Xirq2 )
+IH_IRQ( 3,Xirq3 )
+IH_IRQ( 4,Xirq4 )
+IH_IRQ( 5,Xirq5 )
+IH_IRQ( 6,Xirq6 )
+IH_IRQ( 7,Xirq7 )
+IH_IRQ( 8,Xirq8 )
+IH_IRQ( 9,Xirq9 )
+IH_IRQ(10,Xirq10 )
+IH_IRQ(11,Xirq11 )
+IH_IRQ(12,Xirq12 )
+IH_IRQ(13,Xirq13 )
+IH_IRQ(14,Xirq14 )
+IH_IRQ(15,Xirq15 )
+
 #define IA32_FS_BASE   $0xc0000100UL
 
 TEXT wrfsb(SB), NOSPLIT, $0-8

commit b8c90090d3a8ff80e240c92c6b448123083dac8a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 12 11:40:39 2015 -0500

    zero bss correctly in bootloader

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 21120b9428..f18661432a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -166,6 +166,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	// magic loop
 	//BYTE	$0xeb
 	//BYTE	$0xfe
+	CALL	runtime·sc_setup(SB)
 
 	// save page table and first free address from bootloader.
 	MOVL	DI, kpmap(SB)
@@ -290,8 +291,6 @@ h_ok:
 
 	CALL	fpuinit(SB)
 
-	CALL	runtime·sc_setup(SB)
-
 	//MOVQ	CR0, AX
 	//PUSHQ	AX
 	//CALL	exam(SB)

commit fb559b6811b566960600daa383f55a2c534ed3d6
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 10 19:56:10 2015 -0500

    horrible goddamn bug
    
    go uses SSE non-temporal moves for fast copying and zeroing of newly allocated
    objects. if you are very unlucky, a zero-initializing go routine may be context
    switched after a copying go routine. since my scheduler did not save/restore
    FPU/SSE state, the SSE registers used were not saved/restored on a context
    switch.
    
    the end result was newly allocated maps having segments of 128 non-zero bytes
    and copied objects having 128 byte segments that were zeroed.
    
    save restore SSE state, and fix a few other bugs like serial access to mmaps,
    trap queue, and syscalls with locks (i'll remove these coarse locks next).
    
    also do go routine stack checks manually in C stubs -- the stack check inserted
    by the compiler panicks if any C code runs on a go routine stack. this makes me
    nervous because i'm not exactly sure why...
    
    goodbye four days of my life

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8d325d3849..21120b9428 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -508,6 +508,22 @@ TEXT htpause(SB), NOSPLIT, $0-0
 	PAUSE
 	RET
 
+TEXT fxsave(SB), NOSPLIT, $0-8
+	MOVQ	dst+0(FP), AX
+	// fxsave	(%rax)
+	BYTE	$0x0f
+	BYTE	$0xae
+	BYTE	$0x00
+	RET
+
+TEXT fxrstor(SB), NOSPLIT, $0-8
+	MOVQ	dst+0(FP), AX
+	// fxrstor	(%rax)
+	BYTE	$0x0f
+	BYTE	$0xae
+	BYTE	$0x08
+	RET
+
 TEXT cpu_halt(SB), NOSPLIT, $0-8
 	MOVQ	sp+0(FP), SP
 	STI

commit 0b16699f931fa679ec056e0fdf1b6d2c4a2b61d5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 10 19:58:15 2015 -0500

    stack check invlpg

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a4ed9dbce0..8d325d3849 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -367,7 +367,12 @@ TEXT rcr3(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT runtime·Invlpg(SB), NOSPLIT, $0-8
+TEXT runtime·Invlpg(SB), $0-8
+	MOVQ	va+0(FP), AX
+	INVLPG	(AX)
+	RET
+
+TEXT invlpg(SB), NOSPLIT, $0-8
 	MOVQ	va+0(FP), AX
 	INVLPG	(AX)
 	RET

commit 735431be5d22041d282e226ff9367dfd72142c3e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 10 20:13:11 2015 -0500

    yield in usleep instead of spinning

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 168590faf9..a4ed9dbce0 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -142,11 +142,15 @@ TEXT fixcs(SB),NOSPLIT,$0
 
 TEXT runtime·deray(SB),NOSPLIT,$8
 	MOVQ	times+0(FP), CX
+	SHRQ	$10, CX
+	CMPQ	CX, $0
+	JEQ	done
 back:
 	// inb	$0x80, %al
 	BYTE	$0xe4
 	BYTE	$0x80
 	LOOP	back
+done:
 	RET
 
 // i do it this strange way because if i declare fakeargv in C i get 'missing

commit 135ef49fde39c95fe61212376172c6ad333449c0
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Wed Feb 4 17:31:37 2015 -0800

    runtime: speed up eqstring
    
    eqstring does not need to check the length of the strings.
    
    6g
    
    benchmark                              old ns/op     new ns/op     delta
    BenchmarkCompareStringEqual            7.03          6.14          -12.66%
    BenchmarkCompareStringIdentical        3.36          3.04          -9.52%
    
    5g
    
    benchmark                                 old ns/op     new ns/op     delta
    BenchmarkCompareStringEqual               238           232           -2.52%
    BenchmarkCompareStringIdentical           90.8          80.7          -11.12%
    
    The equivalent PPC changes are in a separate commit
    because I don't have the hardware to test them.
    
    Change-Id: I292874324b9bbd9d24f57a390cfff8b550cdd53c
    Reviewed-on: https://go-review.googlesource.com/3955
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f09e5ae250..f6c1c5f6e6 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1262,26 +1262,22 @@ eq:
 	RET
 
 // eqstring tests whether two strings are equal.
+// The compiler guarantees that strings passed
+// to eqstring have equal length.
 // See runtime_test.go:eqstring_generic for
 // equivalent Go code.
 TEXT runtime·eqstring(SB),NOSPLIT,$0-33
-	MOVQ	s1len+8(FP), AX
-	MOVQ	s2len+24(FP), BX
-	CMPQ	AX, BX
-	JNE	noteq
 	MOVQ	s1str+0(FP), SI
 	MOVQ	s2str+16(FP), DI
 	CMPQ	SI, DI
 	JEQ	eq
+	MOVQ	s1len+8(FP), BX
 	CALL	runtime·memeqbody(SB)
 	MOVB	AX, v+32(FP)
 	RET
 eq:
 	MOVB	$1, v+32(FP)
 	RET
-noteq:
-	MOVB	$0, v+32(FP)
-	RET
 
 // a in SI
 // b in DI

commit 4f03c124a56f2eef79bac417c21fb7c55d1fd7d5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 5 15:40:50 2015 -0500

    write syscall

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9aaeadf508..168590faf9 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -358,6 +358,11 @@ TEXT lcr3(SB), NOSPLIT, $0-8
 	MOVQ	AX, CR3
 	RET
 
+TEXT rcr3(SB), NOSPLIT, $0-8
+	MOVQ	CR3, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
 TEXT runtime·Invlpg(SB), NOSPLIT, $0-8
 	MOVQ	va+0(FP), AX
 	INVLPG	(AX)

commit 39dad4bd6d345346414167243e5e091da239aeb8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 4 15:43:46 2015 -0500

    evil bugfixes, fix futex
    
    all bugs were races in the scheduler/trap handling. they were hidden by a slow
    implementation of futex. there were 3 races:
    
    1- since go code is in ring 0, timer interrupts during go code are taken on go
    stacks. consider the following events on CPU1:
            - takes a timer interrupt while executing go process alpha
            - acquires the scheduling lock
            - marks alpha as runnable
            - finds and marks a new process as running
            - releases the lock
    then before CPU 1 has time to iret to the new process and stop using alpha's
    stack, CPU 2 starts running alpha and itself takes a timer interrupt
    overwriting CPU 1's stack.
    
    solve this by using the IST to always take timer interrupts on kernel stacks.
    alternatively, we could release the lock in trapret() once the stack will no
    longer be used, but it seems better to always take traps on the interrupt stack
    anyway.
    
    2- user application page faults. trap handler running on CPU 1 posts exception
    to go kernel, which is running on CPU 2, and go kernel immediately responds by
    terminating the application, reclaiming the page tables while CPU 1 is using
    them.
    
    solve this by changing to the kernel's page tables whenever posting a trap that
    may result in application termination.
    
    3- when a process on CPU 1 is trying to go to sleep for a small amount of time
    and marks itself sleeping, another CPU wakes and starts running the process
    before CPU 1 has yielded and stored the process' state.
    
    solve this by marking a process as sleeping in the timer interrupt, not
    futex(). thus a CPU trying to wake processes up will not mark the process as
    runnable yet.
    
    in conclusion, i am desperately trying to simplify the design!

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index c53752741e..9aaeadf508 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -483,6 +483,13 @@ TEXT gtr(SB), NOSPLIT, $0-8
 	MOVQ	AX, ret+0(FP)
 	RET
 
+TEXT getret(SB), NOSPLIT, $0-16
+	MOVQ	ptr+0(FP), AX
+	ADDQ	$-8, AX
+	MOVQ	(AX), AX
+	MOVQ	AX, ret+8(FP)
+	RET
+
 TEXT htpause(SB), NOSPLIT, $0-0
 	PAUSE
 	RET

commit 52007cc98cb8963c1f63ef69cd35112d77b9daf3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 2 19:04:18 2015 -0500

    parallel runtime scheduler
    
    there were many bugs. discovered a bug in the plan9 C compiler and the plan9
    assembler on the way.
    
    now kernel parallelism should be easy. scheduling code desperately needs
    cleanup; will do this soon.
    
    hack_futex has been band-aid fixed -- need to fix it properly later.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ebf82c6e8c..c53752741e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -284,11 +284,6 @@ h_ok:
 	CALL	int_setup(SB)
 	CALL	proc_setup(SB)
 
-	FINIT
-	MOVQ	CR4, AX
-	PUSHQ	AX
-	MOVQ	CR0, AX
-	PUSHQ	AX
 	CALL	fpuinit(SB)
 
 	CALL	runtime·sc_setup(SB)
@@ -334,11 +329,25 @@ h_ok:
 	RET
 
 
+TEXT finit(SB), NOSPLIT, $0-0
+	FINIT
+	RET
+
+TEXT rcr0(SB), NOSPLIT, $0-8
+	MOVQ	CR0, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
 TEXT rcr2(SB), NOSPLIT, $0-8
 	MOVQ	CR2, AX
 	MOVQ	AX, ret+0(FP)
 	RET
 
+TEXT rcr4(SB), NOSPLIT, $0-8
+	MOVQ	CR4, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
 TEXT tlbflush(SB), NOSPLIT, $0-0
 	MOVQ	CR3, AX
 	MOVQ	AX, CR3
@@ -465,6 +474,26 @@ TEXT ·Sidt(SB), NOSPLIT, $0-8
 	BYTE	$0x08
 	RET
 
+TEXT gtr(SB), NOSPLIT, $0-8
+	// str	%rax
+	BYTE $0x48
+	BYTE $0x0f
+	BYTE $0x00
+	BYTE $0xc8
+	MOVQ	AX, ret+0(FP)
+	RET
+
+TEXT htpause(SB), NOSPLIT, $0-0
+	PAUSE
+	RET
+
+TEXT cpu_halt(SB), NOSPLIT, $0-8
+	MOVQ	sp+0(FP), SP
+	STI
+hltagain:
+	HLT
+	JMP	hltagain
+
 #define TRAP_TIMER      $32
 TEXT hack_yield(SB), NOSPLIT, $0-0
 	INT	TRAP_TIMER
@@ -615,9 +644,6 @@ TEXT trapret(SB), NOSPLIT, $0-16
 	// iretq
 	BYTE	$0x48
 	BYTE	$0xcf
-	// jmp self
-	BYTE	$0xeb
-	BYTE	$0xfe
 
 /*
  *  go-routine

commit 3c0fee10dbe82771dcaa956a95bdfabdced5fff7
Author: Austin Clements <austin@google.com>
Date:   Wed Jan 14 11:09:50 2015 -0500

    cmd/6g, liblink, runtime: support saving base pointers
    
    This adds a "framepointer" GOEXPERIMENT that that makes the amd64
    toolchain maintain base pointer chains in the same way that gcc
    -fno-omit-frame-pointer does.  Go doesn't use these saved base
    pointers, but this does enable external tools like Linux perf and
    VTune to unwind Go stacks when collecting system-wide profiles.
    
    This requires support in the compilers to not clobber BP, support in
    liblink for generating the BP-saving function prologue and unwinding
    epilogue, and support in the runtime to save BPs across preemption, to
    skip saved BPs during stack unwinding and, and to adjust saved BPs
    during stack moving.
    
    As with other GOEXPERIMENTs, everything from the toolchain to the
    runtime must be compiled with this experiment enabled.  To do this,
    run make.bash (or all.bash) with GOEXPERIMENT=framepointer.
    
    Change-Id: I4024853beefb9539949e5ca381adfdd9cfada544
    Reviewed-on: https://go-review.googlesource.com/2992
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b1bf4ca987..f09e5ae250 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -134,6 +134,7 @@ TEXT runtime·gosave(SB), NOSPLIT, $0-8
 	MOVQ	BX, gobuf_pc(AX)
 	MOVQ	$0, gobuf_ret(AX)
 	MOVQ	$0, gobuf_ctxt(AX)
+	MOVQ	BP, gobuf_bp(AX)
 	get_tls(CX)
 	MOVQ	g(CX), BX
 	MOVQ	BX, gobuf_g(AX)
@@ -150,9 +151,11 @@ TEXT runtime·gogo(SB), NOSPLIT, $0-8
 	MOVQ	gobuf_sp(BX), SP	// restore SP
 	MOVQ	gobuf_ret(BX), AX
 	MOVQ	gobuf_ctxt(BX), DX
+	MOVQ	gobuf_bp(BX), BP
 	MOVQ	$0, gobuf_sp(BX)	// clear to help garbage collector
 	MOVQ	$0, gobuf_ret(BX)
 	MOVQ	$0, gobuf_ctxt(BX)
+	MOVQ	$0, gobuf_bp(BX)
 	MOVQ	gobuf_pc(BX), BX
 	JMP	BX
 
@@ -170,6 +173,7 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 	LEAQ	fn+0(FP), BX	// caller's SP
 	MOVQ	BX, (g_sched+gobuf_sp)(AX)
 	MOVQ	AX, (g_sched+gobuf_g)(AX)
+	MOVQ	BP, (g_sched+gobuf_bp)(AX)
 
 	// switch to m->g0 & its stack, call fn
 	MOVQ	g(CX), BX
@@ -228,6 +232,7 @@ switch:
 	MOVQ	SI, (g_sched+gobuf_pc)(AX)
 	MOVQ	SP, (g_sched+gobuf_sp)(AX)
 	MOVQ	AX, (g_sched+gobuf_g)(AX)
+	MOVQ	BP, (g_sched+gobuf_bp)(AX)
 
 	// switch to g0
 	MOVQ	DX, g(CX)
@@ -303,6 +308,7 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	LEAQ	8(SP), AX // f's SP
 	MOVQ	AX, (g_sched+gobuf_sp)(SI)
 	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)
+	MOVQ	BP, (g_sched+gobuf_bp)(SI)
 
 	// Call newstack on m->g0's stack.
 	MOVQ	m_g0(BX), BX
@@ -592,6 +598,7 @@ TEXT gosave<>(SB),NOSPLIT,$0
 	MOVQ	R9, (g_sched+gobuf_sp)(R8)
 	MOVQ	$0, (g_sched+gobuf_ret)(R8)
 	MOVQ	$0, (g_sched+gobuf_ctxt)(R8)
+	MOVQ	BP, (g_sched+gobuf_bp)(R8)
 	RET
 
 // asmcgocall(void(*fn)(void*), void *arg)
@@ -747,17 +754,30 @@ havem:
 	MOVQ	(g_sched+gobuf_sp)(SI), DI  // prepare stack as DI
 	MOVQ	(g_sched+gobuf_pc)(SI), BX
 	MOVQ	BX, -8(DI)
-	LEAQ	-(8+8)(DI), SP
+	// Compute the size of the frame, including return PC and, if
+	// GOEXPERIMENT=framepointer, the saved based pointer
+	LEAQ	x+0(FP), AX
+	SUBQ	SP, AX
+	SUBQ	AX, DI
+	MOVQ	DI, SP
+
 	MOVQ	R8, 0(SP)
 	CALL	runtime·cgocallbackg(SB)
 	MOVQ	0(SP), R8
 
+	// Compute the size of the frame again.  FP and SP have
+	// completely different values here than they did above,
+	// but only their difference matters.
+	LEAQ	x+0(FP), AX
+	SUBQ	SP, AX
+
 	// Restore g->sched (== m->curg->sched) from saved values.
 	get_tls(CX)
 	MOVQ	g(CX), SI
-	MOVQ	8(SP), BX
+	MOVQ	SP, DI
+	ADDQ	AX, DI
+	MOVQ	-8(DI), BX
 	MOVQ	BX, (g_sched+gobuf_pc)(SI)
-	LEAQ	(8+8)(SP), DI
 	MOVQ	DI, (g_sched+gobuf_sp)(SI)
 
 	// Switch back to m->g0's stack and restore m->g0->sched.sp.

commit 20a6ff7261adecc1ba0dc3f3cd6a29054fdf90b7
Author: Austin Clements <austin@google.com>
Date:   Tue Jan 27 18:29:02 2015 -0500

    runtime: eliminate uses of BP on amd64
    
    Any place that clobbers BP in the runtime can potentially interfere
    with frame pointer unwinding with GOEXPERIMENT=framepointer.  This
    change eliminates uses of BP in the runtime to address this problem.
    We have spare registers everywhere this occurs, so there's no downside
    to eliminating BP.  Where possible, this uses the same new register as
    the amd64p32 runtime, which doesn't use BP due to restrictions placed
    on it by NaCL.
    
    One nice side effect of this is that it will let perf/VTune unwind the
    call stack even through a call to systemstack, which will let us get
    really good call graphs from the garbage collector.
    
    Change-Id: I0ffa14cb4dd2b613a7049b8ec59df37c52286212
    Reviewed-on: https://go-review.googlesource.com/3390
    Reviewed-by: Minux Ma <minux@golang.org>
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8547228ee3..b1bf4ca987 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -96,8 +96,8 @@ ok:
 	CALL	runtime·schedinit(SB)
 
 	// create a new goroutine to start program
-	MOVQ	$runtime·main·f(SB), BP		// entry
-	PUSHQ	BP
+	MOVQ	$runtime·main·f(SB), AX		// entry
+	PUSHQ	AX
 	PUSHQ	$0			// arg size
 	CALL	runtime·newproc(SB)
 	POPQ	AX
@@ -213,8 +213,8 @@ TEXT runtime·systemstack(SB), NOSPLIT, $0-8
 	CMPQ	AX, DX
 	JEQ	noswitch
 
-	MOVQ	m_curg(BX), BP
-	CMPQ	AX, BP
+	MOVQ	m_curg(BX), R8
+	CMPQ	AX, R8
 	JEQ	switch
 	
 	// Bad: g is not gsignal, not g0, not curg. What is it?
@@ -224,8 +224,8 @@ TEXT runtime·systemstack(SB), NOSPLIT, $0-8
 switch:
 	// save our state in g->sched.  Pretend to
 	// be systemstack_switch if the G stack is scanned.
-	MOVQ	$runtime·systemstack_switch(SB), BP
-	MOVQ	BP, (g_sched+gobuf_pc)(AX)
+	MOVQ	$runtime·systemstack_switch(SB), SI
+	MOVQ	SI, (g_sched+gobuf_pc)(AX)
 	MOVQ	SP, (g_sched+gobuf_sp)(AX)
 	MOVQ	AX, (g_sched+gobuf_g)(AX)
 
@@ -305,9 +305,9 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)
 
 	// Call newstack on m->g0's stack.
-	MOVQ	m_g0(BX), BP
-	MOVQ	BP, g(CX)
-	MOVQ	(g_sched+gobuf_sp)(BP), SP
+	MOVQ	m_g0(BX), BX
+	MOVQ	BX, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(BX), SP
 	CALL	runtime·newstack(SB)
 	MOVQ	$0, 0x1003	// crash if newstack returns
 	RET
@@ -619,17 +619,17 @@ TEXT asmcgocall<>(SB),NOSPLIT,$0-0
 	// We get called to create new OS threads too, and those
 	// come in on the m->g0 stack already.
 	get_tls(CX)
-	MOVQ	g(CX), BP
-	MOVQ	g_m(BP), BP
-	MOVQ	m_g0(BP), SI
+	MOVQ	g(CX), R8
+	MOVQ	g_m(R8), R8
+	MOVQ	m_g0(R8), SI
 	MOVQ	g(CX), DI
 	CMPQ	SI, DI
 	JEQ	nosave
-	MOVQ	m_gsignal(BP), SI
+	MOVQ	m_gsignal(R8), SI
 	CMPQ	SI, DI
 	JEQ	nosave
 	
-	MOVQ	m_g0(BP), SI
+	MOVQ	m_g0(R8), SI
 	CALL	gosave<>(SB)
 	MOVQ	SI, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(SI), SP
@@ -683,15 +683,15 @@ TEXT ·cgocallback_gofunc(SB),NOSPLIT,$8-24
 	// the linker analysis by using an indirect call through AX.
 	get_tls(CX)
 #ifdef GOOS_windows
-	MOVL	$0, BP
+	MOVL	$0, BX
 	CMPQ	CX, $0
 	JEQ	2(PC)
 #endif
-	MOVQ	g(CX), BP
-	CMPQ	BP, $0
+	MOVQ	g(CX), BX
+	CMPQ	BX, $0
 	JEQ	needm
-	MOVQ	g_m(BP), BP
-	MOVQ	BP, R8 // holds oldm until end of function
+	MOVQ	g_m(BX), BX
+	MOVQ	BX, R8 // holds oldm until end of function
 	JMP	havem
 needm:
 	MOVQ	$0, 0(SP)
@@ -699,8 +699,8 @@ needm:
 	CALL	AX
 	MOVQ	0(SP), R8
 	get_tls(CX)
-	MOVQ	g(CX), BP
-	MOVQ	g_m(BP), BP
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
 	
 	// Set m->sched.sp = SP, so that if a panic happens
 	// during the function we are about to execute, it will
@@ -713,7 +713,7 @@ needm:
 	// and then systemstack will try to use it. If we don't set it here,
 	// that restored SP will be uninitialized (typically 0) and
 	// will not be usable.
-	MOVQ	m_g0(BP), SI
+	MOVQ	m_g0(BX), SI
 	MOVQ	SP, (g_sched+gobuf_sp)(SI)
 
 havem:
@@ -722,7 +722,7 @@ havem:
 	// Save current sp in m->g0->sched.sp in preparation for
 	// switch back to m->curg stack.
 	// NOTE: unwindm knows that the saved g->sched.sp is at 0(SP).
-	MOVQ	m_g0(BP), SI
+	MOVQ	m_g0(BX), SI
 	MOVQ	(g_sched+gobuf_sp)(SI), AX
 	MOVQ	AX, 0(SP)
 	MOVQ	SP, (g_sched+gobuf_sp)(SI)
@@ -742,11 +742,11 @@ havem:
 	// the earlier calls.
 	//
 	// In the new goroutine, 0(SP) holds the saved R8.
-	MOVQ	m_curg(BP), SI
+	MOVQ	m_curg(BX), SI
 	MOVQ	SI, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(SI), DI  // prepare stack as DI
-	MOVQ	(g_sched+gobuf_pc)(SI), BP
-	MOVQ	BP, -8(DI)
+	MOVQ	(g_sched+gobuf_pc)(SI), BX
+	MOVQ	BX, -8(DI)
 	LEAQ	-(8+8)(DI), SP
 	MOVQ	R8, 0(SP)
 	CALL	runtime·cgocallbackg(SB)
@@ -755,17 +755,17 @@ havem:
 	// Restore g->sched (== m->curg->sched) from saved values.
 	get_tls(CX)
 	MOVQ	g(CX), SI
-	MOVQ	8(SP), BP
-	MOVQ	BP, (g_sched+gobuf_pc)(SI)
+	MOVQ	8(SP), BX
+	MOVQ	BX, (g_sched+gobuf_pc)(SI)
 	LEAQ	(8+8)(SP), DI
 	MOVQ	DI, (g_sched+gobuf_sp)(SI)
 
 	// Switch back to m->g0's stack and restore m->g0->sched.sp.
 	// (Unlike m->curg, the g0 goroutine never uses sched.pc,
 	// so we do not have to restore it.)
-	MOVQ	g(CX), BP
-	MOVQ	g_m(BP), BP
-	MOVQ	m_g0(BP), SI
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVQ	m_g0(BX), SI
 	MOVQ	SI, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(SI), SP
 	MOVQ	0(SP), AX
@@ -915,8 +915,8 @@ aes0to15:
 	// a page boundary, so we can load it directly.
 	MOVOU	-16(AX), X0
 	ADDQ	CX, CX
-	MOVQ	$masks<>(SB), BP
-	PAND	(BP)(CX*8), X0
+	MOVQ	$masks<>(SB), AX
+	PAND	(AX)(CX*8), X0
 
 	// scramble 3 times
 	AESENC	X6, X0
@@ -931,8 +931,8 @@ endofpage:
 	// Then shift bytes down using pshufb.
 	MOVOU	-32(AX)(CX*1), X0
 	ADDQ	CX, CX
-	MOVQ	$shifts<>(SB), BP
-	PSHUFB	(BP)(CX*8), X0
+	MOVQ	$shifts<>(SB), AX
+	PSHUFB	(AX)(CX*8), X0
 	AESENC	X6, X0
 	AESENC	X7, X0
 	AESENC	X7, X0
@@ -1384,13 +1384,13 @@ TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
 	CMPQ	SI, DI
 	JEQ	allsame
 	CMPQ	BX, DX
-	MOVQ	DX, BP
-	CMOVQLT	BX, BP // BP = min(alen, blen) = # of bytes to compare
-	CMPQ	BP, $8
+	MOVQ	DX, R8
+	CMOVQLT	BX, R8 // R8 = min(alen, blen) = # of bytes to compare
+	CMPQ	R8, $8
 	JB	small
 
 loop:
-	CMPQ	BP, $16
+	CMPQ	R8, $16
 	JBE	_0through16
 	MOVOU	(SI), X0
 	MOVOU	(DI), X1
@@ -1400,7 +1400,7 @@ loop:
 	JNE	diff16	// branch if at least one byte is not equal
 	ADDQ	$16, SI
 	ADDQ	$16, DI
-	SUBQ	$16, BP
+	SUBQ	$16, R8
 	JMP	loop
 	
 	// AX = bit mask of differences
@@ -1415,15 +1415,15 @@ diff16:
 
 	// 0 through 16 bytes left, alen>=8, blen>=8
 _0through16:
-	CMPQ	BP, $8
+	CMPQ	R8, $8
 	JBE	_0through8
 	MOVQ	(SI), AX
 	MOVQ	(DI), CX
 	CMPQ	AX, CX
 	JNE	diff8
 _0through8:
-	MOVQ	-8(SI)(BP*1), AX
-	MOVQ	-8(DI)(BP*1), CX
+	MOVQ	-8(SI)(R8*1), AX
+	MOVQ	-8(DI)(R8*1), CX
 	CMPQ	AX, CX
 	JEQ	allsame
 
@@ -1440,7 +1440,7 @@ diff8:
 
 	// 0-7 bytes in common
 small:
-	LEAQ	(BP*8), CX	// bytes left -> bits left
+	LEAQ	(R8*8), CX	// bytes left -> bits left
 	NEGQ	CX		//  - bits lift (== 64 - bits left mod 64)
 	JEQ	allsame
 
@@ -1450,7 +1450,7 @@ small:
 	MOVQ	(SI), SI
 	JMP	si_finish
 si_high:
-	MOVQ	-8(SI)(BP*1), SI
+	MOVQ	-8(SI)(R8*1), SI
 	SHRQ	CX, SI
 si_finish:
 	SHLQ	CX, SI
@@ -1461,7 +1461,7 @@ si_finish:
 	MOVQ	(DI), DI
 	JMP	di_finish
 di_high:
-	MOVQ	-8(DI)(BP*1), DI
+	MOVQ	-8(DI)(R8*1), DI
 	SHRQ	CX, DI
 di_finish:
 	SHLQ	CX, DI

commit ba56baba55a83523ba96b67faa97578208fc3db4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Feb 1 15:44:08 2015 -0500

    serial console driver
    
    now i can actually debug biscuit over slow connection remotely.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b659b01f65..ebf82c6e8c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -192,15 +192,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	MOVL	DX, runtime·cpuid_edx(SB)
 h_nocpuinfo:
 
-	CALL	cls(SB)
-	//PUSHQ	$0x20
-	//CALL	runtime·doc(SB)
-	//POPQ	AX
-
-	//PUSHQ	$0x39
-	//CALL	runtime·doc(SB)
-	//CALL	runtime·doc(SB)
-	//POPQ	AX
+	CALL	runtime·cls(SB)
 
 	// if there is an _cgo_init, call it.
 	//MOVQ	_cgo_init(SB), AX
@@ -298,8 +290,8 @@ h_ok:
 	MOVQ	CR0, AX
 	PUSHQ	AX
 	CALL	fpuinit(SB)
-	POPQ	AX
-	POPQ	AX
+
+	CALL	runtime·sc_setup(SB)
 
 	//MOVQ	CR0, AX
 	//PUSHQ	AX
@@ -410,14 +402,33 @@ TEXT wrmsr(SB), NOSPLIT, $0-16
 	WRMSR
 	RET
 
-//void outb(int32 addr, int32 val)
-TEXT outb(SB), NOSPLIT, $0-8
+TEXT runtime·outb(SB), NOSPLIT, $0-0
+	JMP outb(SB)
+	
+//void outb(int64 port, int64 val)
+TEXT outb(SB), NOSPLIT, $0-16
 	MOVL	reg+0(FP), DX
-	MOVL	val+4(FP), AX
+	MOVL	val+8(FP), AX
 	// outb	%al, (%dx)
 	BYTE	$0xee
 	RET
 
+TEXT runtime·inb(SB), NOSPLIT, $0-0
+	JMP	inb(SB)
+
+//int64 inb(int64 port)
+TEXT inb(SB), NOSPLIT, $0-16
+	MOVL	reg+0(FP), DX
+	// inb	(%dx), %al
+	BYTE	$0xec
+	// movzbq %al, %rax
+	BYTE $0x48
+	BYTE $0x0f
+	BYTE $0xb6
+	BYTE $0xc0
+	MOVQ	AX, ret+8(FP)
+	RET
+
 TEXT rflags(SB), NOSPLIT, $0-8
 	// pushf
 	BYTE	$0x9c

commit 3af60b7bc16a62b07aaefc4089da7c5d65ef1d30
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 30 15:32:01 2015 -0500

    initialize lapic/TSS on APs
    
    shuffle stuff around so APs can reuse init code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6f8f85d75c..b659b01f65 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -289,8 +289,8 @@ h_ok:
 	// save m0 to g0->m
 	MOVQ	AX, g_m(CX)
 
-	CALL	intsetup(SB)
-	CALL	timersetup(SB)
+	CALL	int_setup(SB)
+	CALL	proc_setup(SB)
 
 	FINIT
 	MOVQ	CR4, AX
@@ -309,8 +309,6 @@ h_ok:
 	//CALL	pgtest(SB)
 	//CALL	mmap_test(SB)
 
-	CALL	misc_init(SB)
-
 	CLD				// convention is D is always left cleared
 	CALL	runtime·check(SB)
 

commit 4fcb3dc4cfefff837f3bd453d988314e104d91f8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 30 12:36:59 2015 -0500

    setup AP stacks
    
    enter the terrible world of multicore

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index f321a1c85c..6f8f85d75c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -571,7 +571,8 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 
 TEXT trapret(SB), NOSPLIT, $0-16
 	MOVQ	pmap+8(FP), BX
-	MOVQ	fp+0(FP), AX
+	MOVQ	tf+0(FP), AX	// tf is not on the callers stack frame, but in
+				// threads[]
 	MOVQ	AX, SP
 
 	MOVQ	BX, CR3

commit af4dfcf8673af71c3c914043debc982c6a5a0390
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 29 18:15:00 2015 -0500

    SMP
    
    find and initialize cpus. the test boots up all found cpus (controllable with
    CPUS env variable) each of which print a hello message, spin for a moment, and
    then fault to make sure the IDT/GDT are properly setup.
    
    there is no synchronization yet. next step is to make the runtime/kernel
    thread-safe.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ff539c9916..f321a1c85c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -412,6 +412,7 @@ TEXT wrmsr(SB), NOSPLIT, $0-16
 	WRMSR
 	RET
 
+//void outb(int32 addr, int32 val)
 TEXT outb(SB), NOSPLIT, $0-8
 	MOVL	reg+0(FP), DX
 	MOVL	val+4(FP), AX
@@ -439,6 +440,22 @@ TEXT sti(SB), NOSPLIT, $0-0
 	STI
 	RET
 
+TEXT ·Sgdt(SB), NOSPLIT, $0-8
+	MOVQ	ptr+0(FP), AX
+	// sgdtl (%rax)
+	BYTE	$0x0f
+	BYTE	$0x01
+	BYTE	$0x00
+	RET
+
+TEXT ·Sidt(SB), NOSPLIT, $0-8
+	MOVQ	ptr+0(FP), AX
+	// sidtl (%rax)
+	BYTE	$0x0f
+	BYTE	$0x01
+	BYTE	$0x08
+	RET
+
 #define TRAP_TIMER      $32
 TEXT hack_yield(SB), NOSPLIT, $0-0
 	INT	TRAP_TIMER

commit d14a07795fbfeb289f67a0ff02b0eb4eca9c3f40
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Jan 28 11:07:33 2015 -0500

    remove Death()

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9c83f389bf..ff539c9916 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -504,10 +504,6 @@ IH_NOEC(64,Xsyscall )
 
 #define IA32_FS_BASE   $0xc0000100UL
 
-TEXT runtime·Death(SB), NOSPLIT, $0-0
-	INT	$64
-	RET
-
 TEXT wrfsb(SB), NOSPLIT, $0-8
 	get_tls(BX)
 	MOVQ	val+0(FP), AX

commit a5a9d469f5e63b904b81d81754933b5912fea173
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 23 23:30:39 2015 -0500

    elf user programs
    
    any elf object in user/ will be inserted into the kernel as an exec'able
    binary.
    
    it would be nice to strictly separate address ranges of kernel and user
    programs.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0a19a672e7..9c83f389bf 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -359,7 +359,7 @@ TEXT lcr3(SB), NOSPLIT, $0-8
 	MOVQ	AX, CR3
 	RET
 
-TEXT invlpg(SB), NOSPLIT, $0-8
+TEXT runtime·Invlpg(SB), NOSPLIT, $0-8
 	MOVQ	va+0(FP), AX
 	INVLPG	(AX)
 	RET

commit 311b76b148c5828841f322a8780f849fef719bf1
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 23 19:44:31 2015 -0500

    small bugs, debug code

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index ee83559649..0a19a672e7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -175,6 +175,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
 	LEAQ	(-64*1024+104)(SP), BX
+	//LEAQ	(-4*1024+104)(SP), BX
 	MOVQ	BX, g_stackguard0(DI)
 	MOVQ	BX, g_stackguard1(DI)
 	MOVQ	BX, (g_stack+stack_lo)(DI)
@@ -454,6 +455,19 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	POPQ	AX;			\
 	RET
 
+#define IH_NOEC_LOOP(num, fn)		\
+TEXT fn(SB), NOSPLIT, $0-0;		\
+	PUSHQ	$0;			\
+	PUSHQ	$num;			\
+	BYTE	$0xeb;			\
+	BYTE	$0xfe;			\
+	JMP	alltraps(SB);		\
+	BYTE	$0xeb;			\
+	BYTE	$0xfe;			\
+	POPQ	AX;			\
+	POPQ	AX;			\
+	RET
+
 #define IH_EC(num, fn)			\
 TEXT fn(SB), NOSPLIT, $0-0;		\
 	PUSHQ	$num;			\

commit 814f7e20607c23ef98b7d5906fbde56db9efc9d5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 23 19:33:35 2015 -0500

    fix evil non-deterministic bug
    
    because the go kernel runs in ring 0 and grows its own stack, the stack while
    handling a timer interrupt may not be mapped in every process. thus, during a
    context switch, the ret instruction in the lcr3 function faults. because the
    stack is no longer valid, a triple-fault/reboot results.
    
    one fix is to inline lcr3. that way the stack of the interrupted task is not
    used after the new page map is loaded.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 45891157b7..ee83559649 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -542,10 +542,13 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	BYTE	$0xeb
 	BYTE	$0xfe
 
-TEXT trapret(SB), NOSPLIT, $0-8
+TEXT trapret(SB), NOSPLIT, $0-16
+	MOVQ	pmap+8(FP), BX
 	MOVQ	fp+0(FP), AX
 	MOVQ	AX, SP
 
+	MOVQ	BX, CR3
+
 	// restore fsbase
 	MOVQ	IA32_FS_BASE, CX
 	POPQ	AX

commit 357e037f6e8c9f2487c39fce4d6beb897932284a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 15:26:20 2015 -0500

    always switch page maps during context switch

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4ee976feef..45891157b7 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -164,7 +164,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	//BYTE	$0xfe
 
 	// save page table and first free address from bootloader.
-	MOVL	DI, kpgtbl(SB)
+	MOVL	DI, kpmap(SB)
 	MOVL	SI, runtime·Pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 

commit eee205a1aa4dbd4e9080398be06a0384ee10f8f3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 12:50:12 2015 -0500

    finish test user program
    
    uses its own page tables now.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cc5ff2d658..4ee976feef 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -490,7 +490,7 @@ IH_NOEC(64,Xsyscall )
 
 #define IA32_FS_BASE   $0xc0000100UL
 
-TEXT death(SB), NOSPLIT, $0-0
+TEXT runtime·Death(SB), NOSPLIT, $0-0
 	INT	$64
 	RET
 
@@ -542,7 +542,7 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	BYTE	$0xeb
 	BYTE	$0xfe
 
-TEXT runtime·Trapret(SB), NOSPLIT, $0-8
+TEXT trapret(SB), NOSPLIT, $0-8
 	MOVQ	fp+0(FP), AX
 	MOVQ	AX, SP
 

commit 50ecbbdbd584d578e325aa6bc8fcbf6ec6ee2cc1
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Jan 21 16:31:09 2015 -0500

    go virtual memory management
    
    this will be useful for user program setup.  cleanup types too.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 286103e78a..cc5ff2d658 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -165,7 +165,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 
 	// save page table and first free address from bootloader.
 	MOVL	DI, kpgtbl(SB)
-	MOVL	SI, first_free(SB)
+	MOVL	SI, runtime·Pgfirst(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 
 	ANDQ	$~15, SP
@@ -308,11 +308,8 @@ h_ok:
 	//CALL	pgtest(SB)
 	//CALL	mmap_test(SB)
 
-//	CMPQ	AX, $31337
-//	JZ	forward
-//me:
-//	JMP	me
-//forward:
+	CALL	misc_init(SB)
+
 	CLD				// convention is D is always left cleared
 	CALL	runtime·check(SB)
 

commit 373f222385a59c8e30b980c817d63c94712f40a9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 19:20:50 2015 -0500

    put user programs in ring 3
    
    the test user program will crash unless the page tables are marked PTE_U.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4f30f0775e..286103e78a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -164,7 +164,7 @@ TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 	//BYTE	$0xfe
 
 	// save page table and first free address from bootloader.
-	MOVL	DI, pgtbl(SB)
+	MOVL	DI, kpgtbl(SB)
 	MOVL	SI, first_free(SB)
 	MOVQ	$1, runtime·hackmode(SB)
 
@@ -356,6 +356,11 @@ TEXT tlbflush(SB), NOSPLIT, $0-0
 	MOVQ	AX, CR3
 	RET
 
+TEXT lcr3(SB), NOSPLIT, $0-8
+	MOVQ	pgtbl+0(FP), AX
+	MOVQ	AX, CR3
+	RET
+
 TEXT invlpg(SB), NOSPLIT, $0-8
 	MOVQ	va+0(FP), AX
 	INVLPG	(AX)

commit a9476ab6c941c56ad37dc2c46c9662c320b76767
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 11:31:42 2015 -0500

    new trap/scheduling design and syscall stub

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2d41812934..4f30f0775e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -459,7 +459,6 @@ TEXT fn(SB), NOSPLIT, $0-0;		\
 	BYTE	$0xeb;			\
 	BYTE	$0xfe;			\
 	POPQ	AX;			\
-	POPQ	AX;			\
 	RET
 
 IH_NOEC( 0,Xdz )
@@ -485,9 +484,14 @@ IH_NOEC(19,Xfp )
 IH_NOEC(20,Xve )
 IH_NOEC(32,Xtimer )
 IH_NOEC(47,Xspur )
+IH_NOEC(64,Xsyscall )
 
 #define IA32_FS_BASE   $0xc0000100UL
 
+TEXT death(SB), NOSPLIT, $0-0
+	INT	$64
+	RET
+
 TEXT wrfsb(SB), NOSPLIT, $0-8
 	get_tls(BX)
 	MOVQ	val+0(FP), AX

commit fd4dc91a96518fdbb47781f97ba43ae36df215a5
Author: Russ Cox <rsc@golang.org>
Date:   Sun Jan 18 12:50:22 2015 -0500

    strings: remove overengineered Compare implementation
    
    The function is here ONLY for symmetry with package bytes.
    This function should be used ONLY if it makes code clearer.
    It is not here for performance. Remove any performance benefit.
    
    If performance becomes an issue, the compiler should be fixed to
    recognize the three-way compare (for all comparable types)
    rather than encourage people to micro-optimize by using this function.
    
    Change-Id: I71f4130bce853f7aef724c6044d15def7987b457
    Reviewed-on: https://go-review.googlesource.com/3012
    Reviewed-by: Rob Pike <r@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2f9d520d44..8547228ee3 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1364,9 +1364,6 @@ TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
 	MOVQ	AX, ret+32(FP)
 	RET
 
-TEXT strings·Compare(SB),NOSPLIT,$0
-        JMP	runtime·cmpstring(SB)
-
 TEXT bytes·Compare(SB),NOSPLIT,$0-56
 	MOVQ	s1+0(FP), SI
 	MOVQ	s1+8(FP), BX

commit e0ddec7ec7b0dfbe52f855f75b7d878d4fa6eb32
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 15 18:11:25 2015 -0500

    futex
    
    go uses futexs to sleep

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 056cdda5d4..2d41812934 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -531,14 +531,6 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	MOVQ	SP, AX
 	PUSHQ	AX
 
-	MOVQ	newtrap(SB), AX
-	TESTQ	AX, AX
-	JZ	no_new
-	CALL	AX
-	// jmp self
-	BYTE	$0xeb
-	BYTE	$0xfe
-no_new:
 	CALL	trap(SB)
 	// jmp self
 	BYTE	$0xeb

commit 90ce1936e32f46de4c64752dfc54df75ec5758f0
Author: Alan Donovan <adonovan@google.com>
Date:   Wed Jan 14 18:09:36 2015 -0500

    strings: add Compare(x, y string) int, for symmetry with bytes.Compare
    
    The implementation is the same assembly (or Go) routine.
    
    Change-Id: Ib937c461c24ad2d5be9b692b4eed40d9eb031412
    Reviewed-on: https://go-review.googlesource.com/2828
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 8547228ee3..2f9d520d44 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -1364,6 +1364,9 @@ TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
 	MOVQ	AX, ret+32(FP)
 	RET
 
+TEXT strings·Compare(SB),NOSPLIT,$0
+        JMP	runtime·cmpstring(SB)
+
 TEXT bytes·Compare(SB),NOSPLIT,$0-56
 	MOVQ	s1+0(FP), SI
 	MOVQ	s1+8(FP), BX

commit c1ee664573dc563f1c65e92230cfd6205fc42ed8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 13 21:44:37 2015 -0500

    baby go scheduler
    
    unfortunately, the trap handler itself is probably not that useful to write in
    go since, as go uses cooperative threads, a go trap handler interrupting its
    own go code is likely to cause problems.
    
    export helper functions too

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 963e8f0b86..056cdda5d4 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -159,6 +159,10 @@ GLOBL	fakeargv(SB),RODATA,$24
 
 TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
 
+	// magic loop
+	//BYTE	$0xeb
+	//BYTE	$0xfe
+
 	// save page table and first free address from bootloader.
 	MOVL	DI, pgtbl(SB)
 	MOVL	SI, first_free(SB)
@@ -502,13 +506,6 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	// pusha is not valid in 64bit mode!
 	PUSHQ	AX
 
-//	MOVQ	$(0x80000000 - 4096 + 14*8), AX
-//	CMPQ	SP, AX
-//	JA	sgut
-//	BYTE	$0xeb
-//	BYTE	$0xfe
-//
-//sgut:
 	PUSHQ	BX
 	PUSHQ	CX
 	PUSHQ	DX
@@ -533,12 +530,21 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 
 	MOVQ	SP, AX
 	PUSHQ	AX
+
+	MOVQ	newtrap(SB), AX
+	TESTQ	AX, AX
+	JZ	no_new
+	CALL	AX
+	// jmp self
+	BYTE	$0xeb
+	BYTE	$0xfe
+no_new:
 	CALL	trap(SB)
 	// jmp self
 	BYTE	$0xeb
 	BYTE	$0xfe
 
-TEXT trapret(SB), NOSPLIT, $0-8
+TEXT runtime·Trapret(SB), NOSPLIT, $0-8
 	MOVQ	fp+0(FP), AX
 	MOVQ	AX, SP
 

commit f7e43f14d324815b0884ed1dfee5d655f273e91e
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Wed Jan 7 14:24:18 2015 -0800

    runtime: remove stray commas in assembly
    
    Change-Id: I4dc97ff8111bdc5ca6e4e3af06aaf4f768031c68
    Reviewed-on: https://go-review.googlesource.com/2473
    Reviewed-by: Minux Ma <minux@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3199848599..8547228ee3 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -439,7 +439,7 @@ TEXT runtime·cas(SB), NOSPLIT, $0-17
 	MOVL	new+12(FP), CX
 	LOCK
 	CMPXCHGL	CX, 0(BX)
-	SETEQ	, ret+16(FP)
+	SETEQ	ret+16(FP)
 	RET
 
 // bool	runtime·cas64(uint64 *val, uint64 old, uint64 new)
@@ -456,7 +456,7 @@ TEXT runtime·cas64(SB), NOSPLIT, $0-25
 	MOVQ	new+16(FP), CX
 	LOCK
 	CMPXCHGQ	CX, 0(BX)
-	SETEQ	, ret+24(FP)
+	SETEQ	ret+24(FP)
 	RET
 	
 TEXT runtime·casuintptr(SB), NOSPLIT, $0-25
@@ -484,7 +484,7 @@ TEXT runtime·casp1(SB), NOSPLIT, $0-25
 	MOVQ	new+16(FP), CX
 	LOCK
 	CMPXCHGQ	CX, 0(BX)
-	SETEQ	, ret+24(FP)
+	SETEQ	ret+24(FP)
 	RET
 
 // uint32 xadd(uint32 volatile *val, int32 delta)

commit d5e4c4061b019a36ed3ac954c98f6535b7e78189
Author: Keith Randall <khr@golang.org>
Date:   Tue Jan 6 16:42:48 2015 -0800

    runtime: remove size argument from hash and equal algorithms
    
    The equal algorithm used to take the size
       equal(p, q *T, size uintptr) bool
    With this change, it does not
       equal(p, q *T) bool
    Similarly for the hash algorithm.
    
    The size is rarely used, as most equal functions know the size
    of the thing they are comparing.  For instance f32equal already
    knows its inputs are 4 bytes in size.
    
    For cases where the size is not known, we allocate a closure
    (one for each size needed) that points to an assembly stub that
    reads the size out of the closure and calls generic code that
    has a size argument.
    
    Reduces the size of the go binary by 0.07%.  Performance impact
    is not measurable.
    
    Change-Id: I6e00adf3dde7ad2974adbcff0ee91e86d2194fec
    Reviewed-on: https://go-review.googlesource.com/2392
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3e8ccca512..3199848599 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -853,23 +853,42 @@ TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	MOVQ	AX, ret+0(FP)
 	RET
 
+// memhash_varlen(p unsafe.Pointer, h seed) uintptr
+// redirects to memhash(p, h, size) using the size
+// stored in the closure.
+TEXT runtime·memhash_varlen(SB),NOSPLIT,$32-24
+	GO_ARGS
+	NO_LOCAL_POINTERS
+	MOVQ	p+0(FP), AX
+	MOVQ	h+8(FP), BX
+	MOVQ	8(DX), CX
+	MOVQ	AX, 0(SP)
+	MOVQ	BX, 8(SP)
+	MOVQ	CX, 16(SP)
+	CALL	runtime·memhash(SB)
+	MOVQ	24(SP), AX
+	MOVQ	AX, ret+16(FP)
+	RET
+
 // hash function using AES hardware instructions
 TEXT runtime·aeshash(SB),NOSPLIT,$0-32
 	MOVQ	p+0(FP), AX	// ptr to data
-	MOVQ	s+8(FP), CX	// size
+	MOVQ	s+16(FP), CX	// size
+	LEAQ	ret+24(FP), DX
 	JMP	runtime·aeshashbody(SB)
 
-TEXT runtime·aeshashstr(SB),NOSPLIT,$0-32
+TEXT runtime·aeshashstr(SB),NOSPLIT,$0-24
 	MOVQ	p+0(FP), AX	// ptr to string struct
-	// s+8(FP) is ignored, it is always sizeof(String)
 	MOVQ	8(AX), CX	// length of string
 	MOVQ	(AX), AX	// string data
+	LEAQ	ret+16(FP), DX
 	JMP	runtime·aeshashbody(SB)
 
 // AX: data
 // CX: length
-TEXT runtime·aeshashbody(SB),NOSPLIT,$0-32
-	MOVQ	h+16(FP), X6	// seed to low 64 bits of xmm6
+// DX: address to put return value
+TEXT runtime·aeshashbody(SB),NOSPLIT,$0-0
+	MOVQ	h+8(FP), X6	// seed to low 64 bits of xmm6
 	PINSRQ	$1, CX, X6	// size to high 64 bits of xmm6
 	PSHUFHW	$0, X6, X6	// replace size with its low 2 bytes repeated 4 times
 	MOVO	runtime·aeskeysched(SB), X7
@@ -903,7 +922,7 @@ aes0to15:
 	AESENC	X6, X0
 	AESENC	X7, X0
 	AESENC	X7, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 endofpage:
@@ -917,13 +936,13 @@ endofpage:
 	AESENC	X6, X0
 	AESENC	X7, X0
 	AESENC	X7, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 aes0:
 	// return input seed
-	MOVQ	h+16(FP), AX
-	MOVQ	AX, ret+24(FP)
+	MOVQ	h+8(FP), AX
+	MOVQ	AX, (DX)
 	RET
 
 aes16:
@@ -931,7 +950,7 @@ aes16:
 	AESENC	X6, X0
 	AESENC	X7, X0
 	AESENC	X7, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 aes17to32:
@@ -949,7 +968,7 @@ aes17to32:
 
 	// combine results
 	PXOR	X1, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 aes33to64:
@@ -974,7 +993,7 @@ aes33to64:
 	PXOR	X2, X0
 	PXOR	X3, X1
 	PXOR	X1, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 aes65to128:
@@ -1019,7 +1038,7 @@ aes65to128:
 	PXOR	X2, X0
 	PXOR	X3, X1
 	PXOR	X1, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 
 aes129plus:
@@ -1105,29 +1124,27 @@ aesloop:
 	PXOR	X2, X0
 	PXOR	X3, X1
 	PXOR	X1, X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, (DX)
 	RET
 	
-TEXT runtime·aeshash32(SB),NOSPLIT,$0-32
+TEXT runtime·aeshash32(SB),NOSPLIT,$0-24
 	MOVQ	p+0(FP), AX	// ptr to data
-	// s+8(FP) is ignored, it is always sizeof(int32)
-	MOVQ	h+16(FP), X0	// seed
+	MOVQ	h+8(FP), X0	// seed
 	PINSRD	$2, (AX), X0	// data
 	AESENC	runtime·aeskeysched+0(SB), X0
 	AESENC	runtime·aeskeysched+16(SB), X0
 	AESENC	runtime·aeskeysched+32(SB), X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, ret+16(FP)
 	RET
 
-TEXT runtime·aeshash64(SB),NOSPLIT,$0-32
+TEXT runtime·aeshash64(SB),NOSPLIT,$0-24
 	MOVQ	p+0(FP), AX	// ptr to data
-	// s+8(FP) is ignored, it is always sizeof(int64)
-	MOVQ	h+16(FP), X0	// seed
+	MOVQ	h+8(FP), X0	// seed
 	PINSRQ	$1, (AX), X0	// data
 	AESENC	runtime·aeskeysched+0(SB), X0
 	AESENC	runtime·aeskeysched+16(SB), X0
 	AESENC	runtime·aeskeysched+32(SB), X0
-	MOVQ	X0, ret+24(FP)
+	MOVQ	X0, ret+16(FP)
 	RET
 
 // simple mask to get rid of data in the high part of the register.
@@ -1210,6 +1227,20 @@ TEXT runtime·memeq(SB),NOSPLIT,$0-25
 	MOVB	AX, ret+24(FP)
 	RET
 
+// memequal_varlen(a, b unsafe.Pointer) bool
+TEXT runtime·memequal_varlen(SB),NOSPLIT,$0-17
+	MOVQ	a+0(FP), SI
+	MOVQ	b+8(FP), DI
+	CMPQ	SI, DI
+	JEQ	eq
+	MOVQ	8(DX), BX    // compiler stores size at offset 8 in the closure
+	CALL	runtime·memeqbody(SB)
+	MOVB	AX, ret+16(FP)
+	RET
+eq:
+	MOVB	$1, ret+16(FP)
+	RET
+
 // eqstring tests whether two strings are equal.
 // See runtime_test.go:eqstring_generic for
 // equivalent Go code.

commit df027aceb970a2e9dcafb6e79f8581efb2f30c86
Author: Russ Cox <rsc@golang.org>
Date:   Tue Dec 30 13:59:55 2014 -0500

    reflect: add write barriers
    
    Use typedmemmove, typedslicecopy, and adjust reflect.call
    to execute the necessary write barriers.
    
    Found with GODEBUG=wbshadow=2 mode.
    Eventually that will run automatically, but right now
    it still detects other missing write barriers.
    
    Change-Id: Iec5b5b0c1be5589295e28e5228e37f1a92e07742
    Reviewed-on: https://go-review.googlesource.com/2312
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 5a94e11e5d..3e8ccca512 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -318,7 +318,7 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 	JMP	runtime·morestack(SB)
 
 // reflectcall: call a function with the given argument list
-// func call(f *FuncVal, arg *byte, argsize, retoffset uint32).
+// func call(argtype *_type, f *FuncVal, arg *byte, argsize, retoffset uint32).
 // we don't have variable-sized frames, so we use a small number
 // of constant-sized-frame functions to encode a few bits of size in the pc.
 // Caution: ugly multiline assembly macros in your future!
@@ -333,9 +333,10 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 TEXT reflect·call(SB), NOSPLIT, $0-0
 	JMP	·reflectcall(SB)
 
-TEXT ·reflectcall(SB), NOSPLIT, $0-24
-	MOVLQZX argsize+16(FP), CX
-	DISPATCH(runtime·call16, 16)
+TEXT ·reflectcall(SB), NOSPLIT, $0-32
+	MOVLQZX argsize+24(FP), CX
+	// NOTE(rsc): No call16, because CALLFN needs four words
+	// of argument space to invoke callwritebarrier.
 	DISPATCH(runtime·call32, 32)
 	DISPATCH(runtime·call64, 64)
 	DISPATCH(runtime·call128, 128)
@@ -366,29 +367,38 @@ TEXT ·reflectcall(SB), NOSPLIT, $0-24
 	JMP	AX
 
 #define CALLFN(NAME,MAXSIZE)			\
-TEXT NAME(SB), WRAPPER, $MAXSIZE-24;		\
+TEXT NAME(SB), WRAPPER, $MAXSIZE-32;		\
 	NO_LOCAL_POINTERS;			\
 	/* copy arguments to stack */		\
-	MOVQ	argptr+8(FP), SI;		\
-	MOVLQZX argsize+16(FP), CX;		\
+	MOVQ	argptr+16(FP), SI;		\
+	MOVLQZX argsize+24(FP), CX;		\
 	MOVQ	SP, DI;				\
 	REP;MOVSB;				\
 	/* call function */			\
-	MOVQ	f+0(FP), DX;			\
+	MOVQ	f+8(FP), DX;			\
 	PCDATA  $PCDATA_StackMapIndex, $0;	\
 	CALL	(DX);				\
 	/* copy return values back */		\
-	MOVQ	argptr+8(FP), DI;		\
-	MOVLQZX	argsize+16(FP), CX;		\
-	MOVLQZX retoffset+20(FP), BX;		\
+	MOVQ	argptr+16(FP), DI;		\
+	MOVLQZX	argsize+24(FP), CX;		\
+	MOVLQZX retoffset+28(FP), BX;		\
 	MOVQ	SP, SI;				\
 	ADDQ	BX, DI;				\
 	ADDQ	BX, SI;				\
 	SUBQ	BX, CX;				\
 	REP;MOVSB;				\
+	/* execute write barrier updates */	\
+	MOVQ	argtype+0(FP), DX;		\
+	MOVQ	argptr+16(FP), DI;		\
+	MOVLQZX	argsize+24(FP), CX;		\
+	MOVLQZX retoffset+28(FP), BX;		\
+	MOVQ	DX, 0(SP);			\
+	MOVQ	DI, 8(SP);			\
+	MOVQ	CX, 16(SP);			\
+	MOVQ	BX, 24(SP);			\
+	CALL	runtime·callwritebarrier(SB);	\
 	RET
 
-CALLFN(·call16, 16)
 CALLFN(·call32, 32)
 CALLFN(·call64, 64)
 CALLFN(·call128, 128)

commit ce32db57a5b3b4e2818c11f7208bfc8ad8ab5798
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Jan 5 15:33:15 2015 -0500

    hook runtime-usleep, tweak timings
    
    will set timings correctly later

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 65a235c4be..963e8f0b86 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -142,11 +142,10 @@ TEXT fixcs(SB),NOSPLIT,$0
 
 TEXT runtime·deray(SB),NOSPLIT,$8
 	MOVQ	times+0(FP), CX
-	MOVQ	$0x80, DX
-	MOVQ	$0, AX
 back:
-	// outb	%al, (%dx)
-	BYTE	$0xee
+	// inb	$0x80, %al
+	BYTE	$0xe4
+	BYTE	$0x80
 	LOOP	back
 	RET
 

commit 30bce134e9a7fdc79c647cf9e6e701c245196ba3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Jan 5 15:32:49 2015 -0500

    correctly save 64bit fsbase

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 09cb8f24d2..65a235c4be 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -485,6 +485,18 @@ IH_NOEC(47,Xspur )
 
 #define IA32_FS_BASE   $0xc0000100UL
 
+TEXT wrfsb(SB), NOSPLIT, $0-8
+	get_tls(BX)
+	MOVQ	val+0(FP), AX
+	MOVQ	AX, g(BX)
+	RET
+
+TEXT rdfsb(SB), NOSPLIT, $0-8
+	get_tls(BX)
+	MOVQ	g(BX), AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
 TEXT alltraps(SB), NOSPLIT, $0-0
 	// tf[15] = trapno
 	// 15 + 1 pushes
@@ -516,6 +528,7 @@ TEXT alltraps(SB), NOSPLIT, $0-0
 	// save fsbase
 	MOVQ	IA32_FS_BASE, CX
 	RDMSR
+	SHLQ	$32, DX
 	ORQ	DX, AX
 	PUSHQ	AX
 

commit e6d351126475dd55d9f2094c22a11c63919ab106
Author: Russ Cox <rsc@golang.org>
Date:   Mon Jan 5 16:29:21 2015 +0000

    Revert "liblink, cmd/ld, runtime: remove stackguard1"
    
    This reverts commit ab0535ae3fb45ba734d47542cc4845f27f708d1b.
    
    I think it will remain useful to distinguish code that must
    run on a system stack from code that can run on either stack,
    even if that distinction is no
    longer based on the implementation language.
    
    That is, I expect to add a //go:systemstack comment that,
    in terms of the old implementation, tells the compiler,
    to pretend this function was written in C.
    
    Change-Id: I33d2ebb2f99ae12496484c6ec8ed07233d693275
    Reviewed-on: https://go-review.googlesource.com/2275
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 4061e99c4b..5a94e11e5d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -20,7 +20,8 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
 	LEAQ	(-64*1024+104)(SP), BX
-	MOVQ	BX, g_stackguard(DI)
+	MOVQ	BX, g_stackguard0(DI)
+	MOVQ	BX, g_stackguard1(DI)
 	MOVQ	BX, (g_stack+stack_lo)(DI)
 	MOVQ	SP, (g_stack+stack_hi)(DI)
 
@@ -48,7 +49,8 @@ nocpuinfo:
 	MOVQ	$runtime·g0(SB), CX
 	MOVQ	(g_stack+stack_lo)(CX), AX
 	ADDQ	$const__StackGuard, AX
-	MOVQ	AX, g_stackguard(CX)
+	MOVQ	AX, g_stackguard0(CX)
+	MOVQ	AX, g_stackguard1(CX)
 
 	CMPL	runtime·iswindows(SB), $0
 	JEQ ok

commit 17577e48c907744fbfd09ea5a4002178f3a83972
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Wed Dec 31 17:31:32 2014 -0800

    runtime: use SETEQ instead of JZ for cas
    
    Change-Id: Ibabbca3988d39bdce584924173a912d45f50f0dd
    Reviewed-on: https://go-review.googlesource.com/2243
    Reviewed-by: Dave Cheney <dave@cheney.net>
    Reviewed-by: Minux Ma <minux@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3d96d09014..4061e99c4b 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -427,12 +427,7 @@ TEXT runtime·cas(SB), NOSPLIT, $0-17
 	MOVL	new+12(FP), CX
 	LOCK
 	CMPXCHGL	CX, 0(BX)
-	JZ 4(PC)
-	MOVL	$0, AX
-	MOVB	AX, ret+16(FP)
-	RET
-	MOVL	$1, AX
-	MOVB	AX, ret+16(FP)
+	SETEQ	, ret+16(FP)
 	RET
 
 // bool	runtime·cas64(uint64 *val, uint64 old, uint64 new)
@@ -449,13 +444,7 @@ TEXT runtime·cas64(SB), NOSPLIT, $0-25
 	MOVQ	new+16(FP), CX
 	LOCK
 	CMPXCHGQ	CX, 0(BX)
-	JNZ	fail
-	MOVL	$1, AX
-	MOVB	AX, ret+24(FP)
-	RET
-fail:
-	MOVL	$0, AX
-	MOVB	AX, ret+24(FP)
+	SETEQ	, ret+24(FP)
 	RET
 	
 TEXT runtime·casuintptr(SB), NOSPLIT, $0-25
@@ -483,12 +472,7 @@ TEXT runtime·casp1(SB), NOSPLIT, $0-25
 	MOVQ	new+16(FP), CX
 	LOCK
 	CMPXCHGQ	CX, 0(BX)
-	JZ 4(PC)
-	MOVL	$0, AX
-	MOVB	AX, ret+24(FP)
-	RET
-	MOVL	$1, AX
-	MOVB	AX, ret+24(FP)
+	SETEQ	, ret+24(FP)
 	RET
 
 // uint32 xadd(uint32 volatile *val, int32 delta)

commit ab0535ae3fb45ba734d47542cc4845f27f708d1b
Author: Shenghou Ma <minux@golang.org>
Date:   Mon Dec 29 01:08:40 2014 -0500

    liblink, cmd/ld, runtime: remove stackguard1
    
    Now that we've removed all the C code in runtime and the C compilers,
    there is no need to have a separate stackguard field to check for C
    code on Go stack.
    
    Remove field g.stackguard1 and rename g.stackguard0 to g.stackguard.
    Adjust liblink and cmd/ld as necessary.
    
    Change-Id: I54e75db5a93d783e86af5ff1a6cd497d669d8d33
    Reviewed-on: https://go-review.googlesource.com/2144
    Reviewed-by: Keith Randall <khr@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 0ec5d7a806..3d96d09014 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -20,8 +20,7 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
 	LEAQ	(-64*1024+104)(SP), BX
-	MOVQ	BX, g_stackguard0(DI)
-	MOVQ	BX, g_stackguard1(DI)
+	MOVQ	BX, g_stackguard(DI)
 	MOVQ	BX, (g_stack+stack_lo)(DI)
 	MOVQ	SP, (g_stack+stack_hi)(DI)
 
@@ -49,8 +48,7 @@ nocpuinfo:
 	MOVQ	$runtime·g0(SB), CX
 	MOVQ	(g_stack+stack_lo)(CX), AX
 	ADDQ	$const__StackGuard, AX
-	MOVQ	AX, g_stackguard0(CX)
-	MOVQ	AX, g_stackguard1(CX)
+	MOVQ	AX, g_stackguard(CX)
 
 	CMPL	runtime·iswindows(SB), $0
 	JEQ ok

commit 81c40888fe49d25617643aa9ed0957547b365794
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Dec 28 17:38:32 2014 -0500

    virtual mem, interrupts, multitasking, and runtime initialization

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3f7f608410..09cb8f24d2 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -119,6 +119,450 @@ TEXT runtime·asminit(SB),NOSPLIT,$0-0
 	// No per-thread init.
 	RET
 
+#define		CODESEG		1
+#define		DATASEG		2
+#define		FSSEG		3
+
+TEXT fixcs(SB),NOSPLIT,$0
+	POPQ	AX
+	PUSHQ	$(CODESEG << 3)
+	PUSHQ	AX
+	// lretq
+	BYTE	$0x48
+	BYTE	$0xcb
+	MOVQ	$1, 0
+	//POPQ	DX
+	//MOVQ	$(3 << 3), AX
+	//SHLQ	$32, AX
+	//ORQ	DX, AX
+	//PUSHQ	AX
+	//// lret
+	//BYTE	$0xcb
+	//MOVQ	$1, 0
+
+TEXT runtime·deray(SB),NOSPLIT,$8
+	MOVQ	times+0(FP), CX
+	MOVQ	$0x80, DX
+	MOVQ	$0, AX
+back:
+	// outb	%al, (%dx)
+	BYTE	$0xee
+	LOOP	back
+	RET
+
+// i do it this strange way because if i declare fakeargv in C i get 'missing
+// golang type information'. need two 0 entries because go checks for
+// environment variables too.
+DATA	fakeargv+0(SB)/8,$gostr(SB)
+DATA	fakeargv+8(SB)/8,$0
+DATA	fakeargv+16(SB)/8,$0
+GLOBL	fakeargv(SB),RODATA,$24
+
+TEXT runtime·rt0_go_hack(SB),NOSPLIT,$0
+
+	// save page table and first free address from bootloader.
+	MOVL	DI, pgtbl(SB)
+	MOVL	SI, first_free(SB)
+	MOVQ	$1, runtime·hackmode(SB)
+
+	ANDQ	$~15, SP
+	//SUBQ	$8, SP	// traceback assumes the initial rsp is writable
+
+	// create istack out of the given (operating system) stack.
+	// _cgo_init may update stackguard.
+	MOVQ	$runtime·g0(SB), DI
+	LEAQ	(-64*1024+104)(SP), BX
+	MOVQ	BX, g_stackguard0(DI)
+	MOVQ	BX, g_stackguard1(DI)
+	MOVQ	BX, (g_stack+stack_lo)(DI)
+	MOVQ	SP, (g_stack+stack_hi)(DI)
+
+	// find out information about the processor we're on
+	MOVQ	$0, AX
+	CPUID
+	CMPQ	AX, $0
+	JE	h_nocpuinfo
+	MOVQ	$1, AX
+	CPUID
+	MOVL	CX, runtime·cpuid_ecx(SB)
+	MOVL	DX, runtime·cpuid_edx(SB)
+h_nocpuinfo:
+
+	CALL	cls(SB)
+	//PUSHQ	$0x20
+	//CALL	runtime·doc(SB)
+	//POPQ	AX
+
+	//PUSHQ	$0x39
+	//CALL	runtime·doc(SB)
+	//CALL	runtime·doc(SB)
+	//POPQ	AX
+
+	// if there is an _cgo_init, call it.
+	//MOVQ	_cgo_init(SB), AX
+	//TESTQ	AX, AX
+	//JZ	needtls
+	//// g0 already in DI
+	//MOVQ	DI, CX	// Win64 uses CX for first parameter
+	//MOVQ	$setg_gcc<>(SB), SI
+	//CALL	AX
+
+	//// update stackguard after _cgo_init
+	//MOVQ	$runtime·g0(SB), CX
+	//MOVQ	(g_stack+stack_lo)(CX), AX
+	//ADDQ	$const_StackGuard, AX
+	//MOVQ	AX, g_stackguard0(CX)
+	//MOVQ	AX, g_stackguard1(CX)
+
+	//CMPL	runtime·iswindows(SB), $0
+	//JEQ ok
+h_needtls:
+
+	//// skip TLS setup on Plan 9
+	//CMPL	runtime·isplan9(SB), $1
+	//JEQ ok
+	//// skip TLS setup on Solaris
+	//CMPL	runtime·issolaris(SB), $1
+	//JEQ ok
+
+	// setup tls
+	LEAQ	runtime·tls0(SB), DI
+	PUSHQ	DI
+	CALL	segsetup(SB)
+
+	MOVQ	8(AX), DI
+	PUSHQ	DI
+	MOVQ	(AX), DI
+	PUSHQ	DI
+	// lgdt (%rsp)
+	BYTE	$0x0f
+	BYTE	$0x01
+	BYTE	$0x14
+	BYTE	$0x24
+	POPQ	AX
+	POPQ	AX
+	POPQ	AX
+
+	MOVQ	$(FSSEG << 3), AX
+	PUSHQ	AX
+	POPQ	FS
+
+	MOVL	$(DATASEG << 3), AX
+	//MOVL	AX, ES
+	BYTE	$0x8e
+	BYTE	$0xd8
+	//MOVL	AX, DS
+	BYTE	$0x8e
+	BYTE	$0xc0
+	//MOVL	AX, SS
+	BYTE	$0x8e
+	BYTE	$0xd0
+
+	// i cannot fix CS via far call to a label because i don't know how to
+	// call a label with plan9 compiler.
+	CALL	fixcs(SB)
+
+	// store through it, to make sure it works
+	get_tls(BX)
+	MOVQ	$0x123, g(BX)
+	MOVQ	runtime·tls0(SB), AX
+	CMPQ	AX, $0x123
+	JEQ	h_ok
+	MOVQ	$0x4242424242424242, AX
+	PUSHQ	AX
+	PUSHQ	$0
+	CALL	runtime·pancake(SB)
+h_ok:
+
+	// set the per-goroutine and per-mach "registers"
+	get_tls(BX)
+	LEAQ	runtime·g0(SB), CX
+	MOVQ	CX, g(BX)
+	LEAQ	runtime·m0(SB), AX
+
+	// save m->g0 = g0
+	MOVQ	CX, m_g0(AX)
+	// save m0 to g0->m
+	MOVQ	AX, g_m(CX)
+
+	CALL	intsetup(SB)
+	CALL	timersetup(SB)
+
+	FINIT
+	MOVQ	CR4, AX
+	PUSHQ	AX
+	MOVQ	CR0, AX
+	PUSHQ	AX
+	CALL	fpuinit(SB)
+	POPQ	AX
+	POPQ	AX
+
+	//MOVQ	CR0, AX
+	//PUSHQ	AX
+	//CALL	exam(SB)
+	//POPQ	AX
+
+	//CALL	pgtest(SB)
+	//CALL	mmap_test(SB)
+
+//	CMPQ	AX, $31337
+//	JZ	forward
+//me:
+//	JMP	me
+//forward:
+	CLD				// convention is D is always left cleared
+	CALL	runtime·check(SB)
+
+	//MOVL	16(SP), AX		// copy argc
+	//MOVL	AX, 0(SP)
+	//MOVQ	24(SP), AX		// copy argv
+	//MOVQ	AX, 8(SP)
+	MOVQ	$fakeargv(SB), AX
+	PUSHQ	AX
+	PUSHQ	$1
+	CALL	runtime·args(SB)
+	POPQ	AX
+	POPQ	AX
+	CALL	runtime·osinit(SB)
+	CALL	runtime·schedinit(SB)
+
+	// create a new goroutine to start program
+	MOVQ	$runtime·main·f(SB), BP		// entry
+	PUSHQ	BP
+	PUSHQ	$0			// arg size
+	CALL	runtime·newproc(SB)
+	POPQ	AX
+	POPQ	AX
+
+	// start this M
+	STI
+	//CALL	clone_test(SB)
+	CALL	runtime·mstart(SB)
+
+	MOVL	$0xf1, 0xf1  // crash
+	RET
+
+
+TEXT rcr2(SB), NOSPLIT, $0-8
+	MOVQ	CR2, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
+TEXT tlbflush(SB), NOSPLIT, $0-0
+	MOVQ	CR3, AX
+	MOVQ	AX, CR3
+	RET
+
+TEXT invlpg(SB), NOSPLIT, $0-8
+	MOVQ	va+0(FP), AX
+	INVLPG	(AX)
+	RET
+
+TEXT lidt(SB), NOSPLIT, $0-8
+	MOVQ	idtpd+0(FP), AX
+	MOVQ	8(AX), DI
+	PUSHQ	DI
+	MOVQ	(AX), DI
+	PUSHQ	DI
+	// lidt	(%rsp)
+	BYTE	$0x0f
+	BYTE	$0x01
+	BYTE	$0x1c
+	BYTE	$0x24
+	POPQ	AX
+	POPQ	AX
+	RET
+
+TEXT ltr(SB), NOSPLIT, $0-8
+	MOVQ	seg+0(FP), AX
+	// ltr	%ax
+	BYTE $0x0f
+	BYTE $0x00
+	BYTE $0xd8
+	RET
+
+TEXT lcr0(SB), NOSPLIT, $0-8
+	MOVQ	val+0(FP), AX
+	MOVQ	AX, CR0
+	RET
+
+TEXT lcr4(SB), NOSPLIT, $0-8
+	MOVQ	val+0(FP), AX
+	MOVQ	AX, CR4
+	RET
+
+TEXT rdmsr(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), CX
+	RDMSR
+	MOVL	DX, ret2+12(FP)
+	MOVL	AX, ret1+8(FP)
+	RET
+
+// void wrmsr(uint64 reg, uint64 val)
+TEXT wrmsr(SB), NOSPLIT, $0-16
+	MOVQ	reg+0(FP), CX
+	MOVL	vlo+8(FP), AX
+	MOVL	vhi+12(FP), DX
+	WRMSR
+	RET
+
+TEXT outb(SB), NOSPLIT, $0-8
+	MOVL	reg+0(FP), DX
+	MOVL	val+4(FP), AX
+	// outb	%al, (%dx)
+	BYTE	$0xee
+	RET
+
+TEXT rflags(SB), NOSPLIT, $0-8
+	// pushf
+	BYTE	$0x9c
+	POPQ	AX
+	MOVQ	AX, ret+8(FP)
+	RET
+
+TEXT rrsp(SB), NOSPLIT, $0-8
+	MOVQ	SP, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
+TEXT cli(SB), NOSPLIT, $0-0
+	CLI
+	RET
+
+TEXT sti(SB), NOSPLIT, $0-0
+	STI
+	RET
+
+#define TRAP_TIMER      $32
+TEXT hack_yield(SB), NOSPLIT, $0-0
+	INT	TRAP_TIMER
+	RET
+
+#define IH_NOEC(num, fn)		\
+TEXT fn(SB), NOSPLIT, $0-0;		\
+	PUSHQ	$0;			\
+	PUSHQ	$num;			\
+	JMP	alltraps(SB);		\
+	BYTE	$0xeb;			\
+	BYTE	$0xfe;			\
+	POPQ	AX;			\
+	POPQ	AX;			\
+	RET
+
+#define IH_EC(num, fn)			\
+TEXT fn(SB), NOSPLIT, $0-0;		\
+	PUSHQ	$num;			\
+	JMP	alltraps(SB);		\
+	BYTE	$0xeb;			\
+	BYTE	$0xfe;			\
+	POPQ	AX;			\
+	POPQ	AX;			\
+	RET
+
+IH_NOEC( 0,Xdz )
+IH_NOEC( 1,Xrz )
+IH_NOEC( 2,Xnmi )
+IH_NOEC( 3,Xbp )
+IH_NOEC( 4,Xov )
+IH_NOEC( 5,Xbnd )
+IH_NOEC( 6,Xuo )
+IH_NOEC( 7,Xnm )
+IH_EC  ( 8,Xdf )
+IH_NOEC( 9,Xrz2 )
+IH_EC  (10,Xtss )
+IH_EC  (11,Xsnp )
+IH_EC  (12,Xssf )
+IH_EC  (13,Xgp )
+IH_EC  (14,Xpf )
+IH_NOEC(15,Xrz3 )
+IH_NOEC(16,Xmf )
+IH_EC  (17,Xac )
+IH_NOEC(18,Xmc )
+IH_NOEC(19,Xfp )
+IH_NOEC(20,Xve )
+IH_NOEC(32,Xtimer )
+IH_NOEC(47,Xspur )
+
+#define IA32_FS_BASE   $0xc0000100UL
+
+TEXT alltraps(SB), NOSPLIT, $0-0
+	// tf[15] = trapno
+	// 15 + 1 pushes
+	// pusha is not valid in 64bit mode!
+	PUSHQ	AX
+
+//	MOVQ	$(0x80000000 - 4096 + 14*8), AX
+//	CMPQ	SP, AX
+//	JA	sgut
+//	BYTE	$0xeb
+//	BYTE	$0xfe
+//
+//sgut:
+	PUSHQ	BX
+	PUSHQ	CX
+	PUSHQ	DX
+	PUSHQ	DI
+	PUSHQ	SI
+	PUSHQ	BP
+	PUSHQ	R8
+	PUSHQ	R9
+	PUSHQ	R10
+	PUSHQ	R11
+	PUSHQ	R12
+	PUSHQ	R13
+	PUSHQ	R14
+	PUSHQ	R15
+
+	// save fsbase
+	MOVQ	IA32_FS_BASE, CX
+	RDMSR
+	ORQ	DX, AX
+	PUSHQ	AX
+
+	MOVQ	SP, AX
+	PUSHQ	AX
+	CALL	trap(SB)
+	// jmp self
+	BYTE	$0xeb
+	BYTE	$0xfe
+
+TEXT trapret(SB), NOSPLIT, $0-8
+	MOVQ	fp+0(FP), AX
+	MOVQ	AX, SP
+
+	// restore fsbase
+	MOVQ	IA32_FS_BASE, CX
+	POPQ	AX
+	MOVQ	AX, DX
+	ANDQ	$((1 << 32) - 1), AX
+	SHRQ	$32, DX
+	WRMSR
+
+	POPQ	R15
+	POPQ	R14
+	POPQ	R13
+	POPQ	R12
+	POPQ	R11
+	POPQ	R10
+	POPQ	R9
+	POPQ	R8
+	POPQ	BP
+	POPQ	SI
+	POPQ	DI
+	POPQ	DX
+	POPQ	CX
+	POPQ	BX
+	POPQ	AX
+	// skip trapno and error code
+	ADDQ	$16, SP
+
+	// iretq
+	BYTE	$0x48
+	BYTE	$0xcf
+	// jmp self
+	BYTE	$0xeb
+	BYTE	$0xfe
+
 /*
  *  go-routine
  */

commit 7a524a103647d0b839ff133be1b1b866c92d11fb
Author: Russ Cox <rsc@golang.org>
Date:   Mon Dec 22 13:27:53 2014 -0500

    runtime: remove thunk.s
    
    Replace with uses of //go:linkname in Go files, direct use of name in .s files.
    The only one that really truly needs a jump is reflect.call; the jump is now
    next to the runtime.reflectcall assembly implementations.
    
    Change-Id: Ie7ff3020a8f60a8e4c8645fe236e7883a3f23f46
    Reviewed-on: https://go-review.googlesource.com/1962
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a8a827c1c5..0ec5d7a806 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -330,6 +330,9 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 	JMP	AX
 // Note: can't just "JMP NAME(SB)" - bad inlining results.
 
+TEXT reflect·call(SB), NOSPLIT, $0-0
+	JMP	·reflectcall(SB)
+
 TEXT ·reflectcall(SB), NOSPLIT, $0-24
 	MOVLQZX argsize+16(FP), CX
 	DISPATCH(runtime·call16, 16)
@@ -1336,7 +1339,7 @@ TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
 	MOVQ	AX, ret+32(FP)
 	RET
 
-TEXT runtime·cmpbytes(SB),NOSPLIT,$0-56
+TEXT bytes·Compare(SB),NOSPLIT,$0-56
 	MOVQ	s1+0(FP), SI
 	MOVQ	s1+8(FP), BX
 	MOVQ	s2+24(FP), DI

commit 7a4a64e8f3dc14717695e53c7560992789f8bc9e
Author: Keith Randall <khr@golang.org>
Date:   Wed Dec 10 14:20:17 2014 -0800

    runtime: faster aeshash implementation
    
    The aesenc instruction has high latency.  For hashing large objects,
    hash several streams in parallel.
    
    benchmark                         old ns/op     new ns/op     delta
    BenchmarkHash5                    7.02          7.45          +6.13%
    BenchmarkHash16                   6.53          6.94          +6.28%
    BenchmarkHash32                   8.38          8.26          -1.43%
    BenchmarkHash64                   12.6          12.0          -4.76%
    BenchmarkHash1024                 247           62.9          -74.53%
    BenchmarkHash65536                17335         2966          -82.89%
    BenchmarkHashInt32Speed           14.7          14.9          +1.36%
    BenchmarkHashInt64Speed           14.6          14.9          +2.05%
    BenchmarkHashBytesSpeed           35.4          28.6          -19.21%
    BenchmarkHashStringSpeed          22.0          20.4          -7.27%
    BenchmarkHashStringArraySpeed     65.8          56.3          -14.44%
    
    Change-Id: Ia8ba03063acc64a9066b8ab2d79f2c9aaac1770f
    Reviewed-on: https://go-review.googlesource.com/1330
    Reviewed-by: Russ Cox <rsc@golang.org>

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 14be2fe92d..a8a827c1c5 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -872,62 +872,245 @@ TEXT runtime·aeshashstr(SB),NOSPLIT,$0-32
 // AX: data
 // CX: length
 TEXT runtime·aeshashbody(SB),NOSPLIT,$0-32
-	MOVQ	h+16(FP), X0	// seed to low 64 bits of xmm0
-	PINSRQ	$1, CX, X0	// size to high 64 bits of xmm0
-	MOVO	runtime·aeskeysched+0(SB), X2
-	MOVO	runtime·aeskeysched+16(SB), X3
+	MOVQ	h+16(FP), X6	// seed to low 64 bits of xmm6
+	PINSRQ	$1, CX, X6	// size to high 64 bits of xmm6
+	PSHUFHW	$0, X6, X6	// replace size with its low 2 bytes repeated 4 times
+	MOVO	runtime·aeskeysched(SB), X7
 	CMPQ	CX, $16
-	JB	small
-loop:
-	CMPQ	CX, $16
-	JBE	loopend
-	MOVOU	(AX), X1
-	AESENC	X2, X0
-	AESENC	X1, X0
-	SUBQ	$16, CX
-	ADDQ	$16, AX
-	JMP	loop
-// 1-16 bytes remaining
-loopend:
-	// This load may overlap with the previous load above.
-	// We'll hash some bytes twice, but that's ok.
-	MOVOU	-16(AX)(CX*1), X1
-	JMP	partial
-// 0-15 bytes
-small:
+	JB	aes0to15
+	JE	aes16
+	CMPQ	CX, $32
+	JBE	aes17to32
+	CMPQ	CX, $64
+	JBE	aes33to64
+	CMPQ	CX, $128
+	JBE	aes65to128
+	JMP	aes129plus
+
+aes0to15:
 	TESTQ	CX, CX
-	JE	finalize	// 0 bytes
+	JE	aes0
 
-	CMPB	AX, $0xf0
-	JA	highpartial
+	ADDQ	$16, AX
+	TESTW	$0xff0, AX
+	JE	endofpage
 
 	// 16 bytes loaded at this address won't cross
 	// a page boundary, so we can load it directly.
-	MOVOU	(AX), X1
+	MOVOU	-16(AX), X0
 	ADDQ	CX, CX
 	MOVQ	$masks<>(SB), BP
-	PAND	(BP)(CX*8), X1
-	JMP	partial
-highpartial:
+	PAND	(BP)(CX*8), X0
+
+	// scramble 3 times
+	AESENC	X6, X0
+	AESENC	X7, X0
+	AESENC	X7, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+endofpage:
 	// address ends in 1111xxxx.  Might be up against
 	// a page boundary, so load ending at last byte.
 	// Then shift bytes down using pshufb.
-	MOVOU	-16(AX)(CX*1), X1
+	MOVOU	-32(AX)(CX*1), X0
 	ADDQ	CX, CX
 	MOVQ	$shifts<>(SB), BP
-	PSHUFB	(BP)(CX*8), X1
-partial:
-	// incorporate partial block into hash
-	AESENC	X3, X0
-	AESENC	X1, X0
-finalize:	
-	// finalize hash
-	AESENC	X2, X0
-	AESENC	X3, X0
-	AESENC	X2, X0
-	MOVQ	X0, res+24(FP)
+	PSHUFB	(BP)(CX*8), X0
+	AESENC	X6, X0
+	AESENC	X7, X0
+	AESENC	X7, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+aes0:
+	// return input seed
+	MOVQ	h+16(FP), AX
+	MOVQ	AX, ret+24(FP)
 	RET
 
+aes16:
+	MOVOU	(AX), X0
+	AESENC	X6, X0
+	AESENC	X7, X0
+	AESENC	X7, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+aes17to32:
+	// load data to be hashed
+	MOVOU	(AX), X0
+	MOVOU	-16(AX)(CX*1), X1
+
+	// scramble 3 times
+	AESENC	X6, X0
+	AESENC	runtime·aeskeysched+16(SB), X1
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X0
+	AESENC	X7, X1
+
+	// combine results
+	PXOR	X1, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+aes33to64:
+	MOVOU	(AX), X0
+	MOVOU	16(AX), X1
+	MOVOU	-32(AX)(CX*1), X2
+	MOVOU	-16(AX)(CX*1), X3
+	
+	AESENC	X6, X0
+	AESENC	runtime·aeskeysched+16(SB), X1
+	AESENC	runtime·aeskeysched+32(SB), X2
+	AESENC	runtime·aeskeysched+48(SB), X3
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+
+	PXOR	X2, X0
+	PXOR	X3, X1
+	PXOR	X1, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+aes65to128:
+	MOVOU	(AX), X0
+	MOVOU	16(AX), X1
+	MOVOU	32(AX), X2
+	MOVOU	48(AX), X3
+	MOVOU	-64(AX)(CX*1), X4
+	MOVOU	-48(AX)(CX*1), X5
+	MOVOU	-32(AX)(CX*1), X8
+	MOVOU	-16(AX)(CX*1), X9
+	
+	AESENC	X6, X0
+	AESENC	runtime·aeskeysched+16(SB), X1
+	AESENC	runtime·aeskeysched+32(SB), X2
+	AESENC	runtime·aeskeysched+48(SB), X3
+	AESENC	runtime·aeskeysched+64(SB), X4
+	AESENC	runtime·aeskeysched+80(SB), X5
+	AESENC	runtime·aeskeysched+96(SB), X8
+	AESENC	runtime·aeskeysched+112(SB), X9
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X4
+	AESENC	X7, X5
+	AESENC	X7, X8
+	AESENC	X7, X9
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X4
+	AESENC	X7, X5
+	AESENC	X7, X8
+	AESENC	X7, X9
+
+	PXOR	X4, X0
+	PXOR	X5, X1
+	PXOR	X8, X2
+	PXOR	X9, X3
+	PXOR	X2, X0
+	PXOR	X3, X1
+	PXOR	X1, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+aes129plus:
+	// start with last (possibly overlapping) block
+	MOVOU	-128(AX)(CX*1), X0
+	MOVOU	-112(AX)(CX*1), X1
+	MOVOU	-96(AX)(CX*1), X2
+	MOVOU	-80(AX)(CX*1), X3
+	MOVOU	-64(AX)(CX*1), X4
+	MOVOU	-48(AX)(CX*1), X5
+	MOVOU	-32(AX)(CX*1), X8
+	MOVOU	-16(AX)(CX*1), X9
+
+	// scramble state once
+	AESENC	X6, X0
+	AESENC	runtime·aeskeysched+16(SB), X1
+	AESENC	runtime·aeskeysched+32(SB), X2
+	AESENC	runtime·aeskeysched+48(SB), X3
+	AESENC	runtime·aeskeysched+64(SB), X4
+	AESENC	runtime·aeskeysched+80(SB), X5
+	AESENC	runtime·aeskeysched+96(SB), X8
+	AESENC	runtime·aeskeysched+112(SB), X9
+
+	// compute number of remaining 128-byte blocks
+	DECQ	CX
+	SHRQ	$7, CX
+	
+aesloop:
+	// scramble state, xor in a block
+	MOVOU	(AX), X10
+	MOVOU	16(AX), X11
+	MOVOU	32(AX), X12
+	MOVOU	48(AX), X13
+	AESENC	X10, X0
+	AESENC	X11, X1
+	AESENC	X12, X2
+	AESENC	X13, X3
+	MOVOU	64(AX), X10
+	MOVOU	80(AX), X11
+	MOVOU	96(AX), X12
+	MOVOU	112(AX), X13
+	AESENC	X10, X4
+	AESENC	X11, X5
+	AESENC	X12, X8
+	AESENC	X13, X9
+
+	// scramble state
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X4
+	AESENC	X7, X5
+	AESENC	X7, X8
+	AESENC	X7, X9
+
+	ADDQ	$128, AX
+	DECQ	CX
+	JNE	aesloop
+
+	// 2 more scrambles to finish
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X4
+	AESENC	X7, X5
+	AESENC	X7, X8
+	AESENC	X7, X9
+	AESENC	X7, X0
+	AESENC	X7, X1
+	AESENC	X7, X2
+	AESENC	X7, X3
+	AESENC	X7, X4
+	AESENC	X7, X5
+	AESENC	X7, X8
+	AESENC	X7, X9
+
+	PXOR	X4, X0
+	PXOR	X5, X1
+	PXOR	X8, X2
+	PXOR	X9, X3
+	PXOR	X2, X0
+	PXOR	X3, X1
+	PXOR	X1, X0
+	MOVQ	X0, ret+24(FP)
+	RET
+	
 TEXT runtime·aeshash32(SB),NOSPLIT,$0-32
 	MOVQ	p+0(FP), AX	// ptr to data
 	// s+8(FP) is ignored, it is always sizeof(int32)
@@ -935,7 +1118,7 @@ TEXT runtime·aeshash32(SB),NOSPLIT,$0-32
 	PINSRD	$2, (AX), X0	// data
 	AESENC	runtime·aeskeysched+0(SB), X0
 	AESENC	runtime·aeskeysched+16(SB), X0
-	AESENC	runtime·aeskeysched+0(SB), X0
+	AESENC	runtime·aeskeysched+32(SB), X0
 	MOVQ	X0, ret+24(FP)
 	RET
 
@@ -946,7 +1129,7 @@ TEXT runtime·aeshash64(SB),NOSPLIT,$0-32
 	PINSRQ	$1, (AX), X0	// data
 	AESENC	runtime·aeskeysched+0(SB), X0
 	AESENC	runtime·aeskeysched+16(SB), X0
-	AESENC	runtime·aeskeysched+0(SB), X0
+	AESENC	runtime·aeskeysched+32(SB), X0
 	MOVQ	X0, ret+24(FP)
 	RET
 

commit 8c3f64022a34c140853a3ec6a3be5695e6c05de2
Author: Russ Cox <rsc@golang.org>
Date:   Fri Nov 21 15:57:10 2014 -0500

    [dev.garbage] runtime: add prefetcht0, prefetcht1, prefetcht2, prefetcht3, prefetchnta for GC
    
    We don't know what we need yet, so add them all.
    Add them even on x86 architectures (as no-ops) so that
    the GC can refer to them unconditionally.
    
    Eventually we'll know what we want and probably
    have just one 'prefetch' with an appropriate meaning
    on each architecture.
    
    LGTM=rlh
    R=rlh
    CC=golang-codereviews
    https://golang.org/cl/179160043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 6e3f5ff6ca..14be2fe92d 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2228,3 +2228,23 @@ TEXT runtime·getg(SB),NOSPLIT,$0-8
 	MOVQ	g(CX), AX
 	MOVQ	AX, ret+0(FP)
 	RET
+
+TEXT runtime·prefetcht0(SB),NOSPLIT,$0-8
+	MOVQ	addr+0(FP), AX
+	PREFETCHT0	(AX)
+	RET
+
+TEXT runtime·prefetcht1(SB),NOSPLIT,$0-8
+	MOVQ	addr+0(FP), AX
+	PREFETCHT1	(AX)
+	RET
+
+TEXT runtime·prefetcht2(SB),NOSPLIT,$0-8
+	MOVQ	addr+0(FP), AX
+	PREFETCHT2	(AX)
+	RET
+
+TEXT runtime·prefetchnta(SB),NOSPLIT,$0-8
+	MOVQ	addr+0(FP), AX
+	PREFETCHNTA	(AX)
+	RET

commit 3e804631d905162d9e0fc1eba63bce6f95965a24
Merge: 9ad6b7e322 743bdf612a
Author: Russ Cox <rsc@golang.org>
Date:   Fri Nov 14 12:10:52 2014 -0500

    [dev.cc] all: merge dev.power64 (7667e41f3ced) into dev.cc
    
    This is to reduce the delta between dev.cc and dev.garbage to just garbage collector changes.
    
    These are the files that had merge conflicts and have been edited by hand:
            malloc.go
            mem_linux.go
            mgc.go
            os1_linux.go
            proc1.go
            panic1.go
            runtime1.go
    
    LGTM=austin
    R=austin
    CC=golang-codereviews
    https://golang.org/cl/174180043

commit 656be317d06cd5f2fefc728e9b33475688bdda40
Author: Russ Cox <rsc@golang.org>
Date:   Wed Nov 12 14:54:31 2014 -0500

    [dev.cc] runtime: delete scalararg, ptrarg; rename onM to systemstack
    
    Scalararg and ptrarg are not "signal safe".
    Go code filling them out can be interrupted by a signal,
    and then the signal handler runs, and if it also ends up
    in Go code that uses scalararg or ptrarg, now the old
    values have been smashed.
    For the pieces of code that do need to run in a signal handler,
    we introduced onM_signalok, which is really just onM
    except that the _signalok is meant to convey that the caller
    asserts that scalarg and ptrarg will be restored to their old
    values after the call (instead of the usual behavior, zeroing them).
    
    Scalararg and ptrarg are also untyped and therefore error-prone.
    
    Go code can always pass a closure instead of using scalararg
    and ptrarg; they were only really necessary for C code.
    And there's no more C code.
    
    For all these reasons, delete scalararg and ptrarg, converting
    the few remaining references to use closures.
    
    Once those are gone, there is no need for a distinction between
    onM and onM_signalok, so replace both with a single function
    equivalent to the current onM_signalok (that is, it can be called
    on any of the curg, g0, and gsignal stacks).
    
    The name onM and the phrase 'm stack' are misnomers,
    because on most system an M has two system stacks:
    the main thread stack and the signal handling stack.
    
    Correct the misnomer by naming the replacement function systemstack.
    
    Fix a few references to "M stack" in code.
    
    The main motivation for this change is to eliminate scalararg/ptrarg.
    Rick and I have already seen them cause problems because
    the calling sequence m.ptrarg[0] = p is a heap pointer assignment,
    so it gets a write barrier. The write barrier also uses onM, so it has
    all the same problems as if it were being invoked by a signal handler.
    We worked around this by saving and restoring the old values
    and by calling onM_signalok, but there's no point in keeping this nice
    home for bugs around any longer.
    
    This CL also changes funcline to return the file name as a result
    instead of filling in a passed-in *string. (The *string signature is
    left over from when the code was written in and called from C.)
    That's arguably an unrelated change, except that once I had done
    the ptrarg/scalararg/onM cleanup I started getting false positives
    about the *string argument escaping (not allowed in package runtime).
    The compiler is wrong, but the easiest fix is to write the code like
    Go code instead of like C code. I am a bit worried that the compiler
    is wrong because of some use of uninitialized memory in the escape
    analysis. If that's the reason, it will go away when we convert the
    compiler to Go. (And if not, we'll debug it the next time.)
    
    LGTM=khr
    R=r, khr
    CC=austin, golang-codereviews, iant, rlh
    https://golang.org/cl/174950043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 9a74a5310b..5840c32b5a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -190,55 +190,41 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 	JMP	AX
 	RET
 
-// switchtoM is a dummy routine that onM leaves at the bottom
+// systemstack_switch is a dummy routine that systemstack leaves at the bottom
 // of the G stack.  We need to distinguish the routine that
 // lives at the bottom of the G stack from the one that lives
-// at the top of the M stack because the one at the top of
-// the M stack terminates the stack walk (see topofstack()).
-TEXT runtime·switchtoM(SB), NOSPLIT, $0-0
+// at the top of the system stack because the one at the top of
+// the system stack terminates the stack walk (see topofstack()).
+TEXT runtime·systemstack_switch(SB), NOSPLIT, $0-0
 	RET
 
-// func onM_signalok(fn func())
-TEXT runtime·onM_signalok(SB), NOSPLIT, $0-8
+// func systemstack(fn func())
+TEXT runtime·systemstack(SB), NOSPLIT, $0-8
+	MOVQ	fn+0(FP), DI	// DI = fn
 	get_tls(CX)
 	MOVQ	g(CX), AX	// AX = g
 	MOVQ	g_m(AX), BX	// BX = m
+
 	MOVQ	m_gsignal(BX), DX	// DX = gsignal
 	CMPQ	AX, DX
-	JEQ	ongsignal
-	JMP	runtime·onM(SB)
-
-ongsignal:
-	MOVQ	fn+0(FP), DI	// DI = fn
-	MOVQ	DI, DX
-	MOVQ	0(DI), DI
-	CALL	DI
-	RET
-
-// func onM(fn func())
-TEXT runtime·onM(SB), NOSPLIT, $0-8
-	MOVQ	fn+0(FP), DI	// DI = fn
-	get_tls(CX)
-	MOVQ	g(CX), AX	// AX = g
-	MOVQ	g_m(AX), BX	// BX = m
+	JEQ	noswitch
 
 	MOVQ	m_g0(BX), DX	// DX = g0
 	CMPQ	AX, DX
-	JEQ	onm
+	JEQ	noswitch
 
 	MOVQ	m_curg(BX), BP
 	CMPQ	AX, BP
-	JEQ	oncurg
+	JEQ	switch
 	
-	// Not g0, not curg. Must be gsignal, but that's not allowed.
-	// Hide call from linker nosplit analysis.
-	MOVQ	$runtime·badonm(SB), AX
+	// Bad: g is not gsignal, not g0, not curg. What is it?
+	MOVQ	$runtime·badsystemstack(SB), AX
 	CALL	AX
 
-oncurg:
+switch:
 	// save our state in g->sched.  Pretend to
-	// be switchtoM if the G stack is scanned.
-	MOVQ	$runtime·switchtoM(SB), BP
+	// be systemstack_switch if the G stack is scanned.
+	MOVQ	$runtime·systemstack_switch(SB), BP
 	MOVQ	BP, (g_sched+gobuf_pc)(AX)
 	MOVQ	SP, (g_sched+gobuf_sp)(AX)
 	MOVQ	AX, (g_sched+gobuf_g)(AX)
@@ -246,7 +232,7 @@ oncurg:
 	// switch to g0
 	MOVQ	DX, g(CX)
 	MOVQ	(g_sched+gobuf_sp)(DX), BX
-	// make it look like mstart called onM on g0, to stop traceback
+	// make it look like mstart called systemstack on g0, to stop traceback
 	SUBQ	$8, BX
 	MOVQ	$runtime·mstart(SB), DX
 	MOVQ	DX, 0(BX)
@@ -267,7 +253,7 @@ oncurg:
 	MOVQ	$0, (g_sched+gobuf_sp)(AX)
 	RET
 
-onm:
+noswitch:
 	// already on m stack, just call directly
 	MOVQ	DI, DX
 	MOVQ	0(DI), DI
@@ -727,7 +713,7 @@ needm:
 	// the same SP back to m->sched.sp. That seems redundant,
 	// but if an unrecovered panic happens, unwindm will
 	// restore the g->sched.sp from the stack location
-	// and then onM will try to use it. If we don't set it here,
+	// and then systemstack will try to use it. If we don't set it here,
 	// that restored SP will be uninitialized (typically 0) and
 	// will not be usable.
 	MOVQ	m_g0(BP), SI

commit 15ced2d00832dd9129b4ee0ac53b5367ade24c13
Author: Russ Cox <rsc@golang.org>
Date:   Tue Nov 11 17:06:22 2014 -0500

    [dev.cc] runtime: convert assembly files for C to Go transition
    
    The main change is that #include "zasm_GOOS_GOARCH.h"
    is now #include "go_asm.h" and/or #include "go_tls.h".
    
    Also, because C StackGuard is now Go _StackGuard,
    the assembly name changes from const_StackGuard to
    const__StackGuard.
    
    In asm_$GOARCH.s, add new function getg, formerly
    implemented in C.
    
    The renamed atomics now have Go wrappers, to get
    escape analysis annotations right. Those wrappers
    are in CL 174860043.
    
    LGTM=r, aram
    R=r, aram
    CC=austin, dvyukov, golang-codereviews, iant, khr
    https://golang.org/cl/168510043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 39d7c78f23..9a74a5310b 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2,7 +2,8 @@
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 
-#include "zasm_GOOS_GOARCH.h"
+#include "go_asm.h"
+#include "go_tls.h"
 #include "funcdata.h"
 #include "textflag.h"
 
@@ -47,7 +48,7 @@ nocpuinfo:
 	// update stackguard after _cgo_init
 	MOVQ	$runtime·g0(SB), CX
 	MOVQ	(g_stack+stack_lo)(CX), AX
-	ADDQ	$const_StackGuard, AX
+	ADDQ	$const__StackGuard, AX
 	MOVQ	AX, g_stackguard0(CX)
 	MOVQ	AX, g_stackguard1(CX)
 
@@ -489,7 +490,7 @@ TEXT runtime·atomicstoreuintptr(SB), NOSPLIT, $0-16
 //		return 1;
 //	} else
 //		return 0;
-TEXT runtime·casp(SB), NOSPLIT, $0-25
+TEXT runtime·casp1(SB), NOSPLIT, $0-25
 	MOVQ	ptr+0(FP), BX
 	MOVQ	old+8(FP), AX
 	MOVQ	new+16(FP), CX
@@ -541,7 +542,7 @@ TEXT runtime·xchg64(SB), NOSPLIT, $0-24
 	MOVQ	AX, ret+16(FP)
 	RET
 
-TEXT runtime·xchgp(SB), NOSPLIT, $0-24
+TEXT runtime·xchgp1(SB), NOSPLIT, $0-24
 	MOVQ	ptr+0(FP), BX
 	MOVQ	new+8(FP), AX
 	XCHGQ	AX, 0(BX)
@@ -559,7 +560,7 @@ again:
 	JNZ	again
 	RET
 
-TEXT runtime·atomicstorep(SB), NOSPLIT, $0-16
+TEXT runtime·atomicstorep1(SB), NOSPLIT, $0-16
 	MOVQ	ptr+0(FP), BX
 	MOVQ	val+8(FP), AX
 	XCHGQ	AX, 0(BX)
@@ -2235,3 +2236,9 @@ TEXT _cgo_topofstack(SB),NOSPLIT,$0
 TEXT runtime·goexit(SB),NOSPLIT,$0-0
 	BYTE	$0x90	// NOP
 	CALL	runtime·goexit1(SB)	// does not return
+
+TEXT runtime·getg(SB),NOSPLIT,$0-8
+	get_tls(CX)
+	MOVQ	g(CX), AX
+	MOVQ	AX, ret+0(FP)
+	RET

commit 31b1207fde70531cd5092d83c01ad5e0f07eb951
Merge: 84f7ac98f7 b802240300
Author: Austin Clements <austin@google.com>
Date:   Mon Nov 3 10:53:11 2014 -0500

    [dev.power64] all: merge default into dev.power64
    
    Trivial merge except for src/runtime/asm_power64x.s and
    src/runtime/signal_power64x.c
    
    LGTM=rsc
    R=rsc
    CC=golang-codereviews
    https://golang.org/cl/168950044

commit a5a07331444f9b48a5e09728e3d0085cfbfb2222
Author: Russ Cox <rsc@golang.org>
Date:   Wed Oct 29 20:37:44 2014 -0400

    runtime: change top-most return PC from goexit to goexit+PCQuantum
    
    If you get a stack of PCs from Callers, it would be expected
    that every PC is immediately after a call instruction, so to find
    the line of the call, you look up the line for PC-1.
    CL 163550043 now explicitly documents that.
    
    The most common exception to this is the top-most return PC
    on the stack, which is the entry address of the runtime.goexit
    function. Subtracting 1 from that PC will end up in a different
    function entirely.
    
    To remove this special case, make the top-most return PC
    goexit+PCQuantum and then implement goexit in assembly
    so that the first instruction can be skipped.
    
    Fixes #7690.
    
    LGTM=r
    R=r
    CC=golang-codereviews
    https://golang.org/cl/170720043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a9b082beb8..39d7c78f23 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2229,3 +2229,9 @@ TEXT _cgo_topofstack(SB),NOSPLIT,$0
 	MOVQ	m_curg(AX), AX
 	MOVQ	(g_stack+stack_hi)(AX), AX
 	RET
+
+// The top-most function running on a goroutine
+// returns to goexit+PCQuantum.
+TEXT runtime·goexit(SB),NOSPLIT,$0-0
+	BYTE	$0x90	// NOP
+	CALL	runtime·goexit1(SB)	// does not return

commit 599199fd9f53dc91ccc3f29c41cc318052668f70
Merge: b55791e200 3bbc8638d5
Author: Russ Cox <rsc@golang.org>
Date:   Wed Oct 29 11:45:01 2014 -0400

    [dev.power64] all: merge default (dd5014ed9b01) into dev.power64
    
    Still passes on amd64.
    
    LGTM=austin
    R=austin
    CC=golang-codereviews
    https://golang.org/cl/165110043

commit c4efaac15daac5e15092532dcc7ca9c30a0e0fbc
Author: Russ Cox <rsc@golang.org>
Date:   Tue Oct 28 21:53:09 2014 -0400

    runtime: fix unrecovered panic on external thread
    
    Fixes #8588.
    
    LGTM=austin
    R=austin
    CC=golang-codereviews, khr
    https://golang.org/cl/159700044

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 709834180e..a9b082beb8 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -717,6 +717,20 @@ needm:
 	get_tls(CX)
 	MOVQ	g(CX), BP
 	MOVQ	g_m(BP), BP
+	
+	// Set m->sched.sp = SP, so that if a panic happens
+	// during the function we are about to execute, it will
+	// have a valid SP to run on the g0 stack.
+	// The next few lines (after the havem label)
+	// will save this SP onto the stack and then write
+	// the same SP back to m->sched.sp. That seems redundant,
+	// but if an unrecovered panic happens, unwindm will
+	// restore the g->sched.sp from the stack location
+	// and then onM will try to use it. If we don't set it here,
+	// that restored SP will be uninitialized (typically 0) and
+	// will not be usable.
+	MOVQ	m_g0(BP), SI
+	MOVQ	SP, (g_sched+gobuf_sp)(SI)
 
 havem:
 	// Now there's a valid m, and we're running on its m->g0.

commit b55791e2008703b33883e48e72a347ba07a65486
Author: Russ Cox <rsc@golang.org>
Date:   Tue Oct 28 21:50:16 2014 -0400

    [dev.power64] cmd/5a, cmd/6a, cmd/8a, cmd/9a: make labels function-scoped
    
    I removed support for jumping between functions years ago,
    as part of doing the instruction layout for each function separately.
    
    Given that, it makes sense to treat labels as function-scoped.
    This lets each function have its own 'loop' label, for example.
    
    Makes the assembly much cleaner and removes the last
    reason anyone would reach for the 123(PC) form instead.
    
    Note that this is on the dev.power64 branch, but it changes all
    the assemblers. The change will ship in Go 1.5 (perhaps after
    being ported into the new assembler).
    
    Came up as part of CL 167730043.
    
    LGTM=r
    R=r
    CC=austin, dave, golang-codereviews, minux
    https://golang.org/cl/159670043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 709834180e..7a0fdfa73a 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -461,11 +461,11 @@ TEXT runtime·cas64(SB), NOSPLIT, $0-25
 	MOVQ	new+16(FP), CX
 	LOCK
 	CMPXCHGQ	CX, 0(BX)
-	JNZ	cas64_fail
+	JNZ	fail
 	MOVL	$1, AX
 	MOVB	AX, ret+24(FP)
 	RET
-cas64_fail:
+fail:
 	MOVL	$0, AX
 	MOVB	AX, ret+24(FP)
 	RET
@@ -876,24 +876,24 @@ TEXT runtime·aeshashbody(SB),NOSPLIT,$0-32
 	MOVO	runtime·aeskeysched+0(SB), X2
 	MOVO	runtime·aeskeysched+16(SB), X3
 	CMPQ	CX, $16
-	JB	aessmall
-aesloop:
+	JB	small
+loop:
 	CMPQ	CX, $16
-	JBE	aesloopend
+	JBE	loopend
 	MOVOU	(AX), X1
 	AESENC	X2, X0
 	AESENC	X1, X0
 	SUBQ	$16, CX
 	ADDQ	$16, AX
-	JMP	aesloop
+	JMP	loop
 // 1-16 bytes remaining
-aesloopend:
+loopend:
 	// This load may overlap with the previous load above.
 	// We'll hash some bytes twice, but that's ok.
 	MOVOU	-16(AX)(CX*1), X1
 	JMP	partial
 // 0-15 bytes
-aessmall:
+small:
 	TESTQ	CX, CX
 	JE	finalize	// 0 bytes
 
@@ -1036,18 +1036,18 @@ TEXT runtime·eqstring(SB),NOSPLIT,$0-33
 	MOVQ	s1len+8(FP), AX
 	MOVQ	s2len+24(FP), BX
 	CMPQ	AX, BX
-	JNE	different
+	JNE	noteq
 	MOVQ	s1str+0(FP), SI
 	MOVQ	s2str+16(FP), DI
 	CMPQ	SI, DI
-	JEQ	same
+	JEQ	eq
 	CALL	runtime·memeqbody(SB)
 	MOVB	AX, v+32(FP)
 	RET
-same:
+eq:
 	MOVB	$1, v+32(FP)
 	RET
-different:
+noteq:
 	MOVB	$0, v+32(FP)
 	RET
 
@@ -1170,29 +1170,29 @@ TEXT runtime·cmpbytes(SB),NOSPLIT,$0-56
 //   AX = 1/0/-1
 TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
 	CMPQ	SI, DI
-	JEQ	cmp_allsame
+	JEQ	allsame
 	CMPQ	BX, DX
 	MOVQ	DX, BP
 	CMOVQLT	BX, BP // BP = min(alen, blen) = # of bytes to compare
 	CMPQ	BP, $8
-	JB	cmp_small
+	JB	small
 
-cmp_loop:
+loop:
 	CMPQ	BP, $16
-	JBE	cmp_0through16
+	JBE	_0through16
 	MOVOU	(SI), X0
 	MOVOU	(DI), X1
 	PCMPEQB X0, X1
 	PMOVMSKB X1, AX
 	XORQ	$0xffff, AX	// convert EQ to NE
-	JNE	cmp_diff16	// branch if at least one byte is not equal
+	JNE	diff16	// branch if at least one byte is not equal
 	ADDQ	$16, SI
 	ADDQ	$16, DI
 	SUBQ	$16, BP
-	JMP	cmp_loop
+	JMP	loop
 	
 	// AX = bit mask of differences
-cmp_diff16:
+diff16:
 	BSFQ	AX, BX	// index of first byte that differs
 	XORQ	AX, AX
 	MOVB	(SI)(BX*1), CX
@@ -1202,21 +1202,21 @@ cmp_diff16:
 	RET
 
 	// 0 through 16 bytes left, alen>=8, blen>=8
-cmp_0through16:
+_0through16:
 	CMPQ	BP, $8
-	JBE	cmp_0through8
+	JBE	_0through8
 	MOVQ	(SI), AX
 	MOVQ	(DI), CX
 	CMPQ	AX, CX
-	JNE	cmp_diff8
-cmp_0through8:
+	JNE	diff8
+_0through8:
 	MOVQ	-8(SI)(BP*1), AX
 	MOVQ	-8(DI)(BP*1), CX
 	CMPQ	AX, CX
-	JEQ	cmp_allsame
+	JEQ	allsame
 
 	// AX and CX contain parts of a and b that differ.
-cmp_diff8:
+diff8:
 	BSWAPQ	AX	// reverse order of bytes
 	BSWAPQ	CX
 	XORQ	AX, CX
@@ -1227,44 +1227,44 @@ cmp_diff8:
 	RET
 
 	// 0-7 bytes in common
-cmp_small:
+small:
 	LEAQ	(BP*8), CX	// bytes left -> bits left
 	NEGQ	CX		//  - bits lift (== 64 - bits left mod 64)
-	JEQ	cmp_allsame
+	JEQ	allsame
 
 	// load bytes of a into high bytes of AX
 	CMPB	SI, $0xf8
-	JA	cmp_si_high
+	JA	si_high
 	MOVQ	(SI), SI
-	JMP	cmp_si_finish
-cmp_si_high:
+	JMP	si_finish
+si_high:
 	MOVQ	-8(SI)(BP*1), SI
 	SHRQ	CX, SI
-cmp_si_finish:
+si_finish:
 	SHLQ	CX, SI
 
 	// load bytes of b in to high bytes of BX
 	CMPB	DI, $0xf8
-	JA	cmp_di_high
+	JA	di_high
 	MOVQ	(DI), DI
-	JMP	cmp_di_finish
-cmp_di_high:
+	JMP	di_finish
+di_high:
 	MOVQ	-8(DI)(BP*1), DI
 	SHRQ	CX, DI
-cmp_di_finish:
+di_finish:
 	SHLQ	CX, DI
 
 	BSWAPQ	SI	// reverse order of bytes
 	BSWAPQ	DI
 	XORQ	SI, DI	// find bit differences
-	JEQ	cmp_allsame
+	JEQ	allsame
 	BSRQ	DI, CX	// index of highest bit difference
 	SHRQ	CX, SI	// move a's bit to bottom
 	ANDQ	$1, SI	// mask bit
 	LEAQ	-1(SI*2), AX // 1/0 => +1/-1
 	RET
 
-cmp_allsame:
+allsame:
 	XORQ	AX, AX
 	XORQ	CX, CX
 	CMPQ	BX, DX
@@ -1299,7 +1299,7 @@ TEXT runtime·indexbytebody(SB),NOSPLIT,$0
 	MOVQ SI, DI
 
 	CMPQ BX, $16
-	JLT indexbyte_small
+	JLT small
 
 	// round up to first 16-byte boundary
 	TESTQ $15, SI
@@ -1357,7 +1357,7 @@ failure:
 	RET
 
 // handle for lengths < 16
-indexbyte_small:
+small:
 	MOVQ BX, CX
 	REPN; SCASB
 	JZ success

commit b60d5e12e983e7e48ffab47b15e372bd23fbad98
Author: Keith Randall <khr@golang.org>
Date:   Tue Oct 21 14:46:07 2014 -0700

    runtime: warn that cputicks() might not be monotonic.
    
    Get rid of gocputicks(), it is no longer used.
    
    LGTM=bradfitz, dave
    R=golang-codereviews, bradfitz, dave, minux
    CC=golang-codereviews
    https://golang.org/cl/161110044

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 2ee3312086..709834180e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -855,13 +855,6 @@ TEXT runtime·cputicks(SB),NOSPLIT,$0-0
 	MOVQ	AX, ret+0(FP)
 	RET
 
-TEXT runtime·gocputicks(SB),NOSPLIT,$0-8
-	RDTSC
-	SHLQ    $32, DX
-	ADDQ    DX, AX
-	MOVQ    AX, ret+0(FP)
-	RET
-
 // hash function using AES hardware instructions
 TEXT runtime·aeshash(SB),NOSPLIT,$0-32
 	MOVQ	p+0(FP), AX	// ptr to data

commit cb6f5ac0b0239b6041267d3e9898390f57fc9eb1
Author: Russ Cox <rsc@golang.org>
Date:   Wed Oct 15 13:12:16 2014 -0400

    runtime: remove hand-generated ptr bitmaps for reflectcall
    
    A Go prototype can be used instead now, and the compiler
    will do a better job than we will doing it by hand.
    (We got it wrong in amd64p32, causing the current build
    breakage.)
    
    The auto-prototype-matching only applies to functions
    without an explicit package path, so the TEXT lines for
    reflectcall and callXX are s/runtime·/·/.
    
    LGTM=khr
    R=khr
    CC=golang-codereviews, iant, r
    https://golang.org/cl/153600043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e21270d8cc..2ee3312086 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -339,11 +339,11 @@ TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
 #define DISPATCH(NAME,MAXSIZE)		\
 	CMPQ	CX, $MAXSIZE;		\
 	JA	3(PC);			\
-	MOVQ	$NAME(SB), AX;	\
+	MOVQ	$NAME(SB), AX;		\
 	JMP	AX
 // Note: can't just "JMP NAME(SB)" - bad inlining results.
 
-TEXT runtime·reflectcall(SB), NOSPLIT, $0-24
+TEXT ·reflectcall(SB), NOSPLIT, $0-24
 	MOVLQZX argsize+16(FP), CX
 	DISPATCH(runtime·call16, 16)
 	DISPATCH(runtime·call32, 32)
@@ -375,21 +375,9 @@ TEXT runtime·reflectcall(SB), NOSPLIT, $0-24
 	MOVQ	$runtime·badreflectcall(SB), AX
 	JMP	AX
 
-// Argument map for the callXX frames.  Each has one stack map.
-DATA gcargs_reflectcall<>+0x00(SB)/4, $1  // 1 stackmap
-DATA gcargs_reflectcall<>+0x04(SB)/4, $6  // 3 words
-DATA gcargs_reflectcall<>+0x08(SB)/1, $(const_BitsPointer+(const_BitsPointer<<2)+(const_BitsScalar<<4))
-GLOBL gcargs_reflectcall<>(SB),RODATA,$12
-
-// callXX frames have no locals
-DATA gclocals_reflectcall<>+0x00(SB)/4, $1  // 1 stackmap
-DATA gclocals_reflectcall<>+0x04(SB)/4, $0  // 0 locals
-GLOBL gclocals_reflectcall<>(SB),RODATA,$8
-
 #define CALLFN(NAME,MAXSIZE)			\
 TEXT NAME(SB), WRAPPER, $MAXSIZE-24;		\
-	FUNCDATA $FUNCDATA_ArgsPointerMaps,gcargs_reflectcall<>(SB);	\
-	FUNCDATA $FUNCDATA_LocalsPointerMaps,gclocals_reflectcall<>(SB);\
+	NO_LOCAL_POINTERS;			\
 	/* copy arguments to stack */		\
 	MOVQ	argptr+8(FP), SI;		\
 	MOVLQZX argsize+16(FP), CX;		\
@@ -410,33 +398,33 @@ TEXT NAME(SB), WRAPPER, $MAXSIZE-24;		\
 	REP;MOVSB;				\
 	RET
 
-CALLFN(runtime·call16, 16)
-CALLFN(runtime·call32, 32)
-CALLFN(runtime·call64, 64)
-CALLFN(runtime·call128, 128)
-CALLFN(runtime·call256, 256)
-CALLFN(runtime·call512, 512)
-CALLFN(runtime·call1024, 1024)
-CALLFN(runtime·call2048, 2048)
-CALLFN(runtime·call4096, 4096)
-CALLFN(runtime·call8192, 8192)
-CALLFN(runtime·call16384, 16384)
-CALLFN(runtime·call32768, 32768)
-CALLFN(runtime·call65536, 65536)
-CALLFN(runtime·call131072, 131072)
-CALLFN(runtime·call262144, 262144)
-CALLFN(runtime·call524288, 524288)
-CALLFN(runtime·call1048576, 1048576)
-CALLFN(runtime·call2097152, 2097152)
-CALLFN(runtime·call4194304, 4194304)
-CALLFN(runtime·call8388608, 8388608)
-CALLFN(runtime·call16777216, 16777216)
-CALLFN(runtime·call33554432, 33554432)
-CALLFN(runtime·call67108864, 67108864)
-CALLFN(runtime·call134217728, 134217728)
-CALLFN(runtime·call268435456, 268435456)
-CALLFN(runtime·call536870912, 536870912)
-CALLFN(runtime·call1073741824, 1073741824)
+CALLFN(·call16, 16)
+CALLFN(·call32, 32)
+CALLFN(·call64, 64)
+CALLFN(·call128, 128)
+CALLFN(·call256, 256)
+CALLFN(·call512, 512)
+CALLFN(·call1024, 1024)
+CALLFN(·call2048, 2048)
+CALLFN(·call4096, 4096)
+CALLFN(·call8192, 8192)
+CALLFN(·call16384, 16384)
+CALLFN(·call32768, 32768)
+CALLFN(·call65536, 65536)
+CALLFN(·call131072, 131072)
+CALLFN(·call262144, 262144)
+CALLFN(·call524288, 524288)
+CALLFN(·call1048576, 1048576)
+CALLFN(·call2097152, 2097152)
+CALLFN(·call4194304, 4194304)
+CALLFN(·call8388608, 8388608)
+CALLFN(·call16777216, 16777216)
+CALLFN(·call33554432, 33554432)
+CALLFN(·call67108864, 67108864)
+CALLFN(·call134217728, 134217728)
+CALLFN(·call268435456, 268435456)
+CALLFN(·call536870912, 536870912)
+CALLFN(·call1073741824, 1073741824)
 
 // bool cas(int32 *val, int32 old, int32 new)
 // Atomically:

commit 2b1659b57d6e021029636ee39b4a30c4f9074c6c
Author: Russ Cox <rsc@golang.org>
Date:   Tue Oct 7 23:27:25 2014 -0400

    runtime: change Windows M.thread from void* to uintptr
    
    It appears to be an opaque bit pattern more than a pointer.
    The Go garbage collector has discovered that for m0
    it is set to 0x4c.
    
    Should fix Windows build.
    
    TBR=brainman
    CC=golang-codereviews
    https://golang.org/cl/149640043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 3f7f608410..e21270d8cc 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -491,6 +491,9 @@ TEXT runtime·atomicloaduintptr(SB), NOSPLIT, $0-16
 TEXT runtime·atomicloaduint(SB), NOSPLIT, $0-16
 	JMP	runtime·atomicload64(SB)
 
+TEXT runtime·atomicstoreuintptr(SB), NOSPLIT, $0-16
+	JMP	runtime·atomicstore64(SB)
+
 // bool casp(void **val, void *old, void *new)
 // Atomically:
 //	if(*val == old){

commit 1aa65fe8d4c0ebdd754480d281f378fcd1c42cea
Author: Keith Randall <khr@golang.org>
Date:   Thu Sep 25 08:37:04 2014 -0700

    runtime: add load_g call in arm callback.
    
    Need to restore the g register.  Somehow this line vaporized from
    CL 144130043.  Also cgo_topofstack -> _cgo_topofstack, that vaporized also.
    
    TBR=rsc
    CC=golang-codereviews
    https://golang.org/cl/150940044

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index b4c6c6bdca..3f7f608410 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2224,7 +2224,7 @@ TEXT runtime·return0(SB), NOSPLIT, $0
 
 // Called from cgo wrappers, this function returns g->m->curg.stack.hi.
 // Must obey the gcc calling convention.
-TEXT cgo_topofstack(SB),NOSPLIT,$0
+TEXT _cgo_topofstack(SB),NOSPLIT,$0
 	get_tls(CX)
 	MOVQ	g(CX), AX
 	MOVQ	g_m(AX), AX

commit 1b6807bb069c528447270c3d6c66c5c7597f388f
Author: Keith Randall <khr@golang.org>
Date:   Thu Sep 25 07:59:01 2014 -0700

    cgo: adjust return value location to account for stack copies.
    
    During a cgo call, the stack can be copied.  This copy invalidates
    the pointer that cgo has into the return value area.  To fix this
    problem, pass the address of the location containing the stack
    top value (which is in the G struct).  For cgo functions which
    return values, read the stktop before and after the cgo call to
    compute the adjustment necessary to write the return value.
    
    Fixes #8771
    
    LGTM=iant, rsc
    R=iant, rsc, khr
    CC=golang-codereviews
    https://golang.org/cl/144130043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 7304d79a2f..b4c6c6bdca 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2220,3 +2220,14 @@ TEXT runtime·fastrand1(SB), NOSPLIT, $0-4
 TEXT runtime·return0(SB), NOSPLIT, $0
 	MOVL	$0, AX
 	RET
+
+
+// Called from cgo wrappers, this function returns g->m->curg.stack.hi.
+// Must obey the gcc calling convention.
+TEXT cgo_topofstack(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), AX
+	MOVQ	g_m(AX), AX
+	MOVQ	m_curg(AX), AX
+	MOVQ	(g_stack+stack_hi)(AX), AX
+	RET

commit 193daab9889708f7a20ff46efe0fa4b2bf0468d3
Author: Russ Cox <rsc@golang.org>
Date:   Wed Sep 24 16:55:26 2014 -0400

    cmd/cc, cmd/ld, runtime: disallow conservative data/bss objects
    
    In linker, refuse to write conservative (array of pointers) as the
    garbage collection type for any variable in the data/bss GC program.
    
    In the linker, attach the Go type to an already-read C declaration
    during dedup. This gives us Go types for C globals for free as long
    as the cmd/dist-generated Go code contains the declaration.
    (Most runtime C declarations have a corresponding Go declaration.
    Both are bss declarations and so the linker dedups them.)
    
    In cmd/dist, add a few more C files to the auto-Go-declaration list
    in order to get Go type information for the C declarations into the linker.
    
    In C compiler, mark all non-pointer-containing global declarations
    and all string data as NOPTR. This allows them to exist in C files
    without any corresponding Go declaration. Count C function pointers
    as "non-pointer-containing", since we have no heap-allocated C functions.
    
    In runtime, add NOPTR to the remaining pointer-containing declarations,
    none of which refer to Go heap objects.
    
    In runtime, also move os.Args and syscall.envs data into runtime-owned
    variables. Otherwise, in programs that do not import os or syscall, the
    runtime variables named os.Args and syscall.envs will be missing type
    information.
    
    I believe that this CL eliminates the final source of conservative GC scanning
    in non-SWIG Go programs, and therefore...
    
    Fixes #909.
    
    LGTM=iant
    R=iant
    CC=golang-codereviews
    https://golang.org/cl/149770043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 44159bb57e..7304d79a2f 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -871,8 +871,6 @@ TEXT runtime·gocputicks(SB),NOSPLIT,$0-8
 	MOVQ    AX, ret+0(FP)
 	RET
 
-GLOBL runtime·tls0(SB), $64
-
 // hash function using AES hardware instructions
 TEXT runtime·aeshash(SB),NOSPLIT,$0-32
 	MOVQ	p+0(FP), AX	// ptr to data

commit 653fb6d872e31b05441f313911684d5cd351597e
Author: Russ Cox <rsc@golang.org>
Date:   Tue Sep 16 17:39:55 2014 -0400

    liblink: make GO_ARGS the default for functions beginning with ·
    
    If there is a leading ·, assume there is a Go prototype and
    attach the Go prototype information to the function.
    If the function is not called from Go and does not need a
    Go prototype, it can be made file-local instead (using name<>(SB)).
    
    This fixes the current BSD build failures, by giving functions like
    sync/atomic.StoreUint32 argument stack map information.
    
    Fixes #8753.
    
    LGTM=khr, iant
    R=golang-codereviews, iant, khr, bradfitz
    CC=golang-codereviews, r, rlh
    https://golang.org/cl/142150043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index da29f61ed8..44159bb57e 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -623,15 +623,13 @@ TEXT gosave<>(SB),NOSPLIT,$0
 // Call fn(arg) on the scheduler stack,
 // aligned appropriately for the gcc ABI.
 // See cgocall.c for more details.
-TEXT runtime·asmcgocall(SB),NOSPLIT,$0-16
-	GO_ARGS
+TEXT ·asmcgocall(SB),NOSPLIT,$0-16
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX
 	CALL	asmcgocall<>(SB)
 	RET
 
-TEXT runtime·asmcgocall_errno(SB),NOSPLIT,$0-20
-	GO_ARGS
+TEXT ·asmcgocall_errno(SB),NOSPLIT,$0-20
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX
 	CALL	asmcgocall<>(SB)
@@ -700,8 +698,7 @@ TEXT runtime·cgocallback(SB),NOSPLIT,$24-24
 
 // cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize)
 // See cgocall.c for more details.
-TEXT runtime·cgocallback_gofunc(SB),NOSPLIT,$8-24
-	GO_ARGS
+TEXT ·cgocallback_gofunc(SB),NOSPLIT,$8-24
 	NO_LOCAL_POINTERS
 
 	// If g is nil, Go did not create the current thread.

commit 40dd6bf38e06bf26aa1c15438cdf0965cf778050
Author: Russ Cox <rsc@golang.org>
Date:   Sun Sep 14 13:57:28 2014 -0400

    runtime: mark asmcgocall<>(SB) as having no arguments
    
    It doesn't.
    Fixes 386 build.
    
    While we're here, mark runtime.asmcgocall as GO_ARGS,
    so that it will work with stack copying. I don't think anything
    that uses it can lead to a stack copy, but better safe than sorry.
    Certainly the runtime.asmcgocall_errno variant needs
    (and already has) GO_ARGS.
    
    TBR=iant
    CC=golang-codereviews
    https://golang.org/cl/138400043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index d5e2f56ef0..da29f61ed8 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -624,6 +624,7 @@ TEXT gosave<>(SB),NOSPLIT,$0
 // aligned appropriately for the gcc ABI.
 // See cgocall.c for more details.
 TEXT runtime·asmcgocall(SB),NOSPLIT,$0-16
+	GO_ARGS
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX
 	CALL	asmcgocall<>(SB)

commit e844f53a0198e81b359d198fc0dcf15cf01d6ed1
Author: Russ Cox <rsc@golang.org>
Date:   Fri Sep 12 07:46:11 2014 -0400

    runtime: stop scanning stack frames/args conservatively
    
    The goal here is to commit fully to having precise information
    about stack frames. If we need information we don't have,
    crash instead of assuming we should scan conservatively.
    
    Since the stack copying assumes fully precise information,
    any crashes during garbage collection that are introduced by
    this CL are crashes that could have happened during stack
    copying instead. Those are harder to find because stacks are
    copied much less often than the garbage collector is invoked.
    
    In service of that goal, remove ARGSIZE macros from
    asm_*.s, change switchtoM to have no arguments
    (it doesn't have any live arguments), and add
    args and locals information to some frames that
    can call back into Go.
    
    LGTM=khr
    R=khr, rlh
    CC=golang-codereviews
    https://golang.org/cl/137540043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 241d5feebf..d5e2f56ef0 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -98,9 +98,7 @@ ok:
 	MOVQ	$runtime·main·f(SB), BP		// entry
 	PUSHQ	BP
 	PUSHQ	$0			// arg size
-	ARGSIZE(16)
 	CALL	runtime·newproc(SB)
-	ARGSIZE(-1)
 	POPQ	AX
 	POPQ	AX
 
@@ -183,7 +181,6 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 	MOVQ	SI, g(CX)	// g = m->g0
 	MOVQ	(g_sched+gobuf_sp)(SI), SP	// sp = m->g0->sched.sp
 	PUSHQ	AX
-	ARGSIZE(8)
 	MOVQ	DI, DX
 	MOVQ	0(DI), DI
 	CALL	DI
@@ -197,7 +194,7 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 // lives at the bottom of the G stack from the one that lives
 // at the top of the M stack because the one at the top of
 // the M stack terminates the stack walk (see topofstack()).
-TEXT runtime·switchtoM(SB), NOSPLIT, $0-8
+TEXT runtime·switchtoM(SB), NOSPLIT, $0-0
 	RET
 
 // func onM_signalok(fn func())
@@ -255,7 +252,6 @@ oncurg:
 	MOVQ	BX, SP
 
 	// call target function
-	ARGSIZE(0)
 	MOVQ	DI, DX
 	MOVQ	0(DI), DI
 	CALL	DI
@@ -634,6 +630,7 @@ TEXT runtime·asmcgocall(SB),NOSPLIT,$0-16
 	RET
 
 TEXT runtime·asmcgocall_errno(SB),NOSPLIT,$0-20
+	GO_ARGS
 	MOVQ	fn+0(FP), AX
 	MOVQ	arg+8(FP), BX
 	CALL	asmcgocall<>(SB)
@@ -703,6 +700,9 @@ TEXT runtime·cgocallback(SB),NOSPLIT,$24-24
 // cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize)
 // See cgocall.c for more details.
 TEXT runtime·cgocallback_gofunc(SB),NOSPLIT,$8-24
+	GO_ARGS
+	NO_LOCAL_POINTERS
+
 	// If g is nil, Go did not create the current thread.
 	// Call needm to obtain one m for temporary use.
 	// In this case, we're running on the thread stack, so there's

commit 47f251c1cede13dba3e478b2528fb89eede1b566
Author: Keith Randall <khr@golang.org>
Date:   Thu Sep 11 20:36:23 2014 -0700

    runtime: fix cgo to handle the case where the G stack is copied.
    
    Tests will come in a separate CL after the funcdata stuff is resolved.
    
    Update #8696
    
    LGTM=iant, rsc
    R=rsc, iant
    CC=golang-codereviews
    https://golang.org/cl/138330045

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index bf0f490ae3..241d5feebf 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -670,7 +670,9 @@ nosave:
 	SUBQ	$64, SP
 	ANDQ	$~15, SP	// alignment for gcc ABI
 	MOVQ	DI, 48(SP)	// save g
-	MOVQ	DX, 40(SP)	// save SP
+	MOVQ	(g_stack+stack_hi)(DI), DI
+	SUBQ	DX, DI
+	MOVQ	DI, 40(SP)	// save depth in stack (can't just save SP, as stack might be copied during a callback)
 	MOVQ	BX, DI		// DI = first argument in AMD64 ABI
 	MOVQ	BX, CX		// CX = first argument in Win64
 	CALL	AX
@@ -678,8 +680,10 @@ nosave:
 	// Restore registers, g, stack pointer.
 	get_tls(CX)
 	MOVQ	48(SP), DI
+	MOVQ	(g_stack+stack_hi)(DI), SI
+	SUBQ	40(SP), SI
 	MOVQ	DI, g(CX)
-	MOVQ	40(SP), SP
+	MOVQ	SI, SP
 	RET
 
 // cgocallback(void (*fn)(void*), void *frame, uintptr framesize)

commit 1d550b87dbbe711a1bf2e54e0ba065a27165d2c1
Author: Russ Cox <rsc@golang.org>
Date:   Thu Sep 11 12:08:30 2014 -0400

    runtime: allow crash from gsignal stack
    
    The uses of onM in dopanic/startpanic are okay even from the signal stack.
    
    Fixes #8666.
    
    LGTM=khr
    R=khr
    CC=golang-codereviews
    https://golang.org/cl/134710043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index e5702d074c..bf0f490ae3 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -200,6 +200,23 @@ TEXT runtime·mcall(SB), NOSPLIT, $0-8
 TEXT runtime·switchtoM(SB), NOSPLIT, $0-8
 	RET
 
+// func onM_signalok(fn func())
+TEXT runtime·onM_signalok(SB), NOSPLIT, $0-8
+	get_tls(CX)
+	MOVQ	g(CX), AX	// AX = g
+	MOVQ	g_m(AX), BX	// BX = m
+	MOVQ	m_gsignal(BX), DX	// DX = gsignal
+	CMPQ	AX, DX
+	JEQ	ongsignal
+	JMP	runtime·onM(SB)
+
+ongsignal:
+	MOVQ	fn+0(FP), DI	// DI = fn
+	MOVQ	DI, DX
+	MOVQ	0(DI), DI
+	CALL	DI
+	RET
+
 // func onM(fn func())
 TEXT runtime·onM(SB), NOSPLIT, $0-8
 	MOVQ	fn+0(FP), DI	// DI = fn

commit 2302b21bbeef1c0bca8fff62c2d34cd301ce38a0
Author: Anthony Martin <ality@pbrane.org>
Date:   Wed Sep 10 06:25:05 2014 -0700

    runtime: stop plan9/amd64 build from crashing
    
    LGTM=iant
    R=rsc, 0intro, alex.brainman, iant
    CC=golang-codereviews
    https://golang.org/cl/140460044

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index 1a106dc1f1..e5702d074c 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -272,6 +272,7 @@ onm:
 // record an argument size. For that purpose, it has no arguments.
 TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	// Cannot grow scheduler stack (m->g0).
+	get_tls(CX)
 	MOVQ	g(CX), BX
 	MOVQ	g_m(BX), BX
 	MOVQ	m_g0(BX), SI

commit 1a5e394ab74672f59dd10623717fc3e08b17f0ab
Author: Keith Randall <khr@golang.org>
Date:   Tue Sep 9 14:32:53 2014 -0700

    runtime: more cleanups
    
    Move timenow thunk into time.s
    Move declarations for generic c/asm services into stubs.go
    
    LGTM=bradfitz
    R=golang-codereviews, bradfitz
    CC=golang-codereviews
    https://golang.org/cl/137360043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a32e03e4ee..1a106dc1f1 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -2186,9 +2186,6 @@ TEXT runtime·duffcopy(SB), NOSPLIT, $0-0
 
 	RET
 
-TEXT runtime·timenow(SB), NOSPLIT, $0-0
-	JMP	time·now(SB)
-
 TEXT runtime·fastrand1(SB), NOSPLIT, $0-4
 	get_tls(CX)
 	MOVQ	g(CX), AX

commit 15b76ad94b1054ec7fd6853530bff782790b5727
Author: Russ Cox <rsc@golang.org>
Date:   Tue Sep 9 13:39:57 2014 -0400

    runtime: assume precisestack, copystack, StackCopyAlways, ScanStackByFrames
    
    Commit to stack copying for stack growth.
    
    We're carrying around a surprising amount of cruft from older schemes.
    I am confident that precise stack scans and stack copying are here to stay.
    
    Delete fallback code for when precise stack info is disabled.
    Delete fallback code for when copying stacks is disabled.
    Delete fallback code for when StackCopyAlways is disabled.
    Delete Stktop chain - there is only one stack segment now.
    Delete M.moreargp, M.moreargsize, M.moreframesize, M.cret.
    Delete G.writenbuf (unrelated, just dead).
    Delete runtime.lessstack, runtime.oldstack.
    Delete many amd64 morestack variants.
    Delete initialization of morestack frame/arg sizes (shortens split prologue!).
    
    Replace G's stackguard/stackbase/stack0/stacksize/
    syscallstack/syscallguard/forkstackguard with simple stack
    bounds (lo, hi).
    
    Update liblink, runtime/cgo for adjustments to G.
    
    LGTM=khr
    R=khr, bradfitz
    CC=golang-codereviews, iant, r
    https://golang.org/cl/137410043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index a47fb09522..a32e03e4ee 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -19,9 +19,10 @@ TEXT runtime·rt0_go(SB),NOSPLIT,$0
 	// _cgo_init may update stackguard.
 	MOVQ	$runtime·g0(SB), DI
 	LEAQ	(-64*1024+104)(SP), BX
-	MOVQ	BX, g_stackguard(DI)
 	MOVQ	BX, g_stackguard0(DI)
-	MOVQ	SP, g_stackbase(DI)
+	MOVQ	BX, g_stackguard1(DI)
+	MOVQ	BX, (g_stack+stack_lo)(DI)
+	MOVQ	SP, (g_stack+stack_hi)(DI)
 
 	// find out information about the processor we're on
 	MOVQ	$0, AX
@@ -42,13 +43,16 @@ nocpuinfo:
 	MOVQ	DI, CX	// Win64 uses CX for first parameter
 	MOVQ	$setg_gcc<>(SB), SI
 	CALL	AX
+
 	// update stackguard after _cgo_init
 	MOVQ	$runtime·g0(SB), CX
-	MOVQ	g_stackguard0(CX), AX
-	MOVQ	AX, g_stackguard(CX)
+	MOVQ	(g_stack+stack_lo)(CX), AX
+	ADDQ	$const_StackGuard, AX
+	MOVQ	AX, g_stackguard0(CX)
+	MOVQ	AX, g_stackguard1(CX)
+
 	CMPL	runtime·iswindows(SB), $0
 	JEQ ok
-
 needtls:
 	// skip TLS setup on Plan 9
 	CMPL	runtime·isplan9(SB), $1
@@ -261,7 +265,6 @@ onm:
  */
 
 // Called during function prolog when more stack is needed.
-// Caller has already done get_tls(CX); MOVQ m(CX), BX.
 //
 // The traceback routines see morestack on a g0 as being
 // the top of a stack (for example, morestack calling newstack
@@ -269,6 +272,8 @@ onm:
 // record an argument size. For that purpose, it has no arguments.
 TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	// Cannot grow scheduler stack (m->g0).
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
 	MOVQ	m_g0(BX), SI
 	CMPQ	g(CX), SI
 	JNE	2(PC)
@@ -286,7 +291,6 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	AX, (m_morebuf+gobuf_pc)(BX)
 	LEAQ	16(SP), AX	// f's caller's SP
 	MOVQ	AX, (m_morebuf+gobuf_sp)(BX)
-	MOVQ	AX, m_moreargp(BX)
 	get_tls(CX)
 	MOVQ	g(CX), SI
 	MOVQ	SI, (m_morebuf+gobuf_g)(BX)
@@ -307,6 +311,11 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	$0, 0x1003	// crash if newstack returns
 	RET
 
+// morestack but not preserving ctxt.
+TEXT runtime·morestack_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack(SB)
+
 // reflectcall: call a function with the given argument list
 // func call(f *FuncVal, arg *byte, argsize, retoffset uint32).
 // we don't have variable-sized frames, so we use a small number
@@ -415,142 +424,6 @@ CALLFN(runtime·call268435456, 268435456)
 CALLFN(runtime·call536870912, 536870912)
 CALLFN(runtime·call1073741824, 1073741824)
 
-// Return point when leaving stack.
-//
-// Lessstack can appear in stack traces for the same reason
-// as morestack; in that context, it has 0 arguments.
-TEXT runtime·lessstack(SB), NOSPLIT, $0-0
-	// Save return value in m->cret
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	MOVQ	AX, m_cret(BX)
-
-	// Call oldstack on m->g0's stack.
-	MOVQ	m_g0(BX), BP
-	MOVQ	BP, g(CX)
-	MOVQ	(g_sched+gobuf_sp)(BP), SP
-	CALL	runtime·oldstack(SB)
-	MOVQ	$0, 0x1004	// crash if oldstack returns
-	RET
-
-// morestack trampolines
-TEXT runtime·morestack00(SB),NOSPLIT,$0
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	MOVQ	$0, AX
-	MOVQ	AX, m_moreframesize(BX)
-	MOVQ	$runtime·morestack(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack01(SB),NOSPLIT,$0
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	SHLQ	$32, AX
-	MOVQ	AX, m_moreframesize(BX)
-	MOVQ	$runtime·morestack(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack10(SB),NOSPLIT,$0
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	MOVLQZX	AX, AX
-	MOVQ	AX, m_moreframesize(BX)
-	MOVQ	$runtime·morestack(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack11(SB),NOSPLIT,$0
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	MOVQ	AX, m_moreframesize(BX)
-	MOVQ	$runtime·morestack(SB), AX
-	JMP	AX
-
-// subcases of morestack01
-// with const of 8,16,...48
-TEXT runtime·morestack8(SB),NOSPLIT,$0
-	MOVQ	$1, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack16(SB),NOSPLIT,$0
-	MOVQ	$2, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack24(SB),NOSPLIT,$0
-	MOVQ	$3, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack32(SB),NOSPLIT,$0
-	MOVQ	$4, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack40(SB),NOSPLIT,$0
-	MOVQ	$5, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack48(SB),NOSPLIT,$0
-	MOVQ	$6, R8
-	MOVQ	$morestack<>(SB), AX
-	JMP	AX
-
-TEXT morestack<>(SB),NOSPLIT,$0
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_m(BX), BX
-	SHLQ	$35, R8
-	MOVQ	R8, m_moreframesize(BX)
-	MOVQ	$runtime·morestack(SB), AX
-	JMP	AX
-
-TEXT runtime·morestack00_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack00(SB)
-
-TEXT runtime·morestack01_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack01(SB)
-
-TEXT runtime·morestack10_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack10(SB)
-
-TEXT runtime·morestack11_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack11(SB)
-
-TEXT runtime·morestack8_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack8(SB)
-
-TEXT runtime·morestack16_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack16(SB)
-
-TEXT runtime·morestack24_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack24(SB)
-
-TEXT runtime·morestack32_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack32(SB)
-
-TEXT runtime·morestack40_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack40(SB)
-
-TEXT runtime·morestack48_noctxt(SB),NOSPLIT,$0
-	MOVL	$0, DX
-	JMP	runtime·morestack48(SB)
-
 // bool cas(int32 *val, int32 old, int32 new)
 // Atomically:
 //	if(*val == old){
@@ -922,14 +795,14 @@ TEXT setg_gcc<>(SB),NOSPLIT,$0
 	MOVQ	DI, g(AX)
 	RET
 
-// check that SP is in range [g->stackbase, g->stackguard)
+// check that SP is in range [g->stack.lo, g->stack.hi)
 TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
 	get_tls(CX)
 	MOVQ	g(CX), AX
-	CMPQ	g_stackbase(AX), SP
+	CMPQ	(g_stack+stack_hi)(AX), SP
 	JHI	2(PC)
 	INT	$3
-	CMPQ	SP, g_stackguard(AX)
+	CMPQ	SP, (g_stack+stack_lo)(AX)
 	JHI	2(PC)
 	INT	$3
 	RET
@@ -978,15 +851,6 @@ TEXT runtime·gocputicks(SB),NOSPLIT,$0-8
 	MOVQ    AX, ret+0(FP)
 	RET
 
-TEXT runtime·stackguard(SB),NOSPLIT,$0-16
-	MOVQ	SP, DX
-	MOVQ	DX, sp+0(FP)
-	get_tls(CX)
-	MOVQ	g(CX), BX
-	MOVQ	g_stackguard(BX), DX
-	MOVQ	DX, limit+8(FP)
-	RET
-
 GLOBL runtime·tls0(SB), $64
 
 // hash function using AES hardware instructions

commit 526319830bf0d7778226fa9ef558f51ebe67aaa6
Author: Keith Randall <khr@golang.org>
Date:   Mon Sep 8 10:14:41 2014 -0700

    runtime: a few cleanups.
    
    LGTM=bradfitz
    R=golang-codereviews, bradfitz
    CC=golang-codereviews
    https://golang.org/cl/134630043

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
index cc32ad8a18..a47fb09522 100644
--- a/src/runtime/asm_amd64.s
+++ b/src/runtime/asm_amd64.s
@@ -307,7 +307,7 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	MOVQ	$0, 0x1003	// crash if newstack returns
 	RET
 
-// reflect·call: call a function with the given argument list
+// reflectcall: call a function with the given argument list
 // func call(f *FuncVal, arg *byte, argsize, retoffset uint32).
 // we don't have variable-sized frames, so we use a small number
 // of constant-sized-frame functions to encode a few bits of size in the pc.
@@ -320,7 +320,7 @@ TEXT runtime·morestack(SB),NOSPLIT,$0-0
 	JMP	AX
 // Note: can't just "JMP NAME(SB)" - bad inlining results.
 
-TEXT reflect·call(SB), NOSPLIT, $0-24
+TEXT runtime·reflectcall(SB), NOSPLIT, $0-24
 	MOVLQZX argsize+16(FP), CX
 	DISPATCH(runtime·call16, 16)
 	DISPATCH(runtime·call32, 32)

commit c007ce824d9a4fccb148f9204e04c23ed2984b71
Author: Russ Cox <rsc@golang.org>
Date:   Mon Sep 8 00:08:51 2014 -0400

    build: move package sources from src/pkg to src
    
    Preparation was in CL 134570043.
    This CL contains only the effect of 'hg mv src/pkg/* src'.
    For more about the move, see golang.org/s/go14nopkg.

diff --git a/src/runtime/asm_amd64.s b/src/runtime/asm_amd64.s
new file mode 100644
index 0000000000..cc32ad8a18
--- /dev/null
+++ b/src/runtime/asm_amd64.s
@@ -0,0 +1,2343 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+#include "zasm_GOOS_GOARCH.h"
+#include "funcdata.h"
+#include "textflag.h"
+
+TEXT runtime·rt0_go(SB),NOSPLIT,$0
+	// copy arguments forward on an even stack
+	MOVQ	DI, AX		// argc
+	MOVQ	SI, BX		// argv
+	SUBQ	$(4*8+7), SP		// 2args 2auto
+	ANDQ	$~15, SP
+	MOVQ	AX, 16(SP)
+	MOVQ	BX, 24(SP)
+	
+	// create istack out of the given (operating system) stack.
+	// _cgo_init may update stackguard.
+	MOVQ	$runtime·g0(SB), DI
+	LEAQ	(-64*1024+104)(SP), BX
+	MOVQ	BX, g_stackguard(DI)
+	MOVQ	BX, g_stackguard0(DI)
+	MOVQ	SP, g_stackbase(DI)
+
+	// find out information about the processor we're on
+	MOVQ	$0, AX
+	CPUID
+	CMPQ	AX, $0
+	JE	nocpuinfo
+	MOVQ	$1, AX
+	CPUID
+	MOVL	CX, runtime·cpuid_ecx(SB)
+	MOVL	DX, runtime·cpuid_edx(SB)
+nocpuinfo:	
+	
+	// if there is an _cgo_init, call it.
+	MOVQ	_cgo_init(SB), AX
+	TESTQ	AX, AX
+	JZ	needtls
+	// g0 already in DI
+	MOVQ	DI, CX	// Win64 uses CX for first parameter
+	MOVQ	$setg_gcc<>(SB), SI
+	CALL	AX
+	// update stackguard after _cgo_init
+	MOVQ	$runtime·g0(SB), CX
+	MOVQ	g_stackguard0(CX), AX
+	MOVQ	AX, g_stackguard(CX)
+	CMPL	runtime·iswindows(SB), $0
+	JEQ ok
+
+needtls:
+	// skip TLS setup on Plan 9
+	CMPL	runtime·isplan9(SB), $1
+	JEQ ok
+	// skip TLS setup on Solaris
+	CMPL	runtime·issolaris(SB), $1
+	JEQ ok
+
+	LEAQ	runtime·tls0(SB), DI
+	CALL	runtime·settls(SB)
+
+	// store through it, to make sure it works
+	get_tls(BX)
+	MOVQ	$0x123, g(BX)
+	MOVQ	runtime·tls0(SB), AX
+	CMPQ	AX, $0x123
+	JEQ 2(PC)
+	MOVL	AX, 0	// abort
+ok:
+	// set the per-goroutine and per-mach "registers"
+	get_tls(BX)
+	LEAQ	runtime·g0(SB), CX
+	MOVQ	CX, g(BX)
+	LEAQ	runtime·m0(SB), AX
+
+	// save m->g0 = g0
+	MOVQ	CX, m_g0(AX)
+	// save m0 to g0->m
+	MOVQ	AX, g_m(CX)
+
+	CLD				// convention is D is always left cleared
+	CALL	runtime·check(SB)
+
+	MOVL	16(SP), AX		// copy argc
+	MOVL	AX, 0(SP)
+	MOVQ	24(SP), AX		// copy argv
+	MOVQ	AX, 8(SP)
+	CALL	runtime·args(SB)
+	CALL	runtime·osinit(SB)
+	CALL	runtime·schedinit(SB)
+
+	// create a new goroutine to start program
+	MOVQ	$runtime·main·f(SB), BP		// entry
+	PUSHQ	BP
+	PUSHQ	$0			// arg size
+	ARGSIZE(16)
+	CALL	runtime·newproc(SB)
+	ARGSIZE(-1)
+	POPQ	AX
+	POPQ	AX
+
+	// start this M
+	CALL	runtime·mstart(SB)
+
+	MOVL	$0xf1, 0xf1  // crash
+	RET
+
+DATA	runtime·main·f+0(SB)/8,$runtime·main(SB)
+GLOBL	runtime·main·f(SB),RODATA,$8
+
+TEXT runtime·breakpoint(SB),NOSPLIT,$0-0
+	BYTE	$0xcc
+	RET
+
+TEXT runtime·asminit(SB),NOSPLIT,$0-0
+	// No per-thread init.
+	RET
+
+/*
+ *  go-routine
+ */
+
+// void gosave(Gobuf*)
+// save state in Gobuf; setjmp
+TEXT runtime·gosave(SB), NOSPLIT, $0-8
+	MOVQ	buf+0(FP), AX		// gobuf
+	LEAQ	buf+0(FP), BX		// caller's SP
+	MOVQ	BX, gobuf_sp(AX)
+	MOVQ	0(SP), BX		// caller's PC
+	MOVQ	BX, gobuf_pc(AX)
+	MOVQ	$0, gobuf_ret(AX)
+	MOVQ	$0, gobuf_ctxt(AX)
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	BX, gobuf_g(AX)
+	RET
+
+// void gogo(Gobuf*)
+// restore state from Gobuf; longjmp
+TEXT runtime·gogo(SB), NOSPLIT, $0-8
+	MOVQ	buf+0(FP), BX		// gobuf
+	MOVQ	gobuf_g(BX), DX
+	MOVQ	0(DX), CX		// make sure g != nil
+	get_tls(CX)
+	MOVQ	DX, g(CX)
+	MOVQ	gobuf_sp(BX), SP	// restore SP
+	MOVQ	gobuf_ret(BX), AX
+	MOVQ	gobuf_ctxt(BX), DX
+	MOVQ	$0, gobuf_sp(BX)	// clear to help garbage collector
+	MOVQ	$0, gobuf_ret(BX)
+	MOVQ	$0, gobuf_ctxt(BX)
+	MOVQ	gobuf_pc(BX), BX
+	JMP	BX
+
+// func mcall(fn func(*g))
+// Switch to m->g0's stack, call fn(g).
+// Fn must never return.  It should gogo(&g->sched)
+// to keep running g.
+TEXT runtime·mcall(SB), NOSPLIT, $0-8
+	MOVQ	fn+0(FP), DI
+	
+	get_tls(CX)
+	MOVQ	g(CX), AX	// save state in g->sched
+	MOVQ	0(SP), BX	// caller's PC
+	MOVQ	BX, (g_sched+gobuf_pc)(AX)
+	LEAQ	fn+0(FP), BX	// caller's SP
+	MOVQ	BX, (g_sched+gobuf_sp)(AX)
+	MOVQ	AX, (g_sched+gobuf_g)(AX)
+
+	// switch to m->g0 & its stack, call fn
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVQ	m_g0(BX), SI
+	CMPQ	SI, AX	// if g == m->g0 call badmcall
+	JNE	3(PC)
+	MOVQ	$runtime·badmcall(SB), AX
+	JMP	AX
+	MOVQ	SI, g(CX)	// g = m->g0
+	MOVQ	(g_sched+gobuf_sp)(SI), SP	// sp = m->g0->sched.sp
+	PUSHQ	AX
+	ARGSIZE(8)
+	MOVQ	DI, DX
+	MOVQ	0(DI), DI
+	CALL	DI
+	POPQ	AX
+	MOVQ	$runtime·badmcall2(SB), AX
+	JMP	AX
+	RET
+
+// switchtoM is a dummy routine that onM leaves at the bottom
+// of the G stack.  We need to distinguish the routine that
+// lives at the bottom of the G stack from the one that lives
+// at the top of the M stack because the one at the top of
+// the M stack terminates the stack walk (see topofstack()).
+TEXT runtime·switchtoM(SB), NOSPLIT, $0-8
+	RET
+
+// func onM(fn func())
+TEXT runtime·onM(SB), NOSPLIT, $0-8
+	MOVQ	fn+0(FP), DI	// DI = fn
+	get_tls(CX)
+	MOVQ	g(CX), AX	// AX = g
+	MOVQ	g_m(AX), BX	// BX = m
+
+	MOVQ	m_g0(BX), DX	// DX = g0
+	CMPQ	AX, DX
+	JEQ	onm
+
+	MOVQ	m_curg(BX), BP
+	CMPQ	AX, BP
+	JEQ	oncurg
+	
+	// Not g0, not curg. Must be gsignal, but that's not allowed.
+	// Hide call from linker nosplit analysis.
+	MOVQ	$runtime·badonm(SB), AX
+	CALL	AX
+
+oncurg:
+	// save our state in g->sched.  Pretend to
+	// be switchtoM if the G stack is scanned.
+	MOVQ	$runtime·switchtoM(SB), BP
+	MOVQ	BP, (g_sched+gobuf_pc)(AX)
+	MOVQ	SP, (g_sched+gobuf_sp)(AX)
+	MOVQ	AX, (g_sched+gobuf_g)(AX)
+
+	// switch to g0
+	MOVQ	DX, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(DX), BX
+	// make it look like mstart called onM on g0, to stop traceback
+	SUBQ	$8, BX
+	MOVQ	$runtime·mstart(SB), DX
+	MOVQ	DX, 0(BX)
+	MOVQ	BX, SP
+
+	// call target function
+	ARGSIZE(0)
+	MOVQ	DI, DX
+	MOVQ	0(DI), DI
+	CALL	DI
+
+	// switch back to g
+	get_tls(CX)
+	MOVQ	g(CX), AX
+	MOVQ	g_m(AX), BX
+	MOVQ	m_curg(BX), AX
+	MOVQ	AX, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(AX), SP
+	MOVQ	$0, (g_sched+gobuf_sp)(AX)
+	RET
+
+onm:
+	// already on m stack, just call directly
+	MOVQ	DI, DX
+	MOVQ	0(DI), DI
+	CALL	DI
+	RET
+
+/*
+ * support for morestack
+ */
+
+// Called during function prolog when more stack is needed.
+// Caller has already done get_tls(CX); MOVQ m(CX), BX.
+//
+// The traceback routines see morestack on a g0 as being
+// the top of a stack (for example, morestack calling newstack
+// calling the scheduler calling newm calling gc), so we must
+// record an argument size. For that purpose, it has no arguments.
+TEXT runtime·morestack(SB),NOSPLIT,$0-0
+	// Cannot grow scheduler stack (m->g0).
+	MOVQ	m_g0(BX), SI
+	CMPQ	g(CX), SI
+	JNE	2(PC)
+	INT	$3
+
+	// Cannot grow signal stack (m->gsignal).
+	MOVQ	m_gsignal(BX), SI
+	CMPQ	g(CX), SI
+	JNE	2(PC)
+	INT	$3
+
+	// Called from f.
+	// Set m->morebuf to f's caller.
+	MOVQ	8(SP), AX	// f's caller's PC
+	MOVQ	AX, (m_morebuf+gobuf_pc)(BX)
+	LEAQ	16(SP), AX	// f's caller's SP
+	MOVQ	AX, (m_morebuf+gobuf_sp)(BX)
+	MOVQ	AX, m_moreargp(BX)
+	get_tls(CX)
+	MOVQ	g(CX), SI
+	MOVQ	SI, (m_morebuf+gobuf_g)(BX)
+
+	// Set g->sched to context in f.
+	MOVQ	0(SP), AX // f's PC
+	MOVQ	AX, (g_sched+gobuf_pc)(SI)
+	MOVQ	SI, (g_sched+gobuf_g)(SI)
+	LEAQ	8(SP), AX // f's SP
+	MOVQ	AX, (g_sched+gobuf_sp)(SI)
+	MOVQ	DX, (g_sched+gobuf_ctxt)(SI)
+
+	// Call newstack on m->g0's stack.
+	MOVQ	m_g0(BX), BP
+	MOVQ	BP, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(BP), SP
+	CALL	runtime·newstack(SB)
+	MOVQ	$0, 0x1003	// crash if newstack returns
+	RET
+
+// reflect·call: call a function with the given argument list
+// func call(f *FuncVal, arg *byte, argsize, retoffset uint32).
+// we don't have variable-sized frames, so we use a small number
+// of constant-sized-frame functions to encode a few bits of size in the pc.
+// Caution: ugly multiline assembly macros in your future!
+
+#define DISPATCH(NAME,MAXSIZE)		\
+	CMPQ	CX, $MAXSIZE;		\
+	JA	3(PC);			\
+	MOVQ	$NAME(SB), AX;	\
+	JMP	AX
+// Note: can't just "JMP NAME(SB)" - bad inlining results.
+
+TEXT reflect·call(SB), NOSPLIT, $0-24
+	MOVLQZX argsize+16(FP), CX
+	DISPATCH(runtime·call16, 16)
+	DISPATCH(runtime·call32, 32)
+	DISPATCH(runtime·call64, 64)
+	DISPATCH(runtime·call128, 128)
+	DISPATCH(runtime·call256, 256)
+	DISPATCH(runtime·call512, 512)
+	DISPATCH(runtime·call1024, 1024)
+	DISPATCH(runtime·call2048, 2048)
+	DISPATCH(runtime·call4096, 4096)
+	DISPATCH(runtime·call8192, 8192)
+	DISPATCH(runtime·call16384, 16384)
+	DISPATCH(runtime·call32768, 32768)
+	DISPATCH(runtime·call65536, 65536)
+	DISPATCH(runtime·call131072, 131072)
+	DISPATCH(runtime·call262144, 262144)
+	DISPATCH(runtime·call524288, 524288)
+	DISPATCH(runtime·call1048576, 1048576)
+	DISPATCH(runtime·call2097152, 2097152)
+	DISPATCH(runtime·call4194304, 4194304)
+	DISPATCH(runtime·call8388608, 8388608)
+	DISPATCH(runtime·call16777216, 16777216)
+	DISPATCH(runtime·call33554432, 33554432)
+	DISPATCH(runtime·call67108864, 67108864)
+	DISPATCH(runtime·call134217728, 134217728)
+	DISPATCH(runtime·call268435456, 268435456)
+	DISPATCH(runtime·call536870912, 536870912)
+	DISPATCH(runtime·call1073741824, 1073741824)
+	MOVQ	$runtime·badreflectcall(SB), AX
+	JMP	AX
+
+// Argument map for the callXX frames.  Each has one stack map.
+DATA gcargs_reflectcall<>+0x00(SB)/4, $1  // 1 stackmap
+DATA gcargs_reflectcall<>+0x04(SB)/4, $6  // 3 words
+DATA gcargs_reflectcall<>+0x08(SB)/1, $(const_BitsPointer+(const_BitsPointer<<2)+(const_BitsScalar<<4))
+GLOBL gcargs_reflectcall<>(SB),RODATA,$12
+
+// callXX frames have no locals
+DATA gclocals_reflectcall<>+0x00(SB)/4, $1  // 1 stackmap
+DATA gclocals_reflectcall<>+0x04(SB)/4, $0  // 0 locals
+GLOBL gclocals_reflectcall<>(SB),RODATA,$8
+
+#define CALLFN(NAME,MAXSIZE)			\
+TEXT NAME(SB), WRAPPER, $MAXSIZE-24;		\
+	FUNCDATA $FUNCDATA_ArgsPointerMaps,gcargs_reflectcall<>(SB);	\
+	FUNCDATA $FUNCDATA_LocalsPointerMaps,gclocals_reflectcall<>(SB);\
+	/* copy arguments to stack */		\
+	MOVQ	argptr+8(FP), SI;		\
+	MOVLQZX argsize+16(FP), CX;		\
+	MOVQ	SP, DI;				\
+	REP;MOVSB;				\
+	/* call function */			\
+	MOVQ	f+0(FP), DX;			\
+	PCDATA  $PCDATA_StackMapIndex, $0;	\
+	CALL	(DX);				\
+	/* copy return values back */		\
+	MOVQ	argptr+8(FP), DI;		\
+	MOVLQZX	argsize+16(FP), CX;		\
+	MOVLQZX retoffset+20(FP), BX;		\
+	MOVQ	SP, SI;				\
+	ADDQ	BX, DI;				\
+	ADDQ	BX, SI;				\
+	SUBQ	BX, CX;				\
+	REP;MOVSB;				\
+	RET
+
+CALLFN(runtime·call16, 16)
+CALLFN(runtime·call32, 32)
+CALLFN(runtime·call64, 64)
+CALLFN(runtime·call128, 128)
+CALLFN(runtime·call256, 256)
+CALLFN(runtime·call512, 512)
+CALLFN(runtime·call1024, 1024)
+CALLFN(runtime·call2048, 2048)
+CALLFN(runtime·call4096, 4096)
+CALLFN(runtime·call8192, 8192)
+CALLFN(runtime·call16384, 16384)
+CALLFN(runtime·call32768, 32768)
+CALLFN(runtime·call65536, 65536)
+CALLFN(runtime·call131072, 131072)
+CALLFN(runtime·call262144, 262144)
+CALLFN(runtime·call524288, 524288)
+CALLFN(runtime·call1048576, 1048576)
+CALLFN(runtime·call2097152, 2097152)
+CALLFN(runtime·call4194304, 4194304)
+CALLFN(runtime·call8388608, 8388608)
+CALLFN(runtime·call16777216, 16777216)
+CALLFN(runtime·call33554432, 33554432)
+CALLFN(runtime·call67108864, 67108864)
+CALLFN(runtime·call134217728, 134217728)
+CALLFN(runtime·call268435456, 268435456)
+CALLFN(runtime·call536870912, 536870912)
+CALLFN(runtime·call1073741824, 1073741824)
+
+// Return point when leaving stack.
+//
+// Lessstack can appear in stack traces for the same reason
+// as morestack; in that context, it has 0 arguments.
+TEXT runtime·lessstack(SB), NOSPLIT, $0-0
+	// Save return value in m->cret
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVQ	AX, m_cret(BX)
+
+	// Call oldstack on m->g0's stack.
+	MOVQ	m_g0(BX), BP
+	MOVQ	BP, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(BP), SP
+	CALL	runtime·oldstack(SB)
+	MOVQ	$0, 0x1004	// crash if oldstack returns
+	RET
+
+// morestack trampolines
+TEXT runtime·morestack00(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVQ	$0, AX
+	MOVQ	AX, m_moreframesize(BX)
+	MOVQ	$runtime·morestack(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack01(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	SHLQ	$32, AX
+	MOVQ	AX, m_moreframesize(BX)
+	MOVQ	$runtime·morestack(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack10(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVLQZX	AX, AX
+	MOVQ	AX, m_moreframesize(BX)
+	MOVQ	$runtime·morestack(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack11(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	MOVQ	AX, m_moreframesize(BX)
+	MOVQ	$runtime·morestack(SB), AX
+	JMP	AX
+
+// subcases of morestack01
+// with const of 8,16,...48
+TEXT runtime·morestack8(SB),NOSPLIT,$0
+	MOVQ	$1, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack16(SB),NOSPLIT,$0
+	MOVQ	$2, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack24(SB),NOSPLIT,$0
+	MOVQ	$3, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack32(SB),NOSPLIT,$0
+	MOVQ	$4, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack40(SB),NOSPLIT,$0
+	MOVQ	$5, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack48(SB),NOSPLIT,$0
+	MOVQ	$6, R8
+	MOVQ	$morestack<>(SB), AX
+	JMP	AX
+
+TEXT morestack<>(SB),NOSPLIT,$0
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_m(BX), BX
+	SHLQ	$35, R8
+	MOVQ	R8, m_moreframesize(BX)
+	MOVQ	$runtime·morestack(SB), AX
+	JMP	AX
+
+TEXT runtime·morestack00_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack00(SB)
+
+TEXT runtime·morestack01_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack01(SB)
+
+TEXT runtime·morestack10_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack10(SB)
+
+TEXT runtime·morestack11_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack11(SB)
+
+TEXT runtime·morestack8_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack8(SB)
+
+TEXT runtime·morestack16_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack16(SB)
+
+TEXT runtime·morestack24_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack24(SB)
+
+TEXT runtime·morestack32_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack32(SB)
+
+TEXT runtime·morestack40_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack40(SB)
+
+TEXT runtime·morestack48_noctxt(SB),NOSPLIT,$0
+	MOVL	$0, DX
+	JMP	runtime·morestack48(SB)
+
+// bool cas(int32 *val, int32 old, int32 new)
+// Atomically:
+//	if(*val == old){
+//		*val = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT runtime·cas(SB), NOSPLIT, $0-17
+	MOVQ	ptr+0(FP), BX
+	MOVL	old+8(FP), AX
+	MOVL	new+12(FP), CX
+	LOCK
+	CMPXCHGL	CX, 0(BX)
+	JZ 4(PC)
+	MOVL	$0, AX
+	MOVB	AX, ret+16(FP)
+	RET
+	MOVL	$1, AX
+	MOVB	AX, ret+16(FP)
+	RET
+
+// bool	runtime·cas64(uint64 *val, uint64 old, uint64 new)
+// Atomically:
+//	if(*val == *old){
+//		*val = new;
+//		return 1;
+//	} else {
+//		return 0;
+//	}
+TEXT runtime·cas64(SB), NOSPLIT, $0-25
+	MOVQ	ptr+0(FP), BX
+	MOVQ	old+8(FP), AX
+	MOVQ	new+16(FP), CX
+	LOCK
+	CMPXCHGQ	CX, 0(BX)
+	JNZ	cas64_fail
+	MOVL	$1, AX
+	MOVB	AX, ret+24(FP)
+	RET
+cas64_fail:
+	MOVL	$0, AX
+	MOVB	AX, ret+24(FP)
+	RET
+	
+TEXT runtime·casuintptr(SB), NOSPLIT, $0-25
+	JMP	runtime·cas64(SB)
+
+TEXT runtime·atomicloaduintptr(SB), NOSPLIT, $0-16
+	JMP	runtime·atomicload64(SB)
+
+TEXT runtime·atomicloaduint(SB), NOSPLIT, $0-16
+	JMP	runtime·atomicload64(SB)
+
+// bool casp(void **val, void *old, void *new)
+// Atomically:
+//	if(*val == old){
+//		*val = new;
+//		return 1;
+//	} else
+//		return 0;
+TEXT runtime·casp(SB), NOSPLIT, $0-25
+	MOVQ	ptr+0(FP), BX
+	MOVQ	old+8(FP), AX
+	MOVQ	new+16(FP), CX
+	LOCK
+	CMPXCHGQ	CX, 0(BX)
+	JZ 4(PC)
+	MOVL	$0, AX
+	MOVB	AX, ret+24(FP)
+	RET
+	MOVL	$1, AX
+	MOVB	AX, ret+24(FP)
+	RET
+
+// uint32 xadd(uint32 volatile *val, int32 delta)
+// Atomically:
+//	*val += delta;
+//	return *val;
+TEXT runtime·xadd(SB), NOSPLIT, $0-20
+	MOVQ	ptr+0(FP), BX
+	MOVL	delta+8(FP), AX
+	MOVL	AX, CX
+	LOCK
+	XADDL	AX, 0(BX)
+	ADDL	CX, AX
+	MOVL	AX, ret+16(FP)
+	RET
+
+TEXT runtime·xadd64(SB), NOSPLIT, $0-24
+	MOVQ	ptr+0(FP), BX
+	MOVQ	delta+8(FP), AX
+	MOVQ	AX, CX
+	LOCK
+	XADDQ	AX, 0(BX)
+	ADDQ	CX, AX
+	MOVQ	AX, ret+16(FP)
+	RET
+
+TEXT runtime·xchg(SB), NOSPLIT, $0-20
+	MOVQ	ptr+0(FP), BX
+	MOVL	new+8(FP), AX
+	XCHGL	AX, 0(BX)
+	MOVL	AX, ret+16(FP)
+	RET
+
+TEXT runtime·xchg64(SB), NOSPLIT, $0-24
+	MOVQ	ptr+0(FP), BX
+	MOVQ	new+8(FP), AX
+	XCHGQ	AX, 0(BX)
+	MOVQ	AX, ret+16(FP)
+	RET
+
+TEXT runtime·xchgp(SB), NOSPLIT, $0-24
+	MOVQ	ptr+0(FP), BX
+	MOVQ	new+8(FP), AX
+	XCHGQ	AX, 0(BX)
+	MOVQ	AX, ret+16(FP)
+	RET
+
+TEXT runtime·xchguintptr(SB), NOSPLIT, $0-24
+	JMP	runtime·xchg64(SB)
+
+TEXT runtime·procyield(SB),NOSPLIT,$0-0
+	MOVL	cycles+0(FP), AX
+again:
+	PAUSE
+	SUBL	$1, AX
+	JNZ	again
+	RET
+
+TEXT runtime·atomicstorep(SB), NOSPLIT, $0-16
+	MOVQ	ptr+0(FP), BX
+	MOVQ	val+8(FP), AX
+	XCHGQ	AX, 0(BX)
+	RET
+
+TEXT runtime·atomicstore(SB), NOSPLIT, $0-12
+	MOVQ	ptr+0(FP), BX
+	MOVL	val+8(FP), AX
+	XCHGL	AX, 0(BX)
+	RET
+
+TEXT runtime·atomicstore64(SB), NOSPLIT, $0-16
+	MOVQ	ptr+0(FP), BX
+	MOVQ	val+8(FP), AX
+	XCHGQ	AX, 0(BX)
+	RET
+
+// void	runtime·atomicor8(byte volatile*, byte);
+TEXT runtime·atomicor8(SB), NOSPLIT, $0-9
+	MOVQ	ptr+0(FP), AX
+	MOVB	val+8(FP), BX
+	LOCK
+	ORB	BX, (AX)
+	RET
+
+// void jmpdefer(fn, sp);
+// called from deferreturn.
+// 1. pop the caller
+// 2. sub 5 bytes from the callers return
+// 3. jmp to the argument
+TEXT runtime·jmpdefer(SB), NOSPLIT, $0-16
+	MOVQ	fv+0(FP), DX	// fn
+	MOVQ	argp+8(FP), BX	// caller sp
+	LEAQ	-8(BX), SP	// caller sp after CALL
+	SUBQ	$5, (SP)	// return to CALL again
+	MOVQ	0(DX), BX
+	JMP	BX	// but first run the deferred function
+
+// Save state of caller into g->sched. Smashes R8, R9.
+TEXT gosave<>(SB),NOSPLIT,$0
+	get_tls(R8)
+	MOVQ	g(R8), R8
+	MOVQ	0(SP), R9
+	MOVQ	R9, (g_sched+gobuf_pc)(R8)
+	LEAQ	8(SP), R9
+	MOVQ	R9, (g_sched+gobuf_sp)(R8)
+	MOVQ	$0, (g_sched+gobuf_ret)(R8)
+	MOVQ	$0, (g_sched+gobuf_ctxt)(R8)
+	RET
+
+// asmcgocall(void(*fn)(void*), void *arg)
+// Call fn(arg) on the scheduler stack,
+// aligned appropriately for the gcc ABI.
+// See cgocall.c for more details.
+TEXT runtime·asmcgocall(SB),NOSPLIT,$0-16
+	MOVQ	fn+0(FP), AX
+	MOVQ	arg+8(FP), BX
+	CALL	asmcgocall<>(SB)
+	RET
+
+TEXT runtime·asmcgocall_errno(SB),NOSPLIT,$0-20
+	MOVQ	fn+0(FP), AX
+	MOVQ	arg+8(FP), BX
+	CALL	asmcgocall<>(SB)
+	MOVL	AX, ret+16(FP)
+	RET
+
+// asmcgocall common code. fn in AX, arg in BX. returns errno in AX.
+TEXT asmcgocall<>(SB),NOSPLIT,$0-0
+	MOVQ	SP, DX
+
+	// Figure out if we need to switch to m->g0 stack.
+	// We get called to create new OS threads too, and those
+	// come in on the m->g0 stack already.
+	get_tls(CX)
+	MOVQ	g(CX), BP
+	MOVQ	g_m(BP), BP
+	MOVQ	m_g0(BP), SI
+	MOVQ	g(CX), DI
+	CMPQ	SI, DI
+	JEQ	nosave
+	MOVQ	m_gsignal(BP), SI
+	CMPQ	SI, DI
+	JEQ	nosave
+	
+	MOVQ	m_g0(BP), SI
+	CALL	gosave<>(SB)
+	MOVQ	SI, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(SI), SP
+nosave:
+
+	// Now on a scheduling stack (a pthread-created stack).
+	// Make sure we have enough room for 4 stack-backed fast-call
+	// registers as per windows amd64 calling convention.
+	SUBQ	$64, SP
+	ANDQ	$~15, SP	// alignment for gcc ABI
+	MOVQ	DI, 48(SP)	// save g
+	MOVQ	DX, 40(SP)	// save SP
+	MOVQ	BX, DI		// DI = first argument in AMD64 ABI
+	MOVQ	BX, CX		// CX = first argument in Win64
+	CALL	AX
+
+	// Restore registers, g, stack pointer.
+	get_tls(CX)
+	MOVQ	48(SP), DI
+	MOVQ	DI, g(CX)
+	MOVQ	40(SP), SP
+	RET
+
+// cgocallback(void (*fn)(void*), void *frame, uintptr framesize)
+// Turn the fn into a Go func (by taking its address) and call
+// cgocallback_gofunc.
+TEXT runtime·cgocallback(SB),NOSPLIT,$24-24
+	LEAQ	fn+0(FP), AX
+	MOVQ	AX, 0(SP)
+	MOVQ	frame+8(FP), AX
+	MOVQ	AX, 8(SP)
+	MOVQ	framesize+16(FP), AX
+	MOVQ	AX, 16(SP)
+	MOVQ	$runtime·cgocallback_gofunc(SB), AX
+	CALL	AX
+	RET
+
+// cgocallback_gofunc(FuncVal*, void *frame, uintptr framesize)
+// See cgocall.c for more details.
+TEXT runtime·cgocallback_gofunc(SB),NOSPLIT,$8-24
+	// If g is nil, Go did not create the current thread.
+	// Call needm to obtain one m for temporary use.
+	// In this case, we're running on the thread stack, so there's
+	// lots of space, but the linker doesn't know. Hide the call from
+	// the linker analysis by using an indirect call through AX.
+	get_tls(CX)
+#ifdef GOOS_windows
+	MOVL	$0, BP
+	CMPQ	CX, $0
+	JEQ	2(PC)
+#endif
+	MOVQ	g(CX), BP
+	CMPQ	BP, $0
+	JEQ	needm
+	MOVQ	g_m(BP), BP
+	MOVQ	BP, R8 // holds oldm until end of function
+	JMP	havem
+needm:
+	MOVQ	$0, 0(SP)
+	MOVQ	$runtime·needm(SB), AX
+	CALL	AX
+	MOVQ	0(SP), R8
+	get_tls(CX)
+	MOVQ	g(CX), BP
+	MOVQ	g_m(BP), BP
+
+havem:
+	// Now there's a valid m, and we're running on its m->g0.
+	// Save current m->g0->sched.sp on stack and then set it to SP.
+	// Save current sp in m->g0->sched.sp in preparation for
+	// switch back to m->curg stack.
+	// NOTE: unwindm knows that the saved g->sched.sp is at 0(SP).
+	MOVQ	m_g0(BP), SI
+	MOVQ	(g_sched+gobuf_sp)(SI), AX
+	MOVQ	AX, 0(SP)
+	MOVQ	SP, (g_sched+gobuf_sp)(SI)
+
+	// Switch to m->curg stack and call runtime.cgocallbackg.
+	// Because we are taking over the execution of m->curg
+	// but *not* resuming what had been running, we need to
+	// save that information (m->curg->sched) so we can restore it.
+	// We can restore m->curg->sched.sp easily, because calling
+	// runtime.cgocallbackg leaves SP unchanged upon return.
+	// To save m->curg->sched.pc, we push it onto the stack.
+	// This has the added benefit that it looks to the traceback
+	// routine like cgocallbackg is going to return to that
+	// PC (because the frame we allocate below has the same
+	// size as cgocallback_gofunc's frame declared above)
+	// so that the traceback will seamlessly trace back into
+	// the earlier calls.
+	//
+	// In the new goroutine, 0(SP) holds the saved R8.
+	MOVQ	m_curg(BP), SI
+	MOVQ	SI, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(SI), DI  // prepare stack as DI
+	MOVQ	(g_sched+gobuf_pc)(SI), BP
+	MOVQ	BP, -8(DI)
+	LEAQ	-(8+8)(DI), SP
+	MOVQ	R8, 0(SP)
+	CALL	runtime·cgocallbackg(SB)
+	MOVQ	0(SP), R8
+
+	// Restore g->sched (== m->curg->sched) from saved values.
+	get_tls(CX)
+	MOVQ	g(CX), SI
+	MOVQ	8(SP), BP
+	MOVQ	BP, (g_sched+gobuf_pc)(SI)
+	LEAQ	(8+8)(SP), DI
+	MOVQ	DI, (g_sched+gobuf_sp)(SI)
+
+	// Switch back to m->g0's stack and restore m->g0->sched.sp.
+	// (Unlike m->curg, the g0 goroutine never uses sched.pc,
+	// so we do not have to restore it.)
+	MOVQ	g(CX), BP
+	MOVQ	g_m(BP), BP
+	MOVQ	m_g0(BP), SI
+	MOVQ	SI, g(CX)
+	MOVQ	(g_sched+gobuf_sp)(SI), SP
+	MOVQ	0(SP), AX
+	MOVQ	AX, (g_sched+gobuf_sp)(SI)
+	
+	// If the m on entry was nil, we called needm above to borrow an m
+	// for the duration of the call. Since the call is over, return it with dropm.
+	CMPQ	R8, $0
+	JNE 3(PC)
+	MOVQ	$runtime·dropm(SB), AX
+	CALL	AX
+
+	// Done!
+	RET
+
+// void setg(G*); set g. for use by needm.
+TEXT runtime·setg(SB), NOSPLIT, $0-8
+	MOVQ	gg+0(FP), BX
+#ifdef GOOS_windows
+	CMPQ	BX, $0
+	JNE	settls
+	MOVQ	$0, 0x28(GS)
+	RET
+settls:
+	MOVQ	g_m(BX), AX
+	LEAQ	m_tls(AX), AX
+	MOVQ	AX, 0x28(GS)
+#endif
+	get_tls(CX)
+	MOVQ	BX, g(CX)
+	RET
+
+// void setg_gcc(G*); set g called from gcc.
+TEXT setg_gcc<>(SB),NOSPLIT,$0
+	get_tls(AX)
+	MOVQ	DI, g(AX)
+	RET
+
+// check that SP is in range [g->stackbase, g->stackguard)
+TEXT runtime·stackcheck(SB), NOSPLIT, $0-0
+	get_tls(CX)
+	MOVQ	g(CX), AX
+	CMPQ	g_stackbase(AX), SP
+	JHI	2(PC)
+	INT	$3
+	CMPQ	SP, g_stackguard(AX)
+	JHI	2(PC)
+	INT	$3
+	RET
+
+TEXT runtime·getcallerpc(SB),NOSPLIT,$0-16
+	MOVQ	argp+0(FP),AX		// addr of first arg
+	MOVQ	-8(AX),AX		// get calling pc
+	MOVQ	AX, ret+8(FP)
+	RET
+
+TEXT runtime·gogetcallerpc(SB),NOSPLIT,$0-16
+	MOVQ	p+0(FP),AX		// addr of first arg
+	MOVQ	-8(AX),AX		// get calling pc
+	MOVQ	AX,ret+8(FP)
+	RET
+
+TEXT runtime·setcallerpc(SB),NOSPLIT,$0-16
+	MOVQ	argp+0(FP),AX		// addr of first arg
+	MOVQ	pc+8(FP), BX
+	MOVQ	BX, -8(AX)		// set calling pc
+	RET
+
+TEXT runtime·getcallersp(SB),NOSPLIT,$0-16
+	MOVQ	argp+0(FP), AX
+	MOVQ	AX, ret+8(FP)
+	RET
+
+// func gogetcallersp(p unsafe.Pointer) uintptr
+TEXT runtime·gogetcallersp(SB),NOSPLIT,$0-16
+	MOVQ	p+0(FP),AX		// addr of first arg
+	MOVQ	AX, ret+8(FP)
+	RET
+
+// int64 runtime·cputicks(void)
+TEXT runtime·cputicks(SB),NOSPLIT,$0-0
+	RDTSC
+	SHLQ	$32, DX
+	ADDQ	DX, AX
+	MOVQ	AX, ret+0(FP)
+	RET
+
+TEXT runtime·gocputicks(SB),NOSPLIT,$0-8
+	RDTSC
+	SHLQ    $32, DX
+	ADDQ    DX, AX
+	MOVQ    AX, ret+0(FP)
+	RET
+
+TEXT runtime·stackguard(SB),NOSPLIT,$0-16
+	MOVQ	SP, DX
+	MOVQ	DX, sp+0(FP)
+	get_tls(CX)
+	MOVQ	g(CX), BX
+	MOVQ	g_stackguard(BX), DX
+	MOVQ	DX, limit+8(FP)
+	RET
+
+GLOBL runtime·tls0(SB), $64
+
+// hash function using AES hardware instructions
+TEXT runtime·aeshash(SB),NOSPLIT,$0-32
+	MOVQ	p+0(FP), AX	// ptr to data
+	MOVQ	s+8(FP), CX	// size
+	JMP	runtime·aeshashbody(SB)
+
+TEXT runtime·aeshashstr(SB),NOSPLIT,$0-32
+	MOVQ	p+0(FP), AX	// ptr to string struct
+	// s+8(FP) is ignored, it is always sizeof(String)
+	MOVQ	8(AX), CX	// length of string
+	MOVQ	(AX), AX	// string data
+	JMP	runtime·aeshashbody(SB)
+
+// AX: data
+// CX: length
+TEXT runtime·aeshashbody(SB),NOSPLIT,$0-32
+	MOVQ	h+16(FP), X0	// seed to low 64 bits of xmm0
+	PINSRQ	$1, CX, X0	// size to high 64 bits of xmm0
+	MOVO	runtime·aeskeysched+0(SB), X2
+	MOVO	runtime·aeskeysched+16(SB), X3
+	CMPQ	CX, $16
+	JB	aessmall
+aesloop:
+	CMPQ	CX, $16
+	JBE	aesloopend
+	MOVOU	(AX), X1
+	AESENC	X2, X0
+	AESENC	X1, X0
+	SUBQ	$16, CX
+	ADDQ	$16, AX
+	JMP	aesloop
+// 1-16 bytes remaining
+aesloopend:
+	// This load may overlap with the previous load above.
+	// We'll hash some bytes twice, but that's ok.
+	MOVOU	-16(AX)(CX*1), X1
+	JMP	partial
+// 0-15 bytes
+aessmall:
+	TESTQ	CX, CX
+	JE	finalize	// 0 bytes
+
+	CMPB	AX, $0xf0
+	JA	highpartial
+
+	// 16 bytes loaded at this address won't cross
+	// a page boundary, so we can load it directly.
+	MOVOU	(AX), X1
+	ADDQ	CX, CX
+	MOVQ	$masks<>(SB), BP
+	PAND	(BP)(CX*8), X1
+	JMP	partial
+highpartial:
+	// address ends in 1111xxxx.  Might be up against
+	// a page boundary, so load ending at last byte.
+	// Then shift bytes down using pshufb.
+	MOVOU	-16(AX)(CX*1), X1
+	ADDQ	CX, CX
+	MOVQ	$shifts<>(SB), BP
+	PSHUFB	(BP)(CX*8), X1
+partial:
+	// incorporate partial block into hash
+	AESENC	X3, X0
+	AESENC	X1, X0
+finalize:	
+	// finalize hash
+	AESENC	X2, X0
+	AESENC	X3, X0
+	AESENC	X2, X0
+	MOVQ	X0, res+24(FP)
+	RET
+
+TEXT runtime·aeshash32(SB),NOSPLIT,$0-32
+	MOVQ	p+0(FP), AX	// ptr to data
+	// s+8(FP) is ignored, it is always sizeof(int32)
+	MOVQ	h+16(FP), X0	// seed
+	PINSRD	$2, (AX), X0	// data
+	AESENC	runtime·aeskeysched+0(SB), X0
+	AESENC	runtime·aeskeysched+16(SB), X0
+	AESENC	runtime·aeskeysched+0(SB), X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+TEXT runtime·aeshash64(SB),NOSPLIT,$0-32
+	MOVQ	p+0(FP), AX	// ptr to data
+	// s+8(FP) is ignored, it is always sizeof(int64)
+	MOVQ	h+16(FP), X0	// seed
+	PINSRQ	$1, (AX), X0	// data
+	AESENC	runtime·aeskeysched+0(SB), X0
+	AESENC	runtime·aeskeysched+16(SB), X0
+	AESENC	runtime·aeskeysched+0(SB), X0
+	MOVQ	X0, ret+24(FP)
+	RET
+
+// simple mask to get rid of data in the high part of the register.
+DATA masks<>+0x00(SB)/8, $0x0000000000000000
+DATA masks<>+0x08(SB)/8, $0x0000000000000000
+DATA masks<>+0x10(SB)/8, $0x00000000000000ff
+DATA masks<>+0x18(SB)/8, $0x0000000000000000
+DATA masks<>+0x20(SB)/8, $0x000000000000ffff
+DATA masks<>+0x28(SB)/8, $0x0000000000000000
+DATA masks<>+0x30(SB)/8, $0x0000000000ffffff
+DATA masks<>+0x38(SB)/8, $0x0000000000000000
+DATA masks<>+0x40(SB)/8, $0x00000000ffffffff
+DATA masks<>+0x48(SB)/8, $0x0000000000000000
+DATA masks<>+0x50(SB)/8, $0x000000ffffffffff
+DATA masks<>+0x58(SB)/8, $0x0000000000000000
+DATA masks<>+0x60(SB)/8, $0x0000ffffffffffff
+DATA masks<>+0x68(SB)/8, $0x0000000000000000
+DATA masks<>+0x70(SB)/8, $0x00ffffffffffffff
+DATA masks<>+0x78(SB)/8, $0x0000000000000000
+DATA masks<>+0x80(SB)/8, $0xffffffffffffffff
+DATA masks<>+0x88(SB)/8, $0x0000000000000000
+DATA masks<>+0x90(SB)/8, $0xffffffffffffffff
+DATA masks<>+0x98(SB)/8, $0x00000000000000ff
+DATA masks<>+0xa0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xa8(SB)/8, $0x000000000000ffff
+DATA masks<>+0xb0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xb8(SB)/8, $0x0000000000ffffff
+DATA masks<>+0xc0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xc8(SB)/8, $0x00000000ffffffff
+DATA masks<>+0xd0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xd8(SB)/8, $0x000000ffffffffff
+DATA masks<>+0xe0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xe8(SB)/8, $0x0000ffffffffffff
+DATA masks<>+0xf0(SB)/8, $0xffffffffffffffff
+DATA masks<>+0xf8(SB)/8, $0x00ffffffffffffff
+GLOBL masks<>(SB),RODATA,$256
+
+// these are arguments to pshufb.  They move data down from
+// the high bytes of the register to the low bytes of the register.
+// index is how many bytes to move.
+DATA shifts<>+0x00(SB)/8, $0x0000000000000000
+DATA shifts<>+0x08(SB)/8, $0x0000000000000000
+DATA shifts<>+0x10(SB)/8, $0xffffffffffffff0f
+DATA shifts<>+0x18(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x20(SB)/8, $0xffffffffffff0f0e
+DATA shifts<>+0x28(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x30(SB)/8, $0xffffffffff0f0e0d
+DATA shifts<>+0x38(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x40(SB)/8, $0xffffffff0f0e0d0c
+DATA shifts<>+0x48(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x50(SB)/8, $0xffffff0f0e0d0c0b
+DATA shifts<>+0x58(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x60(SB)/8, $0xffff0f0e0d0c0b0a
+DATA shifts<>+0x68(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x70(SB)/8, $0xff0f0e0d0c0b0a09
+DATA shifts<>+0x78(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x80(SB)/8, $0x0f0e0d0c0b0a0908
+DATA shifts<>+0x88(SB)/8, $0xffffffffffffffff
+DATA shifts<>+0x90(SB)/8, $0x0e0d0c0b0a090807
+DATA shifts<>+0x98(SB)/8, $0xffffffffffffff0f
+DATA shifts<>+0xa0(SB)/8, $0x0d0c0b0a09080706
+DATA shifts<>+0xa8(SB)/8, $0xffffffffffff0f0e
+DATA shifts<>+0xb0(SB)/8, $0x0c0b0a0908070605
+DATA shifts<>+0xb8(SB)/8, $0xffffffffff0f0e0d
+DATA shifts<>+0xc0(SB)/8, $0x0b0a090807060504
+DATA shifts<>+0xc8(SB)/8, $0xffffffff0f0e0d0c
+DATA shifts<>+0xd0(SB)/8, $0x0a09080706050403
+DATA shifts<>+0xd8(SB)/8, $0xffffff0f0e0d0c0b
+DATA shifts<>+0xe0(SB)/8, $0x0908070605040302
+DATA shifts<>+0xe8(SB)/8, $0xffff0f0e0d0c0b0a
+DATA shifts<>+0xf0(SB)/8, $0x0807060504030201
+DATA shifts<>+0xf8(SB)/8, $0xff0f0e0d0c0b0a09
+GLOBL shifts<>(SB),RODATA,$256
+
+TEXT runtime·memeq(SB),NOSPLIT,$0-25
+	MOVQ	a+0(FP), SI
+	MOVQ	b+8(FP), DI
+	MOVQ	size+16(FP), BX
+	CALL	runtime·memeqbody(SB)
+	MOVB	AX, ret+24(FP)
+	RET
+
+// eqstring tests whether two strings are equal.
+// See runtime_test.go:eqstring_generic for
+// equivalent Go code.
+TEXT runtime·eqstring(SB),NOSPLIT,$0-33
+	MOVQ	s1len+8(FP), AX
+	MOVQ	s2len+24(FP), BX
+	CMPQ	AX, BX
+	JNE	different
+	MOVQ	s1str+0(FP), SI
+	MOVQ	s2str+16(FP), DI
+	CMPQ	SI, DI
+	JEQ	same
+	CALL	runtime·memeqbody(SB)
+	MOVB	AX, v+32(FP)
+	RET
+same:
+	MOVB	$1, v+32(FP)
+	RET
+different:
+	MOVB	$0, v+32(FP)
+	RET
+
+// a in SI
+// b in DI
+// count in BX
+TEXT runtime·memeqbody(SB),NOSPLIT,$0-0
+	XORQ	AX, AX
+
+	CMPQ	BX, $8
+	JB	small
+	
+	// 64 bytes at a time using xmm registers
+hugeloop:
+	CMPQ	BX, $64
+	JB	bigloop
+	MOVOU	(SI), X0
+	MOVOU	(DI), X1
+	MOVOU	16(SI), X2
+	MOVOU	16(DI), X3
+	MOVOU	32(SI), X4
+	MOVOU	32(DI), X5
+	MOVOU	48(SI), X6
+	MOVOU	48(DI), X7
+	PCMPEQB	X1, X0
+	PCMPEQB	X3, X2
+	PCMPEQB	X5, X4
+	PCMPEQB	X7, X6
+	PAND	X2, X0
+	PAND	X6, X4
+	PAND	X4, X0
+	PMOVMSKB X0, DX
+	ADDQ	$64, SI
+	ADDQ	$64, DI
+	SUBQ	$64, BX
+	CMPL	DX, $0xffff
+	JEQ	hugeloop
+	RET
+
+	// 8 bytes at a time using 64-bit register
+bigloop:
+	CMPQ	BX, $8
+	JBE	leftover
+	MOVQ	(SI), CX
+	MOVQ	(DI), DX
+	ADDQ	$8, SI
+	ADDQ	$8, DI
+	SUBQ	$8, BX
+	CMPQ	CX, DX
+	JEQ	bigloop
+	RET
+
+	// remaining 0-8 bytes
+leftover:
+	MOVQ	-8(SI)(BX*1), CX
+	MOVQ	-8(DI)(BX*1), DX
+	CMPQ	CX, DX
+	SETEQ	AX
+	RET
+
+small:
+	CMPQ	BX, $0
+	JEQ	equal
+
+	LEAQ	0(BX*8), CX
+	NEGQ	CX
+
+	CMPB	SI, $0xf8
+	JA	si_high
+
+	// load at SI won't cross a page boundary.
+	MOVQ	(SI), SI
+	JMP	si_finish
+si_high:
+	// address ends in 11111xxx.  Load up to bytes we want, move to correct position.
+	MOVQ	-8(SI)(BX*1), SI
+	SHRQ	CX, SI
+si_finish:
+
+	// same for DI.
+	CMPB	DI, $0xf8
+	JA	di_high
+	MOVQ	(DI), DI
+	JMP	di_finish
+di_high:
+	MOVQ	-8(DI)(BX*1), DI
+	SHRQ	CX, DI
+di_finish:
+
+	SUBQ	SI, DI
+	SHLQ	CX, DI
+equal:
+	SETEQ	AX
+	RET
+
+TEXT runtime·cmpstring(SB),NOSPLIT,$0-40
+	MOVQ	s1_base+0(FP), SI
+	MOVQ	s1_len+8(FP), BX
+	MOVQ	s2_base+16(FP), DI
+	MOVQ	s2_len+24(FP), DX
+	CALL	runtime·cmpbody(SB)
+	MOVQ	AX, ret+32(FP)
+	RET
+
+TEXT runtime·cmpbytes(SB),NOSPLIT,$0-56
+	MOVQ	s1+0(FP), SI
+	MOVQ	s1+8(FP), BX
+	MOVQ	s2+24(FP), DI
+	MOVQ	s2+32(FP), DX
+	CALL	runtime·cmpbody(SB)
+	MOVQ	AX, res+48(FP)
+	RET
+
+// input:
+//   SI = a
+//   DI = b
+//   BX = alen
+//   DX = blen
+// output:
+//   AX = 1/0/-1
+TEXT runtime·cmpbody(SB),NOSPLIT,$0-0
+	CMPQ	SI, DI
+	JEQ	cmp_allsame
+	CMPQ	BX, DX
+	MOVQ	DX, BP
+	CMOVQLT	BX, BP // BP = min(alen, blen) = # of bytes to compare
+	CMPQ	BP, $8
+	JB	cmp_small
+
+cmp_loop:
+	CMPQ	BP, $16
+	JBE	cmp_0through16
+	MOVOU	(SI), X0
+	MOVOU	(DI), X1
+	PCMPEQB X0, X1
+	PMOVMSKB X1, AX
+	XORQ	$0xffff, AX	// convert EQ to NE
+	JNE	cmp_diff16	// branch if at least one byte is not equal
+	ADDQ	$16, SI
+	ADDQ	$16, DI
+	SUBQ	$16, BP
+	JMP	cmp_loop
+	
+	// AX = bit mask of differences
+cmp_diff16:
+	BSFQ	AX, BX	// index of first byte that differs
+	XORQ	AX, AX
+	MOVB	(SI)(BX*1), CX
+	CMPB	CX, (DI)(BX*1)
+	SETHI	AX
+	LEAQ	-1(AX*2), AX	// convert 1/0 to +1/-1
+	RET
+
+	// 0 through 16 bytes left, alen>=8, blen>=8
+cmp_0through16:
+	CMPQ	BP, $8
+	JBE	cmp_0through8
+	MOVQ	(SI), AX
+	MOVQ	(DI), CX
+	CMPQ	AX, CX
+	JNE	cmp_diff8
+cmp_0through8:
+	MOVQ	-8(SI)(BP*1), AX
+	MOVQ	-8(DI)(BP*1), CX
+	CMPQ	AX, CX
+	JEQ	cmp_allsame
+
+	// AX and CX contain parts of a and b that differ.
+cmp_diff8:
+	BSWAPQ	AX	// reverse order of bytes
+	BSWAPQ	CX
+	XORQ	AX, CX
+	BSRQ	CX, CX	// index of highest bit difference
+	SHRQ	CX, AX	// move a's bit to bottom
+	ANDQ	$1, AX	// mask bit
+	LEAQ	-1(AX*2), AX // 1/0 => +1/-1
+	RET
+
+	// 0-7 bytes in common
+cmp_small:
+	LEAQ	(BP*8), CX	// bytes left -> bits left
+	NEGQ	CX		//  - bits lift (== 64 - bits left mod 64)
+	JEQ	cmp_allsame
+
+	// load bytes of a into high bytes of AX
+	CMPB	SI, $0xf8
+	JA	cmp_si_high
+	MOVQ	(SI), SI
+	JMP	cmp_si_finish
+cmp_si_high:
+	MOVQ	-8(SI)(BP*1), SI
+	SHRQ	CX, SI
+cmp_si_finish:
+	SHLQ	CX, SI
+
+	// load bytes of b in to high bytes of BX
+	CMPB	DI, $0xf8
+	JA	cmp_di_high
+	MOVQ	(DI), DI
+	JMP	cmp_di_finish
+cmp_di_high:
+	MOVQ	-8(DI)(BP*1), DI
+	SHRQ	CX, DI
+cmp_di_finish:
+	SHLQ	CX, DI
+
+	BSWAPQ	SI	// reverse order of bytes
+	BSWAPQ	DI
+	XORQ	SI, DI	// find bit differences
+	JEQ	cmp_allsame
+	BSRQ	DI, CX	// index of highest bit difference
+	SHRQ	CX, SI	// move a's bit to bottom
+	ANDQ	$1, SI	// mask bit
+	LEAQ	-1(SI*2), AX // 1/0 => +1/-1
+	RET
+
+cmp_allsame:
+	XORQ	AX, AX
+	XORQ	CX, CX
+	CMPQ	BX, DX
+	SETGT	AX	// 1 if alen > blen
+	SETEQ	CX	// 1 if alen == blen
+	LEAQ	-1(CX)(AX*2), AX	// 1,0,-1 result
+	RET
+
+TEXT bytes·IndexByte(SB),NOSPLIT,$0
+	MOVQ s+0(FP), SI
+	MOVQ s_len+8(FP), BX
+	MOVB c+24(FP), AL
+	CALL runtime·indexbytebody(SB)
+	MOVQ AX, ret+32(FP)
+	RET
+
+TEXT strings·IndexByte(SB),NOSPLIT,$0
+	MOVQ s+0(FP), SI
+	MOVQ s_len+8(FP), BX
+	MOVB c+16(FP), AL
+	CALL runtime·indexbytebody(SB)
+	MOVQ AX, ret+24(FP)
+	RET
+
+// input:
+//   SI: data
+//   BX: data len
+//   AL: byte sought
+// output:
+//   AX
+TEXT runtime·indexbytebody(SB),NOSPLIT,$0
+	MOVQ SI, DI
+
+	CMPQ BX, $16
+	JLT indexbyte_small
+
+	// round up to first 16-byte boundary
+	TESTQ $15, SI
+	JZ aligned
+	MOVQ SI, CX
+	ANDQ $~15, CX
+	ADDQ $16, CX
+
+	// search the beginning
+	SUBQ SI, CX
+	REPN; SCASB
+	JZ success
+
+// DI is 16-byte aligned; get ready to search using SSE instructions
+aligned:
+	// round down to last 16-byte boundary
+	MOVQ BX, R11
+	ADDQ SI, R11
+	ANDQ $~15, R11
+
+	// shuffle X0 around so that each byte contains c
+	MOVD AX, X0
+	PUNPCKLBW X0, X0
+	PUNPCKLBW X0, X0
+	PSHUFL $0, X0, X0
+	JMP condition
+
+sse:
+	// move the next 16-byte chunk of the buffer into X1
+	MOVO (DI), X1
+	// compare bytes in X0 to X1
+	PCMPEQB X0, X1
+	// take the top bit of each byte in X1 and put the result in DX
+	PMOVMSKB X1, DX
+	TESTL DX, DX
+	JNZ ssesuccess
+	ADDQ $16, DI
+
+condition:
+	CMPQ DI, R11
+	JLT sse
+
+	// search the end
+	MOVQ SI, CX
+	ADDQ BX, CX
+	SUBQ R11, CX
+	// if CX == 0, the zero flag will be set and we'll end up
+	// returning a false success
+	JZ failure
+	REPN; SCASB
+	JZ success
+
+failure:
+	MOVQ $-1, AX
+	RET
+
+// handle for lengths < 16
+indexbyte_small:
+	MOVQ BX, CX
+	REPN; SCASB
+	JZ success
+	MOVQ $-1, AX
+	RET
+
+// we've found the chunk containing the byte
+// now just figure out which specific byte it is
+ssesuccess:
+	// get the index of the least significant set bit
+	BSFW DX, DX
+	SUBQ SI, DI
+	ADDQ DI, DX
+	MOVQ DX, AX
+	RET
+
+success:
+	SUBQ SI, DI
+	SUBL $1, DI
+	MOVQ DI, AX
+	RET
+
+TEXT bytes·Equal(SB),NOSPLIT,$0-49
+	MOVQ	a_len+8(FP), BX
+	MOVQ	b_len+32(FP), CX
+	XORQ	AX, AX
+	CMPQ	BX, CX
+	JNE	eqret
+	MOVQ	a+0(FP), SI
+	MOVQ	b+24(FP), DI
+	CALL	runtime·memeqbody(SB)
+eqret:
+	MOVB	AX, ret+48(FP)
+	RET
+
+// A Duff's device for zeroing memory.
+// The compiler jumps to computed addresses within
+// this routine to zero chunks of memory.  Do not
+// change this code without also changing the code
+// in ../../cmd/6g/ggen.c:clearfat.
+// AX: zero
+// DI: ptr to memory to be zeroed
+// DI is updated as a side effect.
+TEXT runtime·duffzero(SB), NOSPLIT, $0-0
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	STOSQ
+	RET
+
+// A Duff's device for copying memory.
+// The compiler jumps to computed addresses within
+// this routine to copy chunks of memory.  Source
+// and destination must not overlap.  Do not
+// change this code without also changing the code
+// in ../../cmd/6g/cgen.c:sgen.
+// SI: ptr to source memory
+// DI: ptr to destination memory
+// SI and DI are updated as a side effect.
+
+// NOTE: this is equivalent to a sequence of MOVSQ but
+// for some reason that is 3.5x slower than this code.
+// The STOSQ above seem fine, though.
+TEXT runtime·duffcopy(SB), NOSPLIT, $0-0
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	MOVQ	(SI),CX
+	ADDQ	$8,SI
+	MOVQ	CX,(DI)
+	ADDQ	$8,DI
+
+	RET
+
+TEXT runtime·timenow(SB), NOSPLIT, $0-0
+	JMP	time·now(SB)
+
+TEXT runtime·fastrand1(SB), NOSPLIT, $0-4
+	get_tls(CX)
+	MOVQ	g(CX), AX
+	MOVQ	g_m(AX), AX
+	MOVL	m_fastrand(AX), DX
+	ADDL	DX, DX
+	MOVL	DX, BX
+	XORL	$0x88888eef, DX
+	CMOVLMI	BX, DX
+	MOVL	DX, m_fastrand(AX)
+	MOVL	DX, ret+0(FP)
+	RET
+
+TEXT runtime·return0(SB), NOSPLIT, $0
+	MOVL	$0, AX
+	RET
