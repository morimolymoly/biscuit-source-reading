commit 175c5ab63fc4fe6e11b821541566f5c496f422e1
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Feb 23 10:48:38 2019 -0500

    raise MAXCPUS to 64

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1ccb2e4ab3..d65c192d8d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -482,7 +482,7 @@ func (c *cpu_t) setthread(t *thread_t) {
 var Cpumhz uint
 var Pspercycle uint
 
-const MAXCPUS int = 32
+const MAXCPUS int = 64
 
 var cpus [MAXCPUS]cpu_t
 

commit 565ddaf86cea6461649f22a362409682c9ea1d58
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Feb 23 10:44:08 2019 -0500

    fine-grained control of CPU use
    
    bhw2 has 2 sockets with 10 cores and hyperthreads. add support to
    enable/disable specific CPUs at various levels.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3ae89f09fc..1ccb2e4ab3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -391,7 +391,7 @@ func fxsave(*[FXREGS]uintptr)
 func _Gscpu() *cpu_t
 func gs_null()
 func gs_set(*cpu_t)
-func htpause()
+func Htpause()
 func invlpg(uintptr)
 func Inb(uint16) uint
 func Inl(int) int
@@ -1105,7 +1105,7 @@ func Splock(l *Spinlock_t) {
 			break
 		}
 		for l.v != 0 {
-			htpause()
+			Htpause()
 		}
 	}
 }

commit 878d28f8c995cc129073be59a68c9f9d6df03f7f
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Thu Feb 14 17:35:37 2019 -0500

    fix race between AP bootup and first TLB shootdown
    
    if a CPU sends a TLB shootdown IPI before an AP has enabled its LAPIC, the IPI
    will apparently be lost. the sender will loop forever, waiting for a TLB
    shootdown acknowledgment that will never occur.
    
    the race was unlikely to occur with low core counts, but with bhw2's 20 CPUs,
    it occurred every boot.
    
    fix this race by making the BSP wait until all APs have configured their
    LAPICs.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8e8946ca1f..3ae89f09fc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2078,7 +2078,7 @@ func lapic_setup(calibrate bool) {
 	wlap(LAPICNT, _lapic_quantum)
 
 	maskint := uint32(1 << 16)
-	// mask cmci, lint[01], error, perf counters, and thermal sensor
+	// mask cmci, error, perf counters, and thermal sensor
 	wlap(LVCMCI,    maskint)
 	// unmask LINT0 and LINT1
 	wlap(LVINT0,    rlap(LVINT0) &^ maskint)
@@ -2465,10 +2465,10 @@ func trap(tf *[TFSIZE]uintptr) {
 		wakeup()
 		if !yielding {
 			lap_eoi()
-			if cpu.num == 0 {
-				//wakeup()
-				proftick()
-			}
+			//if cpu.num == 0 {
+			//	//wakeup()
+			//	proftick()
+			//}
 		}
 		// yieldy doesn't return
 		yieldy()
@@ -2803,6 +2803,7 @@ out:
 
 var _lastprof int
 
+// XXX remove this crap
 //go:nosplit
 func proftick() {
 	// goprofile period = 10ms

commit 7d320f31de71051a996978dd6ced377e327d03d1
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Feb 13 15:26:13 2019 -0500

    disable syscall fast path
    
    it improves performance only a little and has been broken for a while.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d640aa8878..8e8946ca1f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -429,8 +429,8 @@ func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
 func _userret()
 func _Userrun(*[TFSIZE]uintptr, bool, *cpu_t) (int, int)
-func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
-    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
+//func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
+//    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
 func Wrmsr(int, int)
 
 // adds src to dst
@@ -574,14 +574,14 @@ func CPUHint() int {
 
 //go:nowritebarrierrec
 //go:nosplit
-func Userrun_slow(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
+func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
 	// while other cpus execute user programs.
 	//entersyscall(0)
-	//Cli()
+	Cli()
 	//Slows++
 	cpu := _Gscpu()
 

commit 485de00b5adb1ad0247a1684c4bf0f9b6f62b57e
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Feb 12 17:25:33 2019 -0500

    fix LAPIC version check

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ad7f963030..d640aa8878 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2021,7 +2021,7 @@ func lapic_setup(calibrate bool) {
 	_lapaddr = la
 
 	lver := rlap(LAPVER)
-	if lver < 0x10 {
+	if (lver & 0xff) < 0x10 {
 		pancake("82489dx not supported", uintptr(lver))
 	}
 
@@ -2092,6 +2092,9 @@ func lapic_setup(calibrate bool) {
 	if reg & (1 << 11) == 0 {
 		pancake("lapic disabled?", reg)
 	}
+	//if reg & (1 << 10) == 0 {
+	//	pmsg("x2APIC MODE ENABLED\n")
+	//}
 	if (reg >> 12) != 0xfee00 {
 		pancake("weird base addr?", reg >> 12)
 	}

commit 1dc8454c823a570c52bc57e8613cb41437c1d68d
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Feb 4 21:07:21 2019 -0500

    stores to MMIO registers must not be locked
    
    hurray, bhw2 finally works! goodbye four days of my life

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a6f83bd4b3..ad7f963030 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -420,6 +420,7 @@ func Rdtsc() uint64
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
 func Store32(*uint32, uint32)
+func Store64(*uint64, uint64)
 func stackcheck()
 func Sti()
 func _sysentry()

commit eda6a4d1ec6c8495558a86fb8457ecd093bdc3ac
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Aug 22 15:47:38 2018 -0400

    enable reservations after boot
    
    log recovery can allocate a lot, exhausting the 1MB boot reservation. init(1)
    will set the correct heap limit.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1c887a0a3a..a6f83bd4b3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3443,8 +3443,9 @@ func Setheap(n int) {
 	heapminimum = uint64(n)
 }
 
-// the units of maxheap and totalres is bytes
-var _maxheap int64 = 20 << 20
+// initial res size is huge to not disturb boot; init(1) will set it to correct
+// value.
+var _maxheap int64 = 1 << 31
 
 //type res_t struct {
 //	maxheap		int64

commit 4b8cf886583ad0909536cc14b6891cd4cbe10a80
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Aug 6 17:39:35 2018 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 78806de36e..1c887a0a3a 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3499,9 +3499,9 @@ func GCPacerToggle() {
 }
 
 //type Resobjs_t [_NumSizeClasses]uint32
-// the version of biscuit from the paper submission only allocates from 21 size
-// classes. Objsadd/Objssub assumes this is an array of 24 uint32s; fix Objsadd
-// if you change this type.
+// at the time of writing, biscuit allocates from 24 size classes.
+// Objsadd/Objssub assumes this is an array of 24 uint32s; fix Objsadd if you
+// change this type.
 type Resobjs_t [24]uint32 // NOTICE ABOVE!
 
 type Res_t struct {

commit ce3cd47a44aa2293d683ff0d87fa6f21df2e2272
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Jul 30 09:10:24 2018 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 62044319eb..78806de36e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3556,7 +3556,7 @@ func robcentral(scidx int, want uint32) uint32 {
 var Maxa	uint32
 var Byuf	[]uintptr
 
-func cas(ne uint32) {
+func casbt(ne uint32) {
 	for {
 		v := atomic.Load(&Maxa)
 		if ne <= v {

commit 5fc29bd3e1d27383e9d4b20f2693030f80e9d1e0
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Jul 28 18:22:09 2018 -0400

    SSE optimized, distributed size-class reservations
    
    (still uses live bytes since MAXLIVE doesn't support size-classes)
    
    throughput is still good: parrun performance is 16.1k msgs/sec and nginx is
    higher than the submission. using SIMD for adding/subtracting/comparing
    size-classes improves throughput by ~6% compared to iteration.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3245cb9d76..62044319eb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -432,6 +432,14 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
 func Wrmsr(int, int)
 
+// adds src to dst
+func Objsadd(src *Resobjs_t, dst *Resobjs_t)
+// subs src from dst
+func Objssub(src *Resobjs_t, dst *Resobjs_t)
+// returns a bit mask which is set when the corresponding element of a is
+// larger than b.
+func Objscmp(a *Resobjs_t, b *Resobjs_t) uint
+
 func Lfence()
 func Sfence()
 func Mfence()
@@ -3436,48 +3444,38 @@ func Setheap(n int) {
 }
 
 // the units of maxheap and totalres is bytes
-var _maxheap int64 = 1 << 30
-
-type res_t struct {
-	maxheap		int64
-	ostanding	int64
-	// reservations for ops that have finished; only finished ops must have
-	// their outstanding reservations reduced to actual live by GC
-	fin		int64
-	gclive		int64
-}
-
-var res = res_t{maxheap: _maxheap}
+var _maxheap int64 = 20 << 20
+
+//type res_t struct {
+//	maxheap		int64
+//	ostanding	int64
+//	// reservations for ops that have finished; only finished ops must have
+//	// their outstanding reservations reduced to actual live by GC
+//	fin		int64
+//	gclive		int64
+//}
+//
+//var res = res_t{maxheap: _maxheap}
 
 func SetMaxheap(n int) {
-	res.maxheap = int64(n)
-	_res.maxheap = int64(n)
+	//res.maxheap = int64(n)
+	_centralres.maxheap = int64(n)
+	GC()
 }
 
 // must only be called when the world is stopped at the end of a GC
-func gcrescycle(live uint64) {
-	res.gclive = int64(live)
-	res.fin = 0
-}
+//func gcrescycle(live uint64) {
+//	res.gclive = int64(live)
+//	res.fin = 0
+//}
 
-// returns true if the caller must evict their previous allocations (if any).
-// will use previous iterations reservation credit, if available.
-func Cacheres(_res int, init bool) bool {
-	res := int64(_res)
+func Cacheaccount() {
 	//gp := getg()
-	//used := gp.res.cacheallocs
-	//gp.res.cacheallocs = 0
-	//if !init && used < res {
-	//	res = used
+	//if gp.res.allocs < gp.res.took {
+	//	gp.res.allocs = gp.res.took
 	//}
-	return _restake(res)
-}
-
-func Cacheaccount() {
 	gp := getg()
-	if gp.res.allocs < gp.res.took {
-		gp.res.allocs = gp.res.took
-	}
+	gp.allused = true
 }
 
 func GCDebug(n int) {
@@ -3500,122 +3498,145 @@ func GCPacerToggle() {
 	}
 }
 
-func Memremain() int {
-	a := atomic.Loadint64(&res.ostanding)
-	b := atomic.Loadint64(&res.fin)
-	c := atomic.Loadint64(&res.gclive)
-	rem := res.maxheap - a - b - c
-	return int(rem)
-}
+//type Resobjs_t [_NumSizeClasses]uint32
+// the version of biscuit from the paper submission only allocates from 21 size
+// classes. Objsadd/Objssub assumes this is an array of 24 uint32s; fix Objsadd
+// if you change this type.
+type Resobjs_t [24]uint32 // NOTICE ABOVE!
 
-func Memleak(_n int) bool {
-	n := int64(_n)
-	r := _restake(n)
-	return r
-}
-
-func Memstat() (int64, int64) {
-	g := getg()
-	return g.res.took, g.res.allocs
+type Res_t struct {
+	Objs	Resobjs_t
 }
 
-func Memreserve(_n int) bool {
-	want := int64(_n)
-	return _restake(want)
+var _centralres = struct {
+	avail		Res_t
+	tmp		Res_t
+	// XXX remove when maxlive supports sizeclasses
+	maxheap		int64
+}{
+	avail: Res_t{Resobjs_t{1: uint32(_maxheap)}},
+	maxheap: _maxheap,
 }
 
-func Memresadd(_n int) bool {
-	return Memreserve(_n)
+// must only be called when the world is stopped at the end of a GC
+func grescycle(newlastgc *Res_t, livebytes uint64) {
+	if hackmode == 0 {
+		return
+	}
+	_centralres.tmp = Res_t{}
+	for _, p := range allp {
+		mc := p.mcache
+		mc.avail = Res_t{}
+		Objsadd(&mc.outstand.Objs, &_centralres.tmp.Objs)
+	}
+	// XXX
+	left := uint32(_centralres.maxheap - int64(livebytes))
+	left -= _centralres.tmp.Objs[1]
+	if int32(left) < 0 {
+		left = 0
+	}
+	_centralres.avail.Objs[1] = left
 }
 
-func _restake(want int64) bool {
+// may steal more credit than asked for
+func robcentral(scidx int, want uint32) uint32 {
+	p := &_centralres.avail.Objs[scidx]
 	for {
-		o := atomic.Loadint64(&res.ostanding)
-		b := atomic.Loadint64(&res.fin)
-		c := atomic.Loadint64(&res.gclive)
-
-		if o + b + c + want > res.maxheap {
-			return false
+		left := atomic.Load(p)
+		if want > left {
+			return 0
 		}
-		p := (*uint64)(unsafe.Pointer(&res.ostanding))
-		if atomic.Cas64(p, uint64(o), uint64(o + want)) {
-			break
+		took := want
+		if atomic.Cas(p, left, left - took) {
+			return took
 		}
 	}
-	g := getg()
-	g.res.took += want
-	return true
 }
 
-func Memunres() int {
-	g := getg()
-	r := g.res.took
-	alloc := g.res.allocs
-	g.res.allocs, g.res.took = 0, 0
+var Maxa	uint32
+var Byuf	[]uintptr
 
-	used := r
-	if alloc < used {
-		used = alloc
+func cas(ne uint32) {
+	for {
+		v := atomic.Load(&Maxa)
+		if ne <= v {
+			return
+		}
+		if atomic.Cas(&Maxa, v, ne) {
+			buf := make([]uintptr, 10)
+			got := callers(2, buf)
+			buf = buf[:got]
+			Byuf = buf
+			return
+		}
 	}
-
-	atomic.Xaddint64(&res.fin, used)
-	atomic.Xaddint64(&res.ostanding, -r)
-	return -1
-}
-
-type Res_t struct {
-	Objs	[_NumSizeClasses]uint32
 }
 
-var _res = struct {
-	lastgc		*Res_t
-	finished	*Res_t
-	// XXX remove when maxlive supports sizeclasses
-	maxheap		int64
-}{
-	maxheap: _maxheap,
-}
+var Flushlim uint32 = 1 << 24
 
-func HeapResInit(lastgc, finished *Res_t) {
-	_res.lastgc, _res.finished = lastgc, finished
-}
+func Greserve(want *Res_t) bool {
+	mp := acquirem()
+	g := getg()
+	mc := gomcache()
 
-// must only be called when the world is stopped at the end of a GC
-func grescycle(newlastgc *Res_t, livebytes uint64) {
-	if _res.lastgc == nil || _res.finished == nil {
-		return
+	//mc.alls++
+	if m := Objscmp(&want.Objs, &mc.avail.Objs); m == 0 {
+		Objsadd(&want.Objs, &g.res1.Objs)
+		Objsadd(&want.Objs, &mc.outstand.Objs)
+		Objssub(&want.Objs, &mc.avail.Objs)
+	} else {
+		//mc.robs++
+		for i := 0; i < len(mc.avail.Objs); i++ {
+			if m & (1 << uint(i)) != 0 {
+				//mc.robi++
+				rob := robcentral(i, want.Objs[i])
+				if rob == 0 {
+					releasem(mp)
+					return false
+				}
+				mc.avail.Objs[i] += rob
+			}
+		}
 	}
-	// XXX
-	_res.lastgc.Objs[1] = uint32((_res.maxheap - int64(livebytes)) >> 10)
-	*_res.finished = Res_t{}
+
+	releasem(mp)
+	return true
 }
 
-func Gresup(newres *Res_t) {
+func Gresrelease() {
 	g := getg()
-	for i, r := range newres.Objs {
-		g.res1.Objs[i] += r
+	mc := gomcache()
+
+	Objssub(&g.res1.Objs, &mc.outstand.Objs)
+	// compute unused objects
+	if g.used.Objs[1] < g.res1.Objs[1] {
+		Objssub(&g.used.Objs, &g.res1.Objs)
+	} else {
+		// this block is possible due to implicitly reserved
+		// allocations (for the first process, one-process exit,
+		// logging daemon, etc.)
+		g.res1.Objs[1] = 0
+	}
+	// add unused objects back to credit
+	if g.allused {
+		g.allused = false
+	} else {
+		Objsadd(&g.res1.Objs, &mc.avail.Objs)
+		// XXX SSE max?
+		if mc.avail.Objs[1] > Flushlim {
+			mc.avail.Objs[1] -= Flushlim
+			atomic.Xadd(&_centralres.avail.Objs[1], int32(Flushlim))
+		}
 	}
-}
 
-// caller must prevent concurrent reads/writes to heapr
-func Gresrelease(outstand *Res_t) {
-	g := getg()
-	res := g.res1.Objs
-	used := g.res1.Objs
-	out := outstand.Objs
-	for i := range res {
-		fin := res[i]
-		if used[i] < res[i] {
-			fin = used[i]
-		}
-		_res.finished.Objs[i] += fin
-		out[i] -= fin
-	}
-	// XXX make sure this just zeros
 	g.res1 = Res_t{}
 	g.used = Res_t{}
 }
 
+func Remain() int {
+	return int(_centralres.avail.Objs[1])
+}
+
 func Gptr() unsafe.Pointer {
 	gp := getg()
 	return gp.current

commit f643b76cc4e774f028f9042a555b9f824f646d0b
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Jul 24 17:02:36 2018 -0400

    checkpoint size-class reservations

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f2f867cda4..3245cb9d76 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3451,6 +3451,7 @@ var res = res_t{maxheap: _maxheap}
 
 func SetMaxheap(n int) {
 	res.maxheap = int64(n)
+	_res.maxheap = int64(n)
 }
 
 // must only be called when the world is stopped at the end of a GC
@@ -3562,6 +3563,59 @@ func Memunres() int {
 	return -1
 }
 
+type Res_t struct {
+	Objs	[_NumSizeClasses]uint32
+}
+
+var _res = struct {
+	lastgc		*Res_t
+	finished	*Res_t
+	// XXX remove when maxlive supports sizeclasses
+	maxheap		int64
+}{
+	maxheap: _maxheap,
+}
+
+func HeapResInit(lastgc, finished *Res_t) {
+	_res.lastgc, _res.finished = lastgc, finished
+}
+
+// must only be called when the world is stopped at the end of a GC
+func grescycle(newlastgc *Res_t, livebytes uint64) {
+	if _res.lastgc == nil || _res.finished == nil {
+		return
+	}
+	// XXX
+	_res.lastgc.Objs[1] = uint32((_res.maxheap - int64(livebytes)) >> 10)
+	*_res.finished = Res_t{}
+}
+
+func Gresup(newres *Res_t) {
+	g := getg()
+	for i, r := range newres.Objs {
+		g.res1.Objs[i] += r
+	}
+}
+
+// caller must prevent concurrent reads/writes to heapr
+func Gresrelease(outstand *Res_t) {
+	g := getg()
+	res := g.res1.Objs
+	used := g.res1.Objs
+	out := outstand.Objs
+	for i := range res {
+		fin := res[i]
+		if used[i] < res[i] {
+			fin = used[i]
+		}
+		_res.finished.Objs[i] += fin
+		out[i] -= fin
+	}
+	// XXX make sure this just zeros
+	g.res1 = Res_t{}
+	g.used = Res_t{}
+}
+
 func Gptr() unsafe.Pointer {
 	gp := getg()
 	return gp.current

commit 2925fec25aee2eee1954c3863c6e1deb8a19fce2
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Jul 24 11:04:30 2018 -0400

    simplify how the GC replenishes reservation credit

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6c9ca87737..f2f867cda4 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3453,17 +3453,10 @@ func SetMaxheap(n int) {
 	res.maxheap = int64(n)
 }
 
-func gcrescycle() {
-	p := (*uint64)(unsafe.Pointer(&res.fin))
-	var rl int64
-	for {
-		rl = atomic.Loadint64(&res.fin)
-		atomic.Xaddint64(&res.gclive, rl)
-		if atomic.Cas64(p, uint64(rl), 0) {
-			break
-		}
-		atomic.Xaddint64(&res.gclive, -rl)
-	}
+// must only be called when the world is stopped at the end of a GC
+func gcrescycle(live uint64) {
+	res.gclive = int64(live)
+	res.fin = 0
 }
 
 // returns true if the caller must evict their previous allocations (if any).

commit e02ae2be85c3ce8d75623784c6b08ff9e7145fd0
Merge: fc7f26c04f 38d7c136c9
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Jul 20 16:06:32 2018 -0400

    Merge remote-tracking branch 'origin/refactor' into refactmerge

commit 2081f1a3e270062fa2418a26dab4f37039c151ea
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Jul 11 12:31:04 2018 -0400

    runtime.[LSM]fence()

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 734c31b9c8..6211d68bcd 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -432,6 +432,10 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
 func Wrmsr(int, int)
 
+func Lfence()
+func Sfence()
+func Mfence()
+
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
 // that have the stack splitting prologue. the following is a list of go code

commit 68660f834baed6b60aa48a53d0d4e7b148355197
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Jun 27 17:13:20 2018 -0400

    dump caller rips

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a242b2360d..734c31b9c8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -756,6 +756,21 @@ var Lost struct {
 	User uint
 }
 
+var Freq uint = 4000
+
+func Bluh() {
+	if hackmode == 0 || dumrand(0, Freq) != 0 {
+		return
+	}
+	buf := make([]uintptr, 8)
+	got := callers(1, buf)
+	buf = buf[:got]
+	print("--\n")
+	for i := range buf {
+		print(hex(buf[i]), "\n")
+	}
+}
+
 //go:nosplit
 //go:nowritebarrierrec
 func _addone(rip uintptr) {

commit 5ad5266af47bd33f445372e92e5aa108b8b11bfa
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Jun 25 12:56:33 2018 -0400

    explicitly set mutator assist factor, cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a242b2360d..ab5d816db9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3479,6 +3479,14 @@ func GCDebugToggle() {
 	}
 }
 
+func GCPacerToggle() {
+	if debug.gcpacertrace != 0 {
+		debug.gcpacertrace = 0
+	} else {
+		debug.gcpacertrace = 1
+	}
+}
+
 func Memremain() int {
 	a := atomic.Loadint64(&res.ostanding)
 	b := atomic.Loadint64(&res.fin)

commit 5487da153bfc0da5fd974e6b000305d558b870a1
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue May 22 10:14:40 2018 -0400

    per-CPU backtrace support, too
    
    begin post-submission cleanup tasks!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0b086c988f..a242b2360d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -761,7 +761,8 @@ var Lost struct {
 func _addone(rip uintptr) {
 	idx := atomic.Xadd64(&nmiprof.bufidx, 2) - 2
 	if idx + 2 < uint64(len(nmiprof.buf)) {
-		nmiprof.buf[idx] = 0xfeedfacefeedface
+		cid := uintptr(NMI_Gscpu().num)
+		nmiprof.buf[idx] = 0xfeedfacefeed0000 | cid
 		nmiprof.buf[idx+1] = rip
 	}
 }
@@ -792,7 +793,8 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	idx := last - need
 	if last < uint64(len(nmiprof.buf)) {
 		dst := nmiprof.buf[idx:]
-		dst[0] = 0xdeadbeefdeadbeef
+		cid := uintptr(NMI_Gscpu().num)
+		dst[0] = 0xdeadbeefdead0000 | cid
 		dst = dst[1:]
 		copy(dst, buf)
 	} else {

commit 449f846c111657c25defe1efc355d58490aa351f
Merge: 86bd5624fa 70b528fd84
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed May 16 14:05:23 2018 -0400

    Merge branch 'remerge'
    
    merge go1.10.1! goodbye go1.8

commit 50ee3a12710b78ee4357aa5c1c5ce162d343b6f0
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed May 16 12:20:06 2018 -0400

    simpler gc printer

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 15cda047cb..f0d5d9d802 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3418,6 +3418,14 @@ func GCDebug(n int) {
 	debug.gctrace = int32(n)
 }
 
+func GCDebugToggle() {
+	if debug.gctrace != 0 {
+		debug.gctrace = 0
+	} else {
+		debug.gctrace = 1
+	}
+}
+
 func Memremain() int {
 	a := atomic.Loadint64(&res.ostanding)
 	b := atomic.Loadint64(&res.fin)
@@ -3432,6 +3440,11 @@ func Memleak(_n int) bool {
 	return r
 }
 
+func Memstat() (int64, int64) {
+	g := getg()
+	return g.res.took, g.res.allocs
+}
+
 func Memreserve(_n int) bool {
 	want := int64(_n)
 	return _restake(want)

commit 7cb9b125c79763f489e37c23d060911e11e96548
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed May 2 11:15:29 2018 -0400

    hacked per-cpu profiles

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1892f10458..15cda047cb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -869,7 +869,10 @@ func checky() {
 func perfgather(tf *[TFSIZE]uintptr) {
 	idx := atomic.Xadd64(&nmiprof.bufidx, 1) - 1
 	if idx < uint64(len(nmiprof.buf)) {
-		nmiprof.buf[idx] = tf[TF_RIP]
+		v := tf[TF_RIP]
+		id := uintptr(NMI_Gscpu().num)
+		v |= id << 56
+		nmiprof.buf[idx] = v
 	}
 	//_consumelbr()
 }

commit 01ec84acb8e3a21e95d7f256b2aa8b5c3aa1f5e3
Merge: e6d7de0057 5e7ee04735
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Apr 30 17:27:42 2018 -0400

    Merge branch 'master' (early part) into remerge

commit 21d15638fc7e8baab73083a326368d96c3520ea3
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Apr 23 16:37:43 2018 -0400

    PMI memory corruption bug
    
    if an PMI occurs after the "OS" scheduler stops executing a thread and unlocks
    the threadlock but before the CPU starts executing a new thread, the old thread
    may be picked up by another CPU and have its TLS change unexpectedly due to the
    setg() call

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 2f7fd542be..1892f10458 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -821,11 +821,6 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 		_pmsg("!")
 	}
 
-	if (tf[TF_CS] & 3) != 0 {
-		_addone(tf[TF_RIP])
-		//Lost.User++
-		return
-	}
 	// if the nmi occurred between swapgs pair, getg() will return garbage.
 	// detect this case by making sure gs does not point to the cpu_t
 	cpu := NMI_Gscpu()
@@ -834,6 +829,15 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 		//Lost.Gs++
 		return
 	}
+
+	// XXX
+	if (tf[TF_CS] & 3) != 0 || tf[TF_RFLAGS] & TF_FL_IF == 0 ||
+	    cpu.mythread == nil {
+		_addone(tf[TF_RIP])
+		//Lost.User++
+		return
+	}
+
 	og := getg()
 	if og.m == nil || og.m.gsignal == nil {
 		_addone(tf[TF_RIP])
@@ -843,13 +847,21 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 	if og == og.m.gsignal {
 		pancake("recursive NMIs?", 0)
 	}
-	// make sure we aren't preempted by GC
-	og.m.mallocing++
+	if og.m != og.m.gsignal.m {
+		pancake("oh shite", 0)
+	}
 	setg(og.m.gsignal)
 	// indirectly call nmibacktrace1()...
 	backtracetramp(og.m.gsignal.stack.hi, tf, og)
 	setg(og)
-	og.m.mallocing--
+}
+
+func checky() {
+	if hackmode != 0 {
+		if rflags() & TF_FL_IF == 0 {
+			pancake("must be interruptible", 0)
+		}
+	}
 }
 
 //go:nowritebarrierrec

commit 1a8df1e91a3c402557d345669d0ef1df41c249ab
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Apr 15 13:47:08 2018 -0400

    avoid write barrier in Ap_setup()
    
    be more careful and ensure no write barriers are present in the "shim" layer.
    
    everything seems to work.
    
    (cherry picked from go1.10.1 merge branch)

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f054a425c6..2f7fd542be 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -457,6 +457,14 @@ type cpu_t struct {
 	_		[56]uint8
 }
 
+func (c *cpu_t) _init(p *cpu_t) {
+	*(*uintptr)(unsafe.Pointer(&c.this)) = (uintptr)(unsafe.Pointer(p))
+}
+
+func (c *cpu_t) setthread(t *thread_t) {
+	*(*uintptr)(unsafe.Pointer(&c.mythread)) = (uintptr)(unsafe.Pointer(t))
+}
+
 var Cpumhz uint
 var Pspercycle uint
 
@@ -1780,6 +1788,7 @@ func Tcount() (int, int) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func get_pg() uintptr {
 	if pglast == 0 {
 		phys_init()
@@ -1798,6 +1807,7 @@ func FuncPC(f interface{}) uintptr {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	pte := pgdir_walk(va, true)
 	old := *pte
@@ -1817,6 +1827,7 @@ var Fxinit [FXREGS]uintptr
 
 // nosplit because APs call this function before FS is setup
 //go:nosplit
+//go:nowritebarrierrec
 func fpuinit(amfirst bool) {
 	finit()
 	cr0 := Rcr0()
@@ -1946,6 +1957,7 @@ func pit_disable() {
 
 // wait until 8254 resets the counter
 //go:nosplit
+//go:nowritebarrierrec
 func pit_phasewait() {
 	// 8254 timers are 16 bits, thus always smaller than last;
 	last := uint(1 << 16)
@@ -1961,6 +1973,7 @@ func pit_phasewait() {
 var _lapic_quantum uint32
 
 //go:nosplit
+//go:nowritebarrierrec
 func lapic_setup(calibrate bool) {
 	la := uintptr(0xfee00000)
 
@@ -2064,6 +2077,7 @@ func lapic_setup(calibrate bool) {
 	}
 }
 
+//go:nowritebarrierrec
 func proc_setup() {
 	_userintaddr = funcPC(_userint)
 	_sigsimaddr = funcPC(sigsim)
@@ -2075,7 +2089,7 @@ func proc_setup() {
 
 	// initialize GS pointers
 	for i := range cpus {
-		cpus[i].this = &cpus[i]
+		cpus[i]._init(&cpus[i])
 	}
 
 	lapic_setup(true)
@@ -2092,7 +2106,7 @@ func proc_setup() {
 	gs_set(&cpus[0])
 	cpus[0].apicid = lap_id()
 	Gscpu().num = 0
-	Gscpu().mythread = &threads[0]
+	Gscpu().setthread(&threads[0])
 }
 
 // XXX to prevent CPUs from calling zero_phys concurrently when allocating pmap
@@ -2100,6 +2114,7 @@ func proc_setup() {
 var joinlock = &Spinlock_t{}
 
 //go:nosplit
+//go:nowritebarrierrec
 func Ap_setup(cpunum uint) {
 	// interrupts are probably already cleared
 	fl := Pushcli()
@@ -2134,7 +2149,9 @@ func Ap_setup(cpunum uint) {
 	gs_null()
 	gs_set(mycpu)
 	Gscpu().num = cpunum
-	Gscpu().mythread = nil
+	// avoid write barrier before FS is set to TLS (via the "OS"
+	// scheduler).
+	Gscpu().setthread(nil)
 
 	Spunlock(joinlock)
 	Popcli(fl)
@@ -2156,6 +2173,7 @@ func sysc_setup(myrsp uintptr) {
 	Wrmsr(sysenter_esp, 0)
 }
 
+//go:nowritebarrierrec
 func Condflush(_refp *int32, p_pmap, va uintptr, pgcount int) bool {
 	var refp *uint32
 	var refc uint32
@@ -2199,6 +2217,7 @@ var Tlbshoot struct {
 
 // must be nosplit since called at interrupt time
 //go:nosplit
+//go:nowritebarrierrec
 func tlb_shootdown() {
 	ct := Gscpu().mythread
 	if ct != nil && Rcr3() == Tlbshoot.P_pmap {
@@ -2261,6 +2280,7 @@ func Install_traphandler(newtrap func(*[TFSIZE]uintptr)) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func stack_dump(rsp uintptr) {
 	pte := pgdir_walk(rsp, false)
 	_pmsg("STACK DUMP\n")
@@ -2286,6 +2306,7 @@ func stack_dump(rsp uintptr) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func kernel_fault(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 	_pmsg("trap frame at")
@@ -2444,11 +2465,13 @@ func trap(tf *[TFSIZE]uintptr) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func is_cpuex(trapno uintptr) bool {
 	return trapno < 32
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
 		pancake("must not be interruptible", 0)
@@ -2459,6 +2482,7 @@ func _tchk() {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func sched_halt() {
 	if rflags() & TF_FL_IF != 0 {
 		pancake("must not be interruptible", 0)
@@ -2487,6 +2511,7 @@ func sched_halt() {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func sched_run(t *thread_t) {
 	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
@@ -2495,8 +2520,7 @@ func sched_run(t *thread_t) {
 	// mythread never references a heap allocated object. avoid
 	// writebarrier since sched_run can be executed at any time, even when
 	// GC invariants do not hold (like when g.m.p == nil).
-	//Gscpu().mythread = t
-	*(*uintptr)(unsafe.Pointer(&Gscpu().mythread)) = uintptr(unsafe.Pointer(t))
+	Gscpu().setthread(t)
 	fxrstor(&t.fx)
 	// flush the TLB, otherwise the cpu may use a TLB entry for a page that
 	// has since been unmapped
@@ -2504,6 +2528,7 @@ func sched_run(t *thread_t) {
 	_trapret(&t.tf)
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func sched_resume(ct *thread_t) {
 	if ct != nil {
@@ -2513,6 +2538,7 @@ func sched_resume(ct *thread_t) {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func wakeup() {
 	_tchk()
@@ -2529,18 +2555,21 @@ func wakeup() {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func _waketimeout(now int, t *thread_t) bool {
 	sf := t.sleepfor
 	return t.status == ST_SLEEPING && sf != -1 && sf < now
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func yieldy() {
 	_yieldy()
 	sched_halt()
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func _yieldy() {
 	_tchk()
@@ -2589,9 +2618,11 @@ var _irqv struct {
 func IRQsched(irq uint) {
 	gp := getg()
 	gp.m.irqn = irq
+	gp.waitreason = "waiting for trap"
 	mcall(irqsched_m)
 }
 
+//go:nowritebarrierrec
 func irqsched_m(gp *g) {
 	// have new IRQs arrived?
 	irq := gp.m.irqn
@@ -2618,11 +2649,10 @@ func irqsched_m(gp *g) {
 	sleeping := _irqv.irqs & bit == 0
 	if sleeping {
 		nstatus = _Gwaiting
-		gp.waitreason = "waiting for trap"
 		if _irqv.handlers[irq].igp != nil {
 			pancake("igp exists", uintptr(irq))
 		}
-		_irqv.handlers[irq].igp = gp
+		setGNoWB(&_irqv.handlers[irq].igp, gp)
 		start = false
 	} else {
 		nstatus = _Grunnable
@@ -2701,8 +2731,7 @@ func IRQcheck(pp *p) {
 			if gst &^ _Gscan != _Gwaiting {
 				pancake("bad igp status", uintptr(gst))
 			}
-			*(*uintptr)(unsafe.Pointer(&_irqv.handlers[i].igp)) =
-			    uintptr(unsafe.Pointer(nil))
+			setGNoWB(&_irqv.handlers[i].igp, nil)
 			_irqv.handlers[i].started = true
 			// we cannot set gstatus or put to run queue before we
 			// release the spinlock since either operation may
@@ -3067,7 +3096,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	// avoid write barrier for mp here since we have interrupts clear. Ms
 	// are always reachable from allm anyway. see comments in runtime2.go
 	//gp.m = mp
-	*(*uintptr)(unsafe.Pointer(&gp.m)) = uintptr(unsafe.Pointer(mp))
+	setMNoWB(&gp.m, mp)
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))
 	mp.procid = uint64(ti)
 	mt.status = ST_RUNNABLE

commit 8e67edac1ffedd29d63c4790a26196d7989cbf09
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 21 12:54:16 2018 -0400

    debugging stuff

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ae5e18c2fd..f054a425c6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -746,9 +746,6 @@ var Lost struct {
 	Gs uint
 	User uint
 }
-var All uint
-
-var Tots int
 
 //go:nosplit
 //go:nowritebarrierrec
@@ -787,7 +784,6 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	if gp != gp.m.curg || stklock != nil {
 		did := gentraceback(pc, sp, 0, gp, 0, &buf[0], len(buf), nil,
 		    nil, flags)
-		Tots += did
 		buf = buf[:did]
 		need := uint64(len(buf) + 1)
 		last := atomic.Xadd64(&nmiprof.bufidx, int64(need))
@@ -813,7 +809,6 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 //go:nosplit
 //go:nowritebarrierrec
 func nmibacktrace(tf *[TFSIZE]uintptr) {
-	All++
 	if tf[TF_GSBASE] == 0 {
 		_pmsg("!")
 	}

commit 04ea2dcb99aac2a356870adb029893d16951f87a
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 21 12:47:45 2018 -0400

    res cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 2c1da345ca..ae5e18c2fd 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3370,7 +3370,9 @@ func Cacheres(_res int, init bool) bool {
 
 func Cacheaccount() {
 	gp := getg()
-	gp.res.allocs = gp.res.took
+	if gp.res.allocs < gp.res.took {
+		gp.res.allocs = gp.res.took
+	}
 }
 
 func GCDebug(n int) {

commit 788336d926020df5a56b62f8821ca35bffc2b110
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Apr 20 23:12:45 2018 -0400

    go less crazy with defer
    
    remove a few of the easily replaced defers. i left the defers in rename, open,
    unlink, and exec since replacing those is painful.
    
    before this patch, cmailbench spent ~9% of CPU cycles in defer related stuff.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6e44ced868..2c1da345ca 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -774,7 +774,7 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	// similar to sigprof()
 	if gp == nil || sp < gp.stack.lo || gp.stack.hi < sp || setsSP(pc) {
 		_addone(tf[TF_RIP])
-		Lost.Go++
+		//Lost.Go++
 		return
 	}
 
@@ -798,12 +798,12 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 			dst = dst[1:]
 			copy(dst, buf)
 		} else {
-			Lost.Full++
+			//Lost.Full++
 		}
 
 	} else {
 		_addone(tf[TF_RIP])
-		Lost.Go++
+		//Lost.Go++
 	}
 	if stklock != nil {
 		gcUnlockStackBarriers(stklock)
@@ -820,7 +820,7 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 
 	if (tf[TF_CS] & 3) != 0 {
 		_addone(tf[TF_RIP])
-		Lost.User++
+		//Lost.User++
 		return
 	}
 	// if the nmi occurred between swapgs pair, getg() will return garbage.
@@ -828,13 +828,13 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 	cpu := NMI_Gscpu()
 	if Gscpu() != cpu {
 		_addone(tf[TF_RIP])
-		Lost.Gs++
+		//Lost.Gs++
 		return
 	}
 	og := getg()
 	if og.m == nil || og.m.gsignal == nil {
 		_addone(tf[TF_RIP])
-		Lost.Go++
+		//Lost.Go++
 		return
 	}
 	if og == og.m.gsignal {

commit e6d7de0057ab1b00f3526f4bb89db100c9f6139a
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Apr 20 22:19:59 2018 -0400

    update go backtrace code for go1.10.1

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 62c05ef560..201d627425 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -787,35 +787,20 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 		return
 	}
 
-	var stklock *g
-	flags := uint(_TraceTrap)
-	if gp.m.curg != nil && gcTryLockStackBarriers(gp.m.curg) {
-		stklock = gp.m.curg
-		flags |= _TraceJumpStack
-	}
-	if gp != gp.m.curg || stklock != nil {
-		did := gentraceback(pc, sp, 0, gp, 0, &buf[0], len(buf), nil,
-		    nil, flags)
-		Tots += did
-		buf = buf[:did]
-		need := uint64(len(buf) + 1)
-		last := atomic.Xadd64(&nmiprof.bufidx, int64(need))
-		idx := last - need
-		if last < uint64(len(nmiprof.buf)) {
-			dst := nmiprof.buf[idx:]
-			dst[0] = 0xdeadbeefdeadbeef
-			dst = dst[1:]
-			copy(dst, buf)
-		} else {
-			Lost.Full++
-		}
-
+	did := gentraceback(pc, sp, 0, gp, 0, &buf[0], len(buf), nil,
+	    nil, _TraceTrap|_TraceJumpStack)
+	Tots += did
+	buf = buf[:did]
+	need := uint64(len(buf) + 1)
+	last := atomic.Xadd64(&nmiprof.bufidx, int64(need))
+	idx := last - need
+	if last < uint64(len(nmiprof.buf)) {
+		dst := nmiprof.buf[idx:]
+		dst[0] = 0xdeadbeefdeadbeef
+		dst = dst[1:]
+		copy(dst, buf)
 	} else {
-		_addone(tf[TF_RIP])
-		Lost.Go++
-	}
-	if stklock != nil {
-		gcUnlockStackBarriers(stklock)
+		Lost.Full++
 	}
 }
 

commit d5ca5f676b0ec7f112e33d2d648c60a7e727e212
Merge: 8294fc745a b8826d12a6
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Apr 20 22:15:47 2018 -0400

    Merge commit 'b8826d12a691bdd63c8d7522e9bc26a1df0d717a' into remerge

commit a0974516713f7f073f70c250926e652fef7dffcb
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Thu Apr 19 13:48:16 2018 -0400

    backtrace, graphviz support to munch.py
    
    sadly the graphviz crap was a complete waste of time!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c431b744ea..6e44ced868 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -761,7 +761,9 @@ func _addone(rip uintptr) {
 }
 
 // runs on gsignal stack so that we can use gentraceback(). 0xdeadbeefdeadbeef
-// and 0xfeedfacefeedface are sentinel values to indicate new backtraces.
+// and 0xfeedfacefeedface are sentinel values to indicate distinct backtraces.
+// 0xfeedfacefeedface and 0xdeadbeefdeadbeef indicate that a backtrace failed
+// (and thus only the RIP was recorded) or succeeded, respectively.
 //go:nowritebarrierrec
 func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	pc := tf[TF_RIP]

commit 22e4386647bb4e378aada14bad6f914f342efc39
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Thu Apr 19 10:33:13 2018 -0400

    backtrace flag to time(1)

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fe8ac13da6..c431b744ea 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -740,13 +740,28 @@ func _lbrreset(en bool) {
 
 func backtracetramp(uintptr, *[TFSIZE]uintptr, *g)
 
-var Lost1 uint
-var Lost2 uint
+var Lost struct {
+	Go uint
+	Full uint
+	Gs uint
+	User uint
+}
 var All uint
 
 var Tots int
 
-// runs on gsignal stack so that we can use gentraceback()
+//go:nosplit
+//go:nowritebarrierrec
+func _addone(rip uintptr) {
+	idx := atomic.Xadd64(&nmiprof.bufidx, 2) - 2
+	if idx + 2 < uint64(len(nmiprof.buf)) {
+		nmiprof.buf[idx] = 0xfeedfacefeedface
+		nmiprof.buf[idx+1] = rip
+	}
+}
+
+// runs on gsignal stack so that we can use gentraceback(). 0xdeadbeefdeadbeef
+// and 0xfeedfacefeedface are sentinel values to indicate new backtraces.
 //go:nowritebarrierrec
 func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	pc := tf[TF_RIP]
@@ -756,7 +771,8 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	buf := nmiprof.percpu[cpu.num].scratch[:]
 	// similar to sigprof()
 	if gp == nil || sp < gp.stack.lo || gp.stack.hi < sp || setsSP(pc) {
-		Lost1++
+		_addone(tf[TF_RIP])
+		Lost.Go++
 		return
 	}
 
@@ -780,11 +796,12 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 			dst = dst[1:]
 			copy(dst, buf)
 		} else {
-			Lost2++
+			Lost.Full++
 		}
 
 	} else {
-		Lost1++
+		_addone(tf[TF_RIP])
+		Lost.Go++
 	}
 	if stklock != nil {
 		gcUnlockStackBarriers(stklock)
@@ -800,19 +817,22 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 	}
 
 	if (tf[TF_CS] & 3) != 0 {
-		Lost1++
+		_addone(tf[TF_RIP])
+		Lost.User++
 		return
 	}
 	// if the nmi occurred between swapgs pair, getg() will return garbage.
 	// detect this case by making sure gs does not point to the cpu_t
 	cpu := NMI_Gscpu()
 	if Gscpu() != cpu {
-		Lost1++
+		_addone(tf[TF_RIP])
+		Lost.Gs++
 		return
 	}
 	og := getg()
-	if og == nil || og.m == nil || og.m.gsignal == nil {
-		Lost1++
+	if og.m == nil || og.m.gsignal == nil {
+		_addone(tf[TF_RIP])
+		Lost.Go++
 		return
 	}
 	if og == og.m.gsignal {
@@ -827,12 +847,12 @@ func nmibacktrace(tf *[TFSIZE]uintptr) {
 	og.m.mallocing--
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func perfgather(tf *[TFSIZE]uintptr) {
 	idx := atomic.Xadd64(&nmiprof.bufidx, 1) - 1
 	if idx < uint64(len(nmiprof.buf)) {
-		v := tf[TF_RIP]
-		nmiprof.buf[idx] = v
+		nmiprof.buf[idx] = tf[TF_RIP]
 	}
 	//_consumelbr()
 }

commit 89fffeb593db4be2013e1ac58e08203d07783e2a
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Apr 18 23:20:52 2018 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d8603c1932..fe8ac13da6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -766,7 +766,6 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 		stklock = gp.m.curg
 		flags |= _TraceJumpStack
 	}
-	did := 0
 	if gp != gp.m.curg || stklock != nil {
 		did := gentraceback(pc, sp, 0, gp, 0, &buf[0], len(buf), nil,
 		    nil, flags)
@@ -790,7 +789,6 @@ func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
 	if stklock != nil {
 		gcUnlockStackBarriers(stklock)
 	}
-	Tots += did
 }
 
 //go:nosplit

commit 06c8387457653b06a2c270fb43470217bbc077b2
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Wed Apr 18 21:58:07 2018 -0400

    record backtrace during PMC interrupt for kernel code
    
    a bit tricky since an NMI can occur when interrupts are masked, like between
    biscuit's swapgs pairs. luckily we can use go's backtrace code without much
    difficulty.
    
    hopefully this will be helpful in optimizing our benchmarks.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6d2679f001..d8603c1932 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -450,7 +450,11 @@ type cpu_t struct {
 	shadowfs	uintptr
 	tf	*[TFSIZE]uintptr
 	fxbuf	*[FXREGS]uintptr
-	_clpad		[56]uint8
+	// APIC id is nice because it is a reliable CPU identifier, even during
+	// NMI interrupts (which may occur during a swapgs pair, making Gscpu()
+	// return garbage during the NMI handler).
+	apicid		uint32
+	_		[56]uint8
 }
 
 var Cpumhz uint
@@ -520,6 +524,21 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
+//go:nosplit
+func NMI_Gscpu() *cpu_t {
+	if rflags() & TF_FL_IF != 0 {
+		pancake("must not be interruptible", 0)
+	}
+	me := lap_id()
+	for i := range cpus {
+		if cpus[i].apicid == me {
+			return &cpus[i]
+		}
+	}
+	pancake("apicid not found", 0)
+	return nil
+}
+
 // returns the logical CPU identifier on which the calling thread was executing
 // at some point.
 func CPUHint() int {
@@ -595,16 +614,23 @@ type nmiprof_t struct {
 	evtmin		uint
 	evtmax		uint
 	gctrl		int
+	backtracing	bool
+	percpu [MAXCPUS]struct {
+		scratch	[64]uintptr
+		tfx	[FXREGS]uintptr
+		_	[64]uint8
+	}
 }
 
 var _nmibuf [4096*10]uintptr
 var nmiprof = nmiprof_t{buf: _nmibuf[:]}
 
-func SetNMI(mask bool, evtsel int, min, max uint) {
+func SetNMI(mask bool, evtsel int, min, max uint, backtrace bool) {
 	nmiprof.LVTmask = mask
 	nmiprof.evtsel = evtsel
 	nmiprof.evtmin = min
 	nmiprof.evtmax = max
+	nmiprof.backtracing = backtrace
 	// create value for ia32_perf_global_ctrl, to easily enable pmcs. does
 	// not enable fixed function counters.
 	ax, _, _, _ := Cpuid(0xa, 0)
@@ -712,13 +738,101 @@ func _lbrreset(en bool) {
 	Wrmsr(ia32_debugctl, dv)
 }
 
+func backtracetramp(uintptr, *[TFSIZE]uintptr, *g)
+
+var Lost1 uint
+var Lost2 uint
+var All uint
+
+var Tots int
+
+// runs on gsignal stack so that we can use gentraceback()
+//go:nowritebarrierrec
+func nmibacktrace1(tf *[TFSIZE]uintptr, gp *g) {
+	pc := tf[TF_RIP]
+	sp := tf[TF_RSP]
+
+	cpu := NMI_Gscpu()
+	buf := nmiprof.percpu[cpu.num].scratch[:]
+	// similar to sigprof()
+	if gp == nil || sp < gp.stack.lo || gp.stack.hi < sp || setsSP(pc) {
+		Lost1++
+		return
+	}
+
+	var stklock *g
+	flags := uint(_TraceTrap)
+	if gp.m.curg != nil && gcTryLockStackBarriers(gp.m.curg) {
+		stklock = gp.m.curg
+		flags |= _TraceJumpStack
+	}
+	did := 0
+	if gp != gp.m.curg || stklock != nil {
+		did := gentraceback(pc, sp, 0, gp, 0, &buf[0], len(buf), nil,
+		    nil, flags)
+		Tots += did
+		buf = buf[:did]
+		need := uint64(len(buf) + 1)
+		last := atomic.Xadd64(&nmiprof.bufidx, int64(need))
+		idx := last - need
+		if last < uint64(len(nmiprof.buf)) {
+			dst := nmiprof.buf[idx:]
+			dst[0] = 0xdeadbeefdeadbeef
+			dst = dst[1:]
+			copy(dst, buf)
+		} else {
+			Lost2++
+		}
+
+	} else {
+		Lost1++
+	}
+	if stklock != nil {
+		gcUnlockStackBarriers(stklock)
+	}
+	Tots += did
+}
+
+//go:nosplit
+//go:nowritebarrierrec
+func nmibacktrace(tf *[TFSIZE]uintptr) {
+	All++
+	if tf[TF_GSBASE] == 0 {
+		_pmsg("!")
+	}
+
+	if (tf[TF_CS] & 3) != 0 {
+		Lost1++
+		return
+	}
+	// if the nmi occurred between swapgs pair, getg() will return garbage.
+	// detect this case by making sure gs does not point to the cpu_t
+	cpu := NMI_Gscpu()
+	if Gscpu() != cpu {
+		Lost1++
+		return
+	}
+	og := getg()
+	if og == nil || og.m == nil || og.m.gsignal == nil {
+		Lost1++
+		return
+	}
+	if og == og.m.gsignal {
+		pancake("recursive NMIs?", 0)
+	}
+	// make sure we aren't preempted by GC
+	og.m.mallocing++
+	setg(og.m.gsignal)
+	// indirectly call nmibacktrace1()...
+	backtracetramp(og.m.gsignal.stack.hi, tf, og)
+	setg(og)
+	og.m.mallocing--
+}
+
 //go:nosplit
 func perfgather(tf *[TFSIZE]uintptr) {
 	idx := atomic.Xadd64(&nmiprof.bufidx, 1) - 1
 	if idx < uint64(len(nmiprof.buf)) {
-		//nmiprof.buf[idx] = tf[TF_RIP]
-		//pid := Gscpu().pid
-		//v := tf[TF_RIP] | (pid << 56)
 		v := tf[TF_RIP]
 		nmiprof.buf[idx] = v
 	}
@@ -1710,6 +1824,9 @@ func fpuinit(amfirst bool) {
 		for i := range threads {
 			chkalign(unsafe.Pointer(&threads[i].fx), 16)
 		}
+		for i := range nmiprof.percpu {
+			chkalign(unsafe.Pointer(&nmiprof.percpu[i].tfx), 16)
+		}
 	}
 }
 
@@ -1958,6 +2075,7 @@ func proc_setup() {
 	myrsp := tss_init(0)
 	sysc_setup(myrsp)
 	gs_set(&cpus[0])
+	cpus[0].apicid = lap_id()
 	Gscpu().num = 0
 	Gscpu().mythread = &threads[0]
 }
@@ -1989,6 +2107,14 @@ func Ap_setup(cpunum uint) {
 	if mycpu.num != 0 {
 		pancake("cpu id conflict", uintptr(mycpu.num))
 	}
+	me := lap_id()
+	// sanity
+	for i := range cpus {
+		if cpus[i].apicid == me {
+			pancake("dup apic id?", uintptr(me))
+		}
+	}
+	mycpu.apicid = me
 	fs_null()
 	gs_null()
 	gs_set(mycpu)
@@ -2173,13 +2299,25 @@ func trap(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 
 	if trapno == TRAP_NMI {
-		// prevent SSE corruption: set TS in cr0 to make sure SSE
-		// instructions generate a fault
-		ts := uintptr(1 << 3)
-		Lcr0(Rcr0() | ts)
-		perfgather(tf)
-		perfmask()
-		Lcr0(Rcr0() &^ ts)
+		if nmiprof.backtracing {
+			// go's gentraceback() uses SSE instructions (to zero
+			// large stack variables), so save and restore SSE regs
+			// explicitly
+			cpu := NMI_Gscpu()
+			fxbuf := &nmiprof.percpu[cpu.num].tfx
+			fxsave(fxbuf)
+			nmibacktrace(tf)
+			perfmask()
+			fxrstor(fxbuf)
+		} else {
+			// prevent SSE corruption: set TS in cr0 to make sure
+			// SSE instructions generate a fault
+			ts := uintptr(1 << 3)
+			Lcr0(Rcr0() | ts)
+			perfgather(tf)
+			perfmask()
+			Lcr0(Rcr0() &^ ts)
+		}
 		_trapret(tf)
 	}
 
@@ -2203,8 +2341,8 @@ func trap(tf *[TFSIZE]uintptr) {
 
 	// don't add code before SSE context saving unless you've thought very
 	// carefully! it is easy to accidentally and silently corrupt SSE state
-	// (ie calling memmove indirectly by assignment of large datatypes)
-	// before it is saved below.
+	// (ie calling memmove indirectly by assignment or declaration of large
+	// datatypes) before it is saved below.
 
 	// save SSE state immediately before we clobber it
 	if ct != nil {

commit 032ae82bda72553ff583c69b7dc1da3d81479aa0
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Apr 15 13:47:08 2018 -0400

    avoid write barrier in Ap_setup()
    
    be more careful and ensure no write barriers are present in the "shim" layer.
    
    everything seems to work.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8a71024c3e..7a94524c41 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -454,6 +454,14 @@ type cpu_t struct {
 	_clpad		[56]uint8
 }
 
+func (c *cpu_t) _init(p *cpu_t) {
+	*(*uintptr)(unsafe.Pointer(&c.this)) = (uintptr)(unsafe.Pointer(p))
+}
+
+func (c *cpu_t) setthread(t *thread_t) {
+	*(*uintptr)(unsafe.Pointer(&c.mythread)) = (uintptr)(unsafe.Pointer(t))
+}
+
 var Cpumhz uint
 var Pspercycle uint
 
@@ -1652,6 +1660,7 @@ func Tcount() (int, int) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func get_pg() uintptr {
 	if pglast == 0 {
 		phys_init()
@@ -1670,6 +1679,7 @@ func FuncPC(f interface{}) uintptr {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	pte := pgdir_walk(va, true)
 	old := *pte
@@ -1689,6 +1699,7 @@ var Fxinit [FXREGS]uintptr
 
 // nosplit because APs call this function before FS is setup
 //go:nosplit
+//go:nowritebarrierrec
 func fpuinit(amfirst bool) {
 	finit()
 	cr0 := Rcr0()
@@ -1815,6 +1826,7 @@ func pit_disable() {
 
 // wait until 8254 resets the counter
 //go:nosplit
+//go:nowritebarrierrec
 func pit_phasewait() {
 	// 8254 timers are 16 bits, thus always smaller than last;
 	last := uint(1 << 16)
@@ -1830,6 +1842,7 @@ func pit_phasewait() {
 var _lapic_quantum uint32
 
 //go:nosplit
+//go:nowritebarrierrec
 func lapic_setup(calibrate bool) {
 	la := uintptr(0xfee00000)
 
@@ -1933,6 +1946,7 @@ func lapic_setup(calibrate bool) {
 	}
 }
 
+//go:nowritebarrierrec
 func proc_setup() {
 	_userintaddr = funcPC(_userint)
 	_sigsimaddr = funcPC(sigsim)
@@ -1944,7 +1958,7 @@ func proc_setup() {
 
 	// initialize GS pointers
 	for i := range cpus {
-		cpus[i].this = &cpus[i]
+		cpus[i]._init(&cpus[i])
 	}
 
 	lapic_setup(true)
@@ -1960,7 +1974,7 @@ func proc_setup() {
 	sysc_setup(myrsp)
 	gs_set(&cpus[0])
 	Gscpu().num = 0
-	Gscpu().mythread = &threads[0]
+	Gscpu().setthread(&threads[0])
 }
 
 // XXX to prevent CPUs from calling zero_phys concurrently when allocating pmap
@@ -1968,6 +1982,7 @@ func proc_setup() {
 var joinlock = &Spinlock_t{}
 
 //go:nosplit
+//go:nowritebarrierrec
 func Ap_setup(cpunum uint) {
 	// interrupts are probably already cleared
 	fl := Pushcli()
@@ -1994,7 +2009,9 @@ func Ap_setup(cpunum uint) {
 	gs_null()
 	gs_set(mycpu)
 	Gscpu().num = cpunum
-	Gscpu().mythread = nil
+	// avoid write barrier before FS is set to TLS (via the "OS"
+	// scheduler).
+	Gscpu().setthread(nil)
 
 	Spunlock(joinlock)
 	Popcli(fl)
@@ -2016,6 +2033,7 @@ func sysc_setup(myrsp uintptr) {
 	Wrmsr(sysenter_esp, 0)
 }
 
+//go:nowritebarrierrec
 func Condflush(_refp *int32, p_pmap, va uintptr, pgcount int) bool {
 	var refp *uint32
 	var refc uint32
@@ -2059,6 +2077,7 @@ var Tlbshoot struct {
 
 // must be nosplit since called at interrupt time
 //go:nosplit
+//go:nowritebarrierrec
 func tlb_shootdown() {
 	ct := Gscpu().mythread
 	if ct != nil && Rcr3() == Tlbshoot.P_pmap {
@@ -2121,6 +2140,7 @@ func Install_traphandler(newtrap func(*[TFSIZE]uintptr)) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func stack_dump(rsp uintptr) {
 	pte := pgdir_walk(rsp, false)
 	_pmsg("STACK DUMP\n")
@@ -2146,6 +2166,7 @@ func stack_dump(rsp uintptr) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func kernel_fault(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 	_pmsg("trap frame at")
@@ -2292,11 +2313,13 @@ func trap(tf *[TFSIZE]uintptr) {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func is_cpuex(trapno uintptr) bool {
 	return trapno < 32
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
 		pancake("must not be interruptible", 0)
@@ -2307,6 +2330,7 @@ func _tchk() {
 }
 
 //go:nosplit
+//go:nowritebarrierrec
 func sched_halt() {
 	if rflags() & TF_FL_IF != 0 {
 		pancake("must not be interruptible", 0)
@@ -2335,6 +2359,7 @@ func sched_halt() {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func sched_run(t *thread_t) {
 	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
@@ -2343,8 +2368,7 @@ func sched_run(t *thread_t) {
 	// mythread never references a heap allocated object. avoid
 	// writebarrier since sched_run can be executed at any time, even when
 	// GC invariants do not hold (like when g.m.p == nil).
-	//Gscpu().mythread = t
-	*(*uintptr)(unsafe.Pointer(&Gscpu().mythread)) = uintptr(unsafe.Pointer(t))
+	Gscpu().setthread(t)
 	fxrstor(&t.fx)
 	// flush the TLB, otherwise the cpu may use a TLB entry for a page that
 	// has since been unmapped
@@ -2352,6 +2376,7 @@ func sched_run(t *thread_t) {
 	_trapret(&t.tf)
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func sched_resume(ct *thread_t) {
 	if ct != nil {
@@ -2361,6 +2386,7 @@ func sched_resume(ct *thread_t) {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func wakeup() {
 	_tchk()
@@ -2377,18 +2403,21 @@ func wakeup() {
 	}
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func _waketimeout(now int, t *thread_t) bool {
 	sf := t.sleepfor
 	return t.status == ST_SLEEPING && sf != -1 && sf < now
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func yieldy() {
 	_yieldy()
 	sched_halt()
 }
 
+//go:nowritebarrierrec
 //go:nosplit
 func _yieldy() {
 	_tchk()
@@ -2437,9 +2466,11 @@ var _irqv struct {
 func IRQsched(irq uint) {
 	gp := getg()
 	gp.m.irqn = irq
+	gp.waitreason = "waiting for trap"
 	mcall(irqsched_m)
 }
 
+//go:nowritebarrierrec
 func irqsched_m(gp *g) {
 	// have new IRQs arrived?
 	irq := gp.m.irqn
@@ -2466,11 +2497,10 @@ func irqsched_m(gp *g) {
 	sleeping := _irqv.irqs & bit == 0
 	if sleeping {
 		nstatus = _Gwaiting
-		gp.waitreason = "waiting for trap"
 		if _irqv.handlers[irq].igp != nil {
 			pancake("igp exists", uintptr(irq))
 		}
-		_irqv.handlers[irq].igp = gp
+		setGNoWB(&_irqv.handlers[irq].igp, gp)
 		start = false
 	} else {
 		nstatus = _Grunnable
@@ -2549,8 +2579,7 @@ func IRQcheck(pp *p) {
 			if gst &^ _Gscan != _Gwaiting {
 				pancake("bad igp status", uintptr(gst))
 			}
-			*(*uintptr)(unsafe.Pointer(&_irqv.handlers[i].igp)) =
-			    uintptr(unsafe.Pointer(nil))
+			setGNoWB(&_irqv.handlers[i].igp, nil)
 			_irqv.handlers[i].started = true
 			// we cannot set gstatus or put to run queue before we
 			// release the spinlock since either operation may
@@ -2942,7 +2971,7 @@ func clone_wrap(rip uintptr) {
 
 var _cloneid int32
 
-//go:nowritebarrier
+//go:nowritebarrierrec
 func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) int32 {
 	// _CLONE_SYSVSEM is specified only for strict qemu-arm64 checks; the
 	// runtime doesn't use sysv sems, fortunately
@@ -2978,7 +3007,6 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) int32 {
 	// are always reachable from allm anyway. see comments in runtime2.go
 	//gp.m = mp
 	setMNoWB(&gp.m, mp)
-	//*(*uintptr)(unsafe.Pointer(&gp.m)) = uintptr(unsafe.Pointer(mp))
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))
 	mp.procid = uint64(ti)
 	mt.status = ST_RUNNABLE

commit f64130c1048a9af00cfa5472310df0374c25b8e6
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Apr 15 10:59:53 2018 -0400

    fix runtime startup, some cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 894b098135..8a71024c3e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2772,8 +2772,58 @@ func Pml4freeze() {
 	_nopml4 = true
 }
 
+// this function is dead-code; its purpose is to ensure that the compiler
+// generates an error if the arguments/return values of the runtime functions
+// do not match the hack hooks...
+func test_func_consist() {
+	if r1, r2 := hack_mmap(0, 0, 0, 0, 0, 0); r1 == 0 || r2 != 0 {
+	}
+	if r1, r2 := sysMmap(nil, 0, 0, 0, 0, 0); r1 == nil || r2 != 0 {
+	}
+
+	hack_munmap(0, 0)
+	sysMunmap(nil, 0)
+
+	hack_exit(0)
+	exit(0)
+
+	{
+		if r := write(0, nil, 0); r == 0 {
+		}
+		if r := hack_write(0, 0, 0); r == 0 {
+		}
+		usleep(0)
+		hack_usleep(0)
+	}
+	if r1 := nanotime(); r1 == 0 {
+	}
+	if r1 := hack_nanotime(); r1 == 0 {
+	}
+
+	if r1 := futex(nil, 0, 0, nil, nil, 0); r1 != 0 {
+	}
+	if r1 := hack_futex(nil, 0, 0, nil, nil, 0); r1 != 0 {
+	}
+
+	if r1 := clone(0, nil, nil, nil, nil); r1 != 0 {
+	}
+	if r1 := hack_clone(0, 0, nil, nil, 0); r1 != 0 {
+	}
+
+	sigaltstack(nil, nil)
+	hack_sigaltstack(nil, nil)
+
+	// importing syscall causes build to fail?
+	//if a, b, c := syscall.Syscall(0, 0, 0, 0); a == b || c == 0 {
+	//}
+	if a, b, c := hack_syscall(0, 0, 0, 0); a == b || c == 0 {
+	}
+}
+
+//var didsz uintptr
+
 func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
-    fd int32, offset int32) uintptr {
+    fd int32, offset int32) (uintptr, int) {
 	fl := Pushcli()
 	Splock(maplock)
 
@@ -2787,19 +2837,27 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 	var vaend uintptr
 	var perms uintptr
 	var ret uintptr
+	var err int
 	var t uintptr
 	pgleft := pglast - pgfirst
 	sz := pgroundup(_sz)
 	if sz > pgleft {
 		ret = ^uintptr(0)
+		err = -12 // ENOMEM
 		goto out
 	}
 	sz = pgroundup(va + _sz)
 	sz -= pgrounddown(va)
 	if va == 0 {
+		//_pmsg("ZERO\n")
 		va = find_empty(sz)
 	}
 	vaend = caddr(VUEND, 0, 0, 0, 0)
+	//_pmsg("--"); _pnum(didsz); _pmsg("--\n")
+	//_pnum(va); _pmsg("\n")
+	//_pnum(sz); _pmsg("\n")
+	//_pnum(va + sz); _pmsg("\n")
+	//_pnum(vaend); _pmsg("\n")
 	if va >= vaend || va + sz >= vaend {
 		pancake("va space exhausted", va)
 	}
@@ -2810,6 +2868,7 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 	}
 	perms = PTE_P
 	if prot == PROT_NONE {
+		//_pmsg("PROT_NONE\n")
 		prot_none(va, sz)
 		ret = va
 		goto out
@@ -2834,10 +2893,11 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 		alloc_map(va + i, perms, true)
 	}
 	ret = va
+	//didsz += sz
 out:
 	Spunlock(maplock)
 	Popcli(fl)
-	return ret
+	return ret, err
 }
 
 func hack_munmap(v, _sz uintptr) {
@@ -2880,14 +2940,14 @@ func clone_wrap(rip uintptr) {
 	pancake("clone_wrap returned", 0)
 }
 
-func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
-	CLONE_VM := 0x100
-	CLONE_FS := 0x200
-	CLONE_FILES := 0x400
-	CLONE_SIGHAND := 0x800
-	CLONE_THREAD := 0x10000
-	chk := uint32(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
-	    CLONE_THREAD)
+var _cloneid int32
+
+//go:nowritebarrier
+func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) int32 {
+	// _CLONE_SYSVSEM is specified only for strict qemu-arm64 checks; the
+	// runtime doesn't use sysv sems, fortunately
+	chk := uint32(_CLONE_VM | _CLONE_FS | _CLONE_FILES | _CLONE_SIGHAND |
+	    _CLONE_THREAD | _CLONE_SYSVSEM)
 	if flags != chk {
 		pancake("unexpected clone args", uintptr(flags))
 	}
@@ -2895,6 +2955,8 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 
 	fl := Pushcli()
 	Splock(threadlock)
+	_cloneid++
+	ret := _cloneid
 
 	ti := thread_avail()
 	// provide fn as arg to clone_wrap
@@ -2915,7 +2977,8 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	// avoid write barrier for mp here since we have interrupts clear. Ms
 	// are always reachable from allm anyway. see comments in runtime2.go
 	//gp.m = mp
-	*(*uintptr)(unsafe.Pointer(&gp.m)) = uintptr(unsafe.Pointer(mp))
+	setMNoWB(&gp.m, mp)
+	//*(*uintptr)(unsafe.Pointer(&gp.m)) = uintptr(unsafe.Pointer(mp))
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))
 	mp.procid = uint64(ti)
 	mt.status = ST_RUNNABLE
@@ -2925,8 +2988,11 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 
 	Spunlock(threadlock)
 	Popcli(fl)
+
+	return ret
 }
 
+// XXX remove goprof stuff
 func hack_setitimer(timer uint32, new, old *itimerval) {
 	TIMER_PROF := uint32(2)
 	if timer != TIMER_PROF {
@@ -3030,8 +3096,8 @@ var futexlock = &Spinlock_t{}
 func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
     val2 int32) int64 {
 	stackcheck()
-	FUTEX_WAIT := int32(0)
-	FUTEX_WAKE := int32(1)
+	FUTEX_WAIT := int32(0) | _FUTEX_PRIVATE_FLAG
+	FUTEX_WAKE := int32(1) | _FUTEX_PRIVATE_FLAG
 	uaddrn := uintptr(unsafe.Pointer(uaddr))
 	ret := 0
 	switch op {
@@ -3093,7 +3159,7 @@ func hack_usleep(delay int64) {
 	ts.tv_sec = delay/1000000
 	ts.tv_nsec = (delay%1000000)*1000
 	dummy := int32(0)
-	FUTEX_WAIT := int32(0)
+	FUTEX_WAIT := int32(0) | _FUTEX_PRIVATE_FLAG
 	hack_futex(&dummy, FUTEX_WAIT, 0, &ts, nil, 0)
 }
 

commit f5a071578940cdc8c89faabb6b20747ed6a8619d
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 14 11:46:24 2018 -0400

    merge startup path with go1.10.1

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index cf7734a390..894b098135 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -82,32 +82,32 @@ func futexwakeup(addr *uint32, cnt uint32) {
 	*(*int32)(unsafe.Pointer(uintptr(0x1006))) = 0x1006
 }
 
-//func getproccount() int32 {
-//	// This buffer is huge (8 kB) but we are on the system stack
-//	// and there should be plenty of space (64 kB).
-//	// Also this is a leaf, so we're not holding up the memory for long.
-//	// See golang.org/issue/11823.
-//	// The suggested behavior here is to keep trying with ever-larger
-//	// buffers, but we don't have a dynamic memory allocator at the
-//	// moment, so that's a bit tricky and seems like overkill.
-//	const maxCPUs = 64 * 1024
-//	var buf [maxCPUs / 8]byte
-//	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
-//	if r < 0 {
-//		return 1
-//	}
-//	n := int32(0)
-//	for _, v := range buf[:r] {
-//		for v != 0 {
-//			n += int32(v & 1)
-//			v >>= 1
-//		}
-//	}
-//	if n == 0 {
-//		n = 1
-//	}
-//	return n
-//}
+func getproccount() int32 {
+	// This buffer is huge (8 kB) but we are on the system stack
+	// and there should be plenty of space (64 kB).
+	// Also this is a leaf, so we're not holding up the memory for long.
+	// See golang.org/issue/11823.
+	// The suggested behavior here is to keep trying with ever-larger
+	// buffers, but we don't have a dynamic memory allocator at the
+	// moment, so that's a bit tricky and seems like overkill.
+	const maxCPUs = 64 * 1024
+	var buf [maxCPUs / 8]byte
+	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
+	if r < 0 {
+		return 1
+	}
+	n := int32(0)
+	for _, v := range buf[:r] {
+		for v != 0 {
+			n += int32(v & 1)
+			v >>= 1
+		}
+	}
+	if n == 0 {
+		n = 1
+	}
+	return n
+}
 
 // Clone, the Linux rfork.
 const (
@@ -275,8 +275,13 @@ func sysauxv(auxv []uintptr) int {
 }
 
 func osinit() {
-	//ncpu = getproccount()
-	ncpu = 1
+	if hackmode != 0 {
+		// the kernel uses Setncpu() to update ncpu to the number of
+		// booted CPUs on startup
+		ncpu = 1
+	} else {
+		ncpu = getproccount()
+	}
 	physPageSize = 4096
 }
 

commit 5c034daf10983f18fe9e629f9daa0c20b09fa1c4
Merge: 8d0a669400 9d4215311b
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sat Apr 14 07:50:10 2018 -0400

    Merge commit '9d4215311ba573a5b676de85b053eec03e577478' into rememrge

commit d26e091e4ea5a80357dfb105ca3e785f51c19713
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Apr 13 16:21:23 2018 -0400

    mark kernel text/bss/heap pages "global"
    
    reduces TLB misses for context-switch heavy benchmarks, like ping-pong

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 58878e75e9..6d2679f001 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1664,6 +1664,10 @@ func get_pg() uintptr {
 	return ret
 }
 
+func FuncPC(f interface{}) uintptr {
+	return funcPC(f)
+}
+
 //go:nosplit
 func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	pte := pgdir_walk(va, true)
@@ -1674,7 +1678,7 @@ func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	p_pg := get_pg()
 	zero_phys(p_pg)
 	// XXX goodbye, memory
-	*pte = p_pg | perms | PTE_P
+	*pte = p_pg | perms | PTE_P | PTE_G
 	if old & PTE_P != 0 {
 		invlpg(va)
 	}

commit 690b83a1ace12d6985b0d3e9235e011acb87b0fc
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Apr 3 14:22:09 2018 -0400

    more to the pile of fast syscall hacks
    
    add syscall fast path. i'm tired of optimizing syscall overhead!
    
    biscuit's syscall design incurs additional overhead. it is convenient to treat
    a return to userspace as a function call (i.e. Userrun()) from the kernel. the
    result is that the corresponding return instruction is executed after switching
    to and back from usermode, which is quite slow (70us/getppid vs 50us/gettpid).
    
    i suspect my CPU flushes the return buffer after switching privilege modes. it
    isn't TLB flushes since the TLB was not flushed (getppid benchmark) and
    everything should have been in the cache...

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f97df17442..58878e75e9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -426,7 +426,9 @@ func _trapret(*[TFSIZE]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
 func _userret()
-func _Userrun(*[TFSIZE]uintptr, bool) (int, int)
+func _Userrun(*[TFSIZE]uintptr, bool, *cpu_t) (int, int)
+func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
+    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool)
 func Wrmsr(int, int)
 
 // we have to carefully write go code that may be executed early (during boot)
@@ -484,6 +486,8 @@ type thread_t struct {
 	_pad2		int
 }
 
+//var DUR uintptr
+
 // XXX fix these misleading names
 const(
   TFSIZE       = 24
@@ -525,15 +529,19 @@ func CPUHint() int {
 	return ret
 }
 
+//var Slows int
+
 //go:nowritebarrierrec
-func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
+//go:nosplit
+func Userrun_slow(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
 	// while other cpus execute user programs.
 	//entersyscall(0)
-	Cli()
+	//Cli()
+	//Slows++
 	cpu := _Gscpu()
 
 	var opmap uintptr
@@ -572,7 +580,7 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
 	if !fastret {
 		fxrstor(fxbuf)
 	}
-	intno, aux := _Userrun(tf, fastret)
+	intno, aux := _Userrun(tf, fastret, cpu)
 
 	Sti()
 	//exitsyscall(0)
@@ -589,7 +597,7 @@ type nmiprof_t struct {
 	gctrl		int
 }
 
-var _nmibuf [4096]uintptr
+var _nmibuf [4096*10]uintptr
 var nmiprof = nmiprof_t{buf: _nmibuf[:]}
 
 func SetNMI(mask bool, evtsel int, min, max uint) {

commit 7d7aee8e74b7f71bf2d26882e8ad5bf78f9bac86
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Mar 25 13:03:01 2018 -0400

    reservation counter bugfix

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 34ad42a4e2..f97df17442 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3177,11 +3177,12 @@ func gcrescycle() {
 	var rl int64
 	for {
 		rl = atomic.Loadint64(&res.fin)
+		atomic.Xaddint64(&res.gclive, rl)
 		if atomic.Cas64(p, uint64(rl), 0) {
 			break
 		}
+		atomic.Xaddint64(&res.gclive, -rl)
 	}
-	atomic.Xaddint64(&res.gclive, rl)
 }
 
 // returns true if the caller must evict their previous allocations (if any).

commit 82a3d81b9af3ffe7a4b1416dbebf52186fba5ebd
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Mar 19 09:16:10 2018 -0400

    SQUASH ACCOUNTING

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d35844cdec..34ad42a4e2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3199,7 +3199,7 @@ func Cacheres(_res int, init bool) bool {
 
 func Cacheaccount() {
 	gp := getg()
-	gp.res.allocs = gp.took
+	gp.res.allocs = gp.res.took
 }
 
 func GCDebug(n int) {

commit d1a7bab6d0f11d4d3b725b133a8dbf9561a28024
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Mar 19 09:12:00 2018 -0400

    better and simpler live data accounting

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 646c637ff1..d35844cdec 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3154,127 +3154,114 @@ func Setheap(n int) {
 	heapminimum = uint64(n)
 }
 
-// the units of maxheap and rescredit is bytes
-var maxheap int64 = 1 << 30
-var rescredit int64 = maxheap
+// the units of maxheap and totalres is bytes
+var _maxheap int64 = 1 << 30
 
-var lastswept	int64
+type res_t struct {
+	maxheap		int64
+	ostanding	int64
+	// reservations for ops that have finished; only finished ops must have
+	// their outstanding reservations reduced to actual live by GC
+	fin		int64
+	gclive		int64
+}
 
-var Printres bool
+var res = res_t{maxheap: _maxheap}
 
-var nocreds uint64
+func SetMaxheap(n int) {
+	res.maxheap = int64(n)
+}
 
-func Setmaxheap(n int) {
-	// XXX if less than last maxheap, make sure the difference is no larger
-	// than available reservation
-	maxheap = int64(n)
+func gcrescycle() {
+	p := (*uint64)(unsafe.Pointer(&res.fin))
+	var rl int64
+	for {
+		rl = atomic.Loadint64(&res.fin)
+		if atomic.Cas64(p, uint64(rl), 0) {
+			break
+		}
+	}
+	atomic.Xaddint64(&res.gclive, rl)
 }
 
 // returns true if the caller must evict their previous allocations (if any).
 // will use previous iterations reservation credit, if available.
 func Cacheres(_res int, init bool) bool {
 	res := int64(_res)
-	gp := getg()
-	used := gp.res.cacheallocs
-	gp.res.cacheallocs = 0
-	if !init && used < res {
-		res = used
-	}
-	return _restake(res, true)
+	//gp := getg()
+	//used := gp.res.cacheallocs
+	//gp.res.cacheallocs = 0
+	//if !init && used < res {
+	//	res = used
+	//}
+	return _restake(res)
 }
 
-func GCDebug() {
-	debug.gctrace = 1
+func Cacheaccount() {
+	gp := getg()
+	gp.res.allocs = gp.took
 }
 
-func Getgot() int {
-	gp := getg()
-	return int(gp.res.got)
+func GCDebug(n int) {
+	debug.gctrace = int32(n)
 }
 
 func Memremain() int {
-	return int(atomic.Loadint64(&rescredit))
+	a := atomic.Loadint64(&res.ostanding)
+	b := atomic.Loadint64(&res.fin)
+	c := atomic.Loadint64(&res.gclive)
+	rem := res.maxheap - a - b - c
+	return int(rem)
 }
 
 func Memleak(_n int) bool {
 	n := int64(_n)
-	r := _restake(n, true)
-	if r {
-		g := getg()
-		g.res.credit -= n
-		g.res.got -= n
-	}
+	r := _restake(n)
 	return r
 }
 
-func Memreserve(_n int, rec bool) bool {
+func Memreserve(_n int) bool {
 	want := int64(_n)
-	g := getg()
-	if g.res.credit != 0 {
-		print("inconsistent state?\n")
-		pcbuf := make([]uintptr, 15)
-		got := callers(1, pcbuf)
-		pcbuf = pcbuf[:got]
-		for _, rip := range pcbuf {
-			print("\t", hex(rip), "\n")
-		}
-	}
-	return _restake(want, rec)
+	return _restake(want)
 }
 
-func Memresadd(_n int, rec bool) (bool, int64) {
-	want := int64(_n)
-	g := getg()
-	if g.res.got <= 0 {
-		pcbuf := make([]uintptr, 15)
-		got := callers(1, pcbuf)
-		pcbuf = pcbuf[:got]
-		print("no res?\n")
-		for _, rip := range pcbuf {
-			print("\t", hex(rip), "\n")
-		}
-	}
-	got := _restake(want, rec)
-	return got, g.res.got
+func Memresadd(_n int) bool {
+	return Memreserve(_n)
 }
 
-var Maxgot uintptr
-
-func _restake(want int64, rec bool) bool {
-	g := getg()
+func _restake(want int64) bool {
 	for {
-		v := atomic.Loadint64(&rescredit)
-		if want > v {
+		o := atomic.Loadint64(&res.ostanding)
+		b := atomic.Loadint64(&res.fin)
+		c := atomic.Loadint64(&res.gclive)
+
+		if o + b + c + want > res.maxheap {
 			return false
 		}
-		if atomic.Cas64((*uint64)(unsafe.Pointer(&rescredit)), uint64(v), uint64(v - want)) {
-			g.res.credit += want
-			g.res.got += want
-			dur := uintptr(g.res.got)
-			for rec {
-				v := atomic.Loaduintptr(&Maxgot)
-				if dur <= v {
-					break
-				}
-				if atomic.Casuintptr(&Maxgot, v, dur) {
-					break
-				}
-			}
-			return true
+		p := (*uint64)(unsafe.Pointer(&res.ostanding))
+		if atomic.Cas64(p, uint64(o), uint64(o + want)) {
+			break
 		}
 	}
+	g := getg()
+	g.res.took += want
+	return true
 }
 
 func Memunres() int {
 	g := getg()
-	left := g.res.credit
-	used := g.res.got - left
-	g.res.credit = 0
-	g.res.got = 0
-	if left > 0 {
-		atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
-	}
-	return int(used)
+	r := g.res.took
+	alloc := g.res.allocs
+	g.res.allocs, g.res.took = 0, 0
+
+	used := r
+	if alloc < used {
+		used = alloc
+	}
+
+	atomic.Xaddint64(&res.fin, used)
+	atomic.Xaddint64(&res.ostanding, -r)
+	return -1
 }
 
 func Gptr() unsafe.Pointer {

commit ea849969c7a51f495d9a576ca87550ca0589e263
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Mar 16 15:45:19 2018 -0400

    OOM killer
    
    successfully kills the naughty program and leaves the good citizen program be.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 639be0077e..646c637ff1 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3183,6 +3183,10 @@ func Cacheres(_res int, init bool) bool {
 	return _restake(res, true)
 }
 
+func GCDebug() {
+	debug.gctrace = 1
+}
+
 func Getgot() int {
 	gp := getg()
 	return int(gp.res.got)

commit df63bb9130f9e4617cd7a53ad774dd1e70df1207
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Mar 16 11:26:22 2018 -0400

    reservation leaking
    
    useful for testing

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 5dd313d1f3..639be0077e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3188,6 +3188,21 @@ func Getgot() int {
 	return int(gp.res.got)
 }
 
+func Memremain() int {
+	return int(atomic.Loadint64(&rescredit))
+}
+
+func Memleak(_n int) bool {
+	n := int64(_n)
+	r := _restake(n, true)
+	if r {
+		g := getg()
+		g.res.credit -= n
+		g.res.got -= n
+	}
+	return r
+}
+
 func Memreserve(_n int, rec bool) bool {
 	want := int64(_n)
 	g := getg()

commit da6c5eec72494efb147710361331271da4a4e5ff
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Thu Mar 15 10:07:39 2018 -0400

    res size dumper

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 41583a5fa7..5dd313d1f3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3203,7 +3203,7 @@ func Memreserve(_n int, rec bool) bool {
 	return _restake(want, rec)
 }
 
-func Memresadd(_n int, rec bool) bool {
+func Memresadd(_n int, rec bool) (bool, int64) {
 	want := int64(_n)
 	g := getg()
 	if g.res.got <= 0 {
@@ -3215,7 +3215,8 @@ func Memresadd(_n int, rec bool) bool {
 			print("\t", hex(rip), "\n")
 		}
 	}
-	return _restake(want, rec)
+	got := _restake(want, rec)
+	return got, g.res.got
 }
 
 var Maxgot uintptr

commit de642bbc41dd50e6a1be4b92f154111b7e9064b3
Merge: 0f6136c03c 61e55ea75d
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Mar 11 15:25:50 2018 -0400

    Merge branch 'package' into package-res

commit 61e55ea75dca6907266f1f41c433d2ce49492346
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Mar 11 13:20:52 2018 -0400

    bugfix: deadlock in interrupt waking code with concurrent GC
    
    release spinlock and re-enable interrupts before calling functions that may
    split the stack, preempting the thread into a GC.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3c91784424..7921181185 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2435,6 +2435,10 @@ func irqsched_m(gp *g) {
 		throw("bad irq " + string(irq))
 	}
 
+	// dropg() before taking spinlock since it can split the stack
+	pp := gp.m.p.ptr()
+	dropg()
+
 	fl := Pushcli()
 	Splock(&_irqv.slock)
 
@@ -2464,19 +2468,19 @@ func irqsched_m(gp *g) {
 	}
 
 	_irqv.handlers[irq].started = start
+
 	// _Gscan shouldn't be set since gp's status is running
 	casgstatus(gp, _Grunning, nstatus)
-	pp := gp.m.p.ptr()
-	dropg()
+	Spunlock(&_irqv.slock)
+	Popcli(fl)
 
+	// call runqput() only after re-enabling interrupts since runqput() can
+	// split the stack
 	if !sleeping {
 		// casgstatus must happen before runqput
 		runqput(pp, gp, true)
 	}
 
-	Spunlock(&_irqv.slock)
-	Popcli(fl)
-
 	schedule()
 }
 

commit fd082153c52093c5abd8494bfa3b2d7b64a62536
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Mar 9 13:30:34 2018 -0500

    syscall restarter/reservation failure tester and hacks for user FS
    
    it instantly found bugs. hacks to prevent user-mode FS from bothering with
    (failed) reservations.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 41c16deffe..0281cfc334 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3169,11 +3169,6 @@ func Setmaxheap(n int) {
 // returns true if the caller must evict their previous allocations (if any).
 // will use previous iterations reservation credit, if available.
 func Cacheres(_res int, init bool) bool {
-	// the FS running in user-mode can call memory reservation
-	// functions
-	if hackmode == 0 {
-		return true
-	}
 	res := int64(_res)
 	gp := getg()
 	used := gp.res.cacheallocs
@@ -3190,12 +3185,6 @@ func Getgot() int {
 }
 
 func Memreserve(_n int, rec bool) bool {
-	// the FS running in user-mode can call memory reservation
-	// functions
-	if hackmode == 0 {
-		return true
-	}
-
 	want := int64(_n)
 	g := getg()
 	if g.res.credit != 0 {
@@ -3211,12 +3200,6 @@ func Memreserve(_n int, rec bool) bool {
 }
 
 func Memresadd(_n int, rec bool) bool {
-	// the FS running in user-mode can call memory reservation
-	// functions
-	if hackmode == 0 {
-		return true
-	}
-
 	want := int64(_n)
 	g := getg()
 	if g.res.got <= 0 {
@@ -3259,12 +3242,6 @@ func _restake(want int64, rec bool) bool {
 }
 
 func Memunres() int {
-	// the FS running in user-mode can call memory reservation
-	// functions
-	if hackmode == 0 {
-		return 0
-	}
-
 	g := getg()
 	left := g.res.credit
 	used := g.res.got - left

commit cf916a427f13aa56419293cc7500d86afdb12b56
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Fri Mar 9 12:02:39 2018 -0500

    explicitly try to evict cache allocations
    
    in the two important places: ifree() and bitmap_t.apply

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 848de008a2..41c16deffe 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3178,15 +3178,17 @@ func Cacheres(_res int, init bool) bool {
 	gp := getg()
 	used := gp.res.cacheallocs
 	gp.res.cacheallocs = 0
-	if init {
-		used = res
-	}
-	if used < res {
+	if !init && used < res {
 		res = used
 	}
 	return _restake(res, true)
 }
 
+func Getgot() int {
+	gp := getg()
+	return int(gp.res.got)
+}
+
 func Memreserve(_n int, rec bool) bool {
 	// the FS running in user-mode can call memory reservation
 	// functions

commit f3f5f03213171a628ad5b49da012105471034775
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Thu Mar 8 19:39:20 2018 -0500

    handle cached allocations
    
    for ifree() and bitmap_t.apply()

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 45e64e65ab..848de008a2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3166,7 +3166,28 @@ func Setmaxheap(n int) {
 	maxheap = int64(n)
 }
 
-func Memreserve(_n int) bool {
+// returns true if the caller must evict their previous allocations (if any).
+// will use previous iterations reservation credit, if available.
+func Cacheres(_res int, init bool) bool {
+	// the FS running in user-mode can call memory reservation
+	// functions
+	if hackmode == 0 {
+		return true
+	}
+	res := int64(_res)
+	gp := getg()
+	used := gp.res.cacheallocs
+	gp.res.cacheallocs = 0
+	if init {
+		used = res
+	}
+	if used < res {
+		res = used
+	}
+	return _restake(res, true)
+}
+
+func Memreserve(_n int, rec bool) bool {
 	// the FS running in user-mode can call memory reservation
 	// functions
 	if hackmode == 0 {
@@ -3176,12 +3197,18 @@ func Memreserve(_n int) bool {
 	want := int64(_n)
 	g := getg()
 	if g.res.credit != 0 {
-		pmsg("inconsistent state\n")
+		print("inconsistent state?\n")
+		pcbuf := make([]uintptr, 15)
+		got := callers(1, pcbuf)
+		pcbuf = pcbuf[:got]
+		for _, rip := range pcbuf {
+			print("\t", hex(rip), "\n")
+		}
 	}
-	return _restake(want)
+	return _restake(want, rec)
 }
 
-func Memresadd(_n int) bool {
+func Memresadd(_n int, rec bool) bool {
 	// the FS running in user-mode can call memory reservation
 	// functions
 	if hackmode == 0 {
@@ -3190,28 +3217,40 @@ func Memresadd(_n int) bool {
 
 	want := int64(_n)
 	g := getg()
-	if g.res.credit <= 0 {
-		pmsg("no res?\n")
+	if g.res.got <= 0 {
+		pcbuf := make([]uintptr, 15)
+		got := callers(1, pcbuf)
+		pcbuf = pcbuf[:got]
+		print("no res?\n")
+		for _, rip := range pcbuf {
+			print("\t", hex(rip), "\n")
+		}
 	}
-	return _restake(want)
+	return _restake(want, rec)
 }
 
-func _restake(want int64) bool {
+var Maxgot uintptr
+
+func _restake(want int64, rec bool) bool {
 	g := getg()
 	for {
 		v := atomic.Loadint64(&rescredit)
 		if want > v {
-			//if Printres {
-			//	print("failed to res ", want)
-			//}
 			return false
 		}
 		if atomic.Cas64((*uint64)(unsafe.Pointer(&rescredit)), uint64(v), uint64(v - want)) {
 			g.res.credit += want
 			g.res.got += want
-			//if Printres {
-			//	print("took ", want)
-			//}
+			dur := uintptr(g.res.got)
+			for rec {
+				v := atomic.Loaduintptr(&Maxgot)
+				if dur <= v {
+					break
+				}
+				if atomic.Casuintptr(&Maxgot, v, dur) {
+					break
+				}
+			}
 			return true
 		}
 	}
@@ -3225,20 +3264,10 @@ func Memunres() int {
 	}
 
 	g := getg()
-	if g.res.credit == 0 {
-		pmsg("No credit?\n")
-	}
 	left := g.res.credit
-	//if Printres && left != g.res.got {
-	//	print("used ", g.res.got - left)
-	//}
 	used := g.res.got - left
 	g.res.credit = 0
 	g.res.got = 0
-	if left < 0 {
-		pmsg("Give negative\n")
-		return -1
-	}
 	if left > 0 {
 		atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
 	}

commit 3d69ef37b8b3f7db8162689c88180781612b83c7
Author: Ian Lance Taylor <iant@golang.org>
Date:   Wed Mar 7 22:46:48 2018 -0800

    runtime: use systemstack around throw in sysSigaction
    
    Try to fix the build on ppc64-linux and ppc64le-linux, avoiding:
    
    --- FAIL: TestInlinedRoutineRecords (2.12s)
            dwarf_test.go:97: build: # command-line-arguments
                    runtime.systemstack: nosplit stack overflow
                            752     assumed on entry to runtime.sigtrampgo (nosplit)
                            480     after runtime.sigtrampgo (nosplit) uses 272
                            400     after runtime.sigfwdgo (nosplit) uses 80
                            264     after runtime.setsig (nosplit) uses 136
                            208     after runtime.sigaction (nosplit) uses 56
                            136     after runtime.sysSigaction (nosplit) uses 72
                            88      after runtime.throw (nosplit) uses 48
                            16      after runtime.dopanic (nosplit) uses 72
                            -16     after runtime.systemstack (nosplit) uses 32
    
            dwarf_test.go:98: build error: exit status 2
    --- FAIL: TestAbstractOriginSanity (10.22s)
            dwarf_test.go:97: build: # command-line-arguments
                    runtime.systemstack: nosplit stack overflow
                            752     assumed on entry to runtime.sigtrampgo (nosplit)
                            480     after runtime.sigtrampgo (nosplit) uses 272
                            400     after runtime.sigfwdgo (nosplit) uses 80
                            264     after runtime.setsig (nosplit) uses 136
                            208     after runtime.sigaction (nosplit) uses 56
                            136     after runtime.sysSigaction (nosplit) uses 72
                            88      after runtime.throw (nosplit) uses 48
                            16      after runtime.dopanic (nosplit) uses 72
                            -16     after runtime.systemstack (nosplit) uses 32
    
            dwarf_test.go:98: build error: exit status 2
    FAIL
    FAIL    cmd/link/internal/ld    13.404s
    
    Change-Id: I4840604adb0e9f68a8d8e24f2f2a1a17d1634a58
    Reviewed-on: https://go-review.googlesource.com/99415
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 779f7403ec..265cafdf9b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -414,7 +414,10 @@ func (c *sigctxt) fixsigcode(sig uint32) {
 //go:nosplit
 func sysSigaction(sig uint32, new, old *sigactiont) {
 	if rt_sigaction(uintptr(sig), new, old, unsafe.Sizeof(sigactiont{}.sa_mask)) != 0 {
-		throw("sigaction failed")
+		// Use system stack to avoid split stack overflow on ppc64/ppc64le.
+		systemstack(func() {
+			throw("sigaction failed")
+		})
 	}
 }
 

commit c2f28de732749425ea29b5efa982c407964f8560
Author: Ian Lance Taylor <iant@golang.org>
Date:   Tue Mar 6 20:47:38 2018 -0800

    runtime: change from rt_sigaction to sigaction
    
    This normalizes the Linux code to act like other targets. The size
    argument to the rt_sigaction system call is pushed to a single
    function, sysSigaction.
    
    This is intended as a simplification step for CL 93875 for #14327.
    
    Change-Id: I594788e235f0da20e16e8a028e27ac8c883907c4
    Reviewed-on: https://go-review.googlesource.com/99077
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d8c1592a1d..779f7403ec 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -378,28 +378,26 @@ func setsig(i uint32, fn uintptr) {
 		}
 	}
 	sa.sa_handler = fn
-	rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask))
+	sigaction(i, &sa, nil)
 }
 
 //go:nosplit
 //go:nowritebarrierrec
 func setsigstack(i uint32) {
 	var sa sigactiont
-	rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask))
+	sigaction(i, nil, &sa)
 	if sa.sa_flags&_SA_ONSTACK != 0 {
 		return
 	}
 	sa.sa_flags |= _SA_ONSTACK
-	rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask))
+	sigaction(i, &sa, nil)
 }
 
 //go:nosplit
 //go:nowritebarrierrec
 func getsig(i uint32) uintptr {
 	var sa sigactiont
-	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
-		throw("rt_sigaction read failure")
-	}
+	sigaction(i, nil, &sa)
 	return sa.sa_handler
 }
 
@@ -411,3 +409,15 @@ func setSignalstackSP(s *stackt, sp uintptr) {
 
 func (c *sigctxt) fixsigcode(sig uint32) {
 }
+
+// sysSigaction calls the rt_sigaction system call.
+//go:nosplit
+func sysSigaction(sig uint32, new, old *sigactiont) {
+	if rt_sigaction(uintptr(sig), new, old, unsafe.Sizeof(sigactiont{}.sa_mask)) != 0 {
+		throw("sigaction failed")
+	}
+}
+
+// rt_sigaction is implemented in assembly.
+//go:noescape
+func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32

commit 8e8bc32eb1d35de4f633085097e60b113e7079b5
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Mar 6 20:25:57 2018 -0500

    linux-like "current" to access owning Proc_t from its kernel thread
    
    this is useful to check a process's killed flag for when deciding whether to
    block for more memory or stop a possibly endless sleep/block (to kill).

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c1bf6c54a5..45e64e65ab 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3245,6 +3245,16 @@ func Memunres() int {
 	return int(used)
 }
 
+func Gptr() unsafe.Pointer {
+	gp := getg()
+	return gp.current
+}
+
+func Setgptr(p unsafe.Pointer) {
+	gp := getg()
+	gp.current = p
+}
+
 //go:nosplit
 //go:nowritebarrierrec
 func setsig(i uint32, fn uintptr) {

commit deeb3584bbc64f589e0c53e56dde583005ee1217
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Mar 6 15:20:01 2018 -0500

    kernel thread reservation functions

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 949a9c3c2d..c1bf6c54a5 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3167,6 +3167,12 @@ func Setmaxheap(n int) {
 }
 
 func Memreserve(_n int) bool {
+	// the FS running in user-mode can call memory reservation
+	// functions
+	if hackmode == 0 {
+		return true
+	}
+
 	want := int64(_n)
 	g := getg()
 	if g.res.credit != 0 {
@@ -3176,6 +3182,12 @@ func Memreserve(_n int) bool {
 }
 
 func Memresadd(_n int) bool {
+	// the FS running in user-mode can call memory reservation
+	// functions
+	if hackmode == 0 {
+		return true
+	}
+
 	want := int64(_n)
 	g := getg()
 	if g.res.credit <= 0 {
@@ -3205,7 +3217,13 @@ func _restake(want int64) bool {
 	}
 }
 
-func Memunres() {
+func Memunres() int {
+	// the FS running in user-mode can call memory reservation
+	// functions
+	if hackmode == 0 {
+		return 0
+	}
+
 	g := getg()
 	if g.res.credit == 0 {
 		pmsg("No credit?\n")
@@ -3214,15 +3232,17 @@ func Memunres() {
 	//if Printres && left != g.res.got {
 	//	print("used ", g.res.got - left)
 	//}
+	used := g.res.got - left
 	g.res.credit = 0
 	g.res.got = 0
 	if left < 0 {
 		pmsg("Give negative\n")
-		return
+		return -1
 	}
 	if left > 0 {
 		atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
 	}
+	return int(used)
 }
 
 //go:nosplit

commit 51b027116c2c90e7cb938362b0134ff710fea54e
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Fri Mar 2 11:27:15 2018 +0100

    runtime: use vDSO for clock_gettime on linux/arm
    
    Use the __vdso_clock_gettime fast path via the vDSO on linux/arm to
    speed up nanotime and walltime. This results in the following
    performance improvement for time.Now on a RaspberryPi 3 (running
    32bit Raspbian, i.e. GOOS=linux/GOARCH=arm):
    
    name     old time/op  new time/op  delta
    TimeNow  0.99s  0%  0.39s  1%  -60.74%  (p=0.000 n=12+20)
    
    Change-Id: I3598278a6c88d7f6a6ce66c56b9d25f9dd2f4c9a
    Reviewed-on: https://go-review.googlesource.com/98095
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 7ebca7f856..d8c1592a1d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -268,6 +268,7 @@ func sysauxv(auxv []uintptr) int {
 		}
 
 		archauxv(tag, val)
+		vdsoauxv(tag, val)
 	}
 	return i / 2
 }

commit 60ff68d09b47a2dd9dc436703716957b898a1157
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Mon Feb 12 18:51:38 2018 -0500

    incremental memory reservations

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f739fba0b8..949a9c3c2d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3172,6 +3172,20 @@ func Memreserve(_n int) bool {
 	if g.res.credit != 0 {
 		pmsg("inconsistent state\n")
 	}
+	return _restake(want)
+}
+
+func Memresadd(_n int) bool {
+	want := int64(_n)
+	g := getg()
+	if g.res.credit <= 0 {
+		pmsg("no res?\n")
+	}
+	return _restake(want)
+}
+
+func _restake(want int64) bool {
+	g := getg()
 	for {
 		v := atomic.Loadint64(&rescredit)
 		if want > v {
@@ -3181,8 +3195,8 @@ func Memreserve(_n int) bool {
 			return false
 		}
 		if atomic.Cas64((*uint64)(unsafe.Pointer(&rescredit)), uint64(v), uint64(v - want)) {
-			g.res.credit = want
-			g.res.got = g.res.credit
+			g.res.credit += want
+			g.res.got += want
 			//if Printres {
 			//	print("took ", want)
 			//}
@@ -3206,7 +3220,9 @@ func Memunres() {
 		pmsg("Give negative\n")
 		return
 	}
-	atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
+	if left > 0 {
+		atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
+	}
 }
 
 //go:nosplit

commit 916e7575c70670a7285cfb41bc0ebfd50076721f
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Sun Feb 11 07:29:17 2018 -0500

    memory reservation mechanism
    
    a goroutine reserves n bytes by subtracting n from the global reservation
    counter. if the global reservation counter is less than n, the reservation
    fails. when the goroutine allocates, it subtracts the number of bytes allocated
    from its reservation.
    
    when the system call/interrupt is over, the goroutine returns the remaining
    bytes by adding them to the global reservation counter.
    
    after the GC sweeps all pages, the number of newly swept bytes are added back
    to the global reservation counter.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3c91784424..f739fba0b8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -3150,6 +3150,65 @@ func Setheap(n int) {
 	heapminimum = uint64(n)
 }
 
+// the units of maxheap and rescredit is bytes
+var maxheap int64 = 1 << 30
+var rescredit int64 = maxheap
+
+var lastswept	int64
+
+var Printres bool
+
+var nocreds uint64
+
+func Setmaxheap(n int) {
+	// XXX if less than last maxheap, make sure the difference is no larger
+	// than available reservation
+	maxheap = int64(n)
+}
+
+func Memreserve(_n int) bool {
+	want := int64(_n)
+	g := getg()
+	if g.res.credit != 0 {
+		pmsg("inconsistent state\n")
+	}
+	for {
+		v := atomic.Loadint64(&rescredit)
+		if want > v {
+			//if Printres {
+			//	print("failed to res ", want)
+			//}
+			return false
+		}
+		if atomic.Cas64((*uint64)(unsafe.Pointer(&rescredit)), uint64(v), uint64(v - want)) {
+			g.res.credit = want
+			g.res.got = g.res.credit
+			//if Printres {
+			//	print("took ", want)
+			//}
+			return true
+		}
+	}
+}
+
+func Memunres() {
+	g := getg()
+	if g.res.credit == 0 {
+		pmsg("No credit?\n")
+	}
+	left := g.res.credit
+	//if Printres && left != g.res.got {
+	//	print("used ", g.res.got - left)
+	//}
+	g.res.credit = 0
+	g.res.got = 0
+	if left < 0 {
+		pmsg("Give negative\n")
+		return
+	}
+	atomic.Xadd64((*uint64)(unsafe.Pointer(&rescredit)), left)
+}
+
 //go:nosplit
 //go:nowritebarrierrec
 func setsig(i uint32, fn uintptr) {

commit 1b1c8b34d129eefcdbad234914df999581e62b2f
Author: Tobias Klauser <tklauser@distanz.ch>
Date:   Fri Feb 16 11:26:14 2018 +0100

    runtime: remove unused getrlimit function
    
    Follow CL 93655 which removed the (commented-out) usage of this
    function.
    
    Also remove unused constant _RLIMIT_AS and type rlimit.
    
    Change-Id: Ifb6e6b2104f4c2555269f8ced72bfcae24f5d5e9
    Reviewed-on: https://go-review.googlesource.com/94775
    Run-TryBot: Tobias Klauser <tobias.klauser@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 69850a9ccb..7ebca7f856 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -350,8 +350,6 @@ func sigprocmask(how int32, new, old *sigset) {
 	rtsigprocmask(how, new, old, int32(unsafe.Sizeof(*new)))
 }
 
-//go:noescape
-func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig uint32)
 func raiseproc(sig uint32)
 

commit 51ae88ee2f9a1063c272a497527751d786291c89
Author: Austin Clements <austin@google.com>
Date:   Sat Dec 30 19:35:46 2017 -0500

    runtime: remove non-reserved heap logic
    
    Currently large sysReserve calls on some OSes don't actually reserve
    the memory, but just check that it can be reserved. This was important
    when we called sysReserve to "reserve" many gigabytes for the heap up
    front, but now that we map memory in small increments as we need it,
    this complication is no longer necessary.
    
    This has one curious side benefit: currently, on Linux, allocations
    that are large enough to be rejected by mmap wind up freezing the
    application for a long time before it panics. This happens because
    sysReserve doesn't reserve the memory, so sysMap calls mmap_fixed,
    which calls mmap, which fails because the mapping is too large.
    However, mmap_fixed doesn't inspect *why* mmap fails, so it falls back
    to probing every page in the desired region individually with mincore
    before performing an (otherwise dangerous) MAP_FIXED mapping, which
    will also fail. This takes a long time for a large region. Now this
    logic is gone, so the mmap failure leads to an immediate panic.
    
    Updates #10460.
    
    Change-Id: I8efe88c611871cdb14f99fadd09db83e0161ca2e
    Reviewed-on: https://go-review.googlesource.com/85888
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 7231cf1226..69850a9ccb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -194,6 +194,8 @@ const (
 
 var procAuxv = []byte("/proc/self/auxv\x00")
 
+var addrspace_vec [1]byte
+
 func mincore(addr unsafe.Pointer, n uintptr, dst *byte) int32
 
 func sysargs(argc int32, argv **byte) {

commit 07751f4b582d17289dc19511e2f45d48021f827d
Author: Ian Lance Taylor <iant@golang.org>
Date:   Mon Nov 27 15:40:28 2017 -0800

    runtime: use private futexes on Linux
    
    By default futexes are permitted in shared memory regions, which
    requires the kernel to translate the memory address. Since our futexes
    are never in shared memory, set FUTEX_PRIVATE_FLAG, which makes futex
    operations slightly more efficient.
    
    Change-Id: I2a82365ed27d5cd8d53c5382ebaca1a720a80952
    Reviewed-on: https://go-review.googlesource.com/80144
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: David Crawshaw <crawshaw@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 2e442192cc..7231cf1226 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -24,8 +24,9 @@ func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer,
 // Futexsleep is allowed to wake up spuriously.
 
 const (
-	_FUTEX_WAIT = 0
-	_FUTEX_WAKE = 1
+	_FUTEX_PRIVATE_FLAG = 128
+	_FUTEX_WAIT_PRIVATE = 0 | _FUTEX_PRIVATE_FLAG
+	_FUTEX_WAKE_PRIVATE = 1 | _FUTEX_PRIVATE_FLAG
 )
 
 // Atomically,
@@ -42,7 +43,7 @@ func futexsleep(addr *uint32, val uint32, ns int64) {
 	// here, and so can we: as it says a few lines up,
 	// spurious wakeups are allowed.
 	if ns < 0 {
-		futex(unsafe.Pointer(addr), _FUTEX_WAIT, val, nil, nil, 0)
+		futex(unsafe.Pointer(addr), _FUTEX_WAIT_PRIVATE, val, nil, nil, 0)
 		return
 	}
 
@@ -59,13 +60,13 @@ func futexsleep(addr *uint32, val uint32, ns int64) {
 		ts.tv_nsec = 0
 		ts.set_sec(int64(timediv(ns, 1000000000, (*int32)(unsafe.Pointer(&ts.tv_nsec)))))
 	}
-	futex(unsafe.Pointer(addr), _FUTEX_WAIT, val, unsafe.Pointer(&ts), nil, 0)
+	futex(unsafe.Pointer(addr), _FUTEX_WAIT_PRIVATE, val, unsafe.Pointer(&ts), nil, 0)
 }
 
 // If any procs are sleeping on addr, wake up at most cnt.
 //go:nosplit
 func futexwakeup(addr *uint32, cnt uint32) {
-	ret := futex(unsafe.Pointer(addr), _FUTEX_WAKE, cnt, nil, nil, 0)
+	ret := futex(unsafe.Pointer(addr), _FUTEX_WAKE_PRIVATE, cnt, nil, nil, 0)
 	if ret >= 0 {
 		return
 	}

commit 8693b4f095b2efdbd11967655579606bc3192c59
Author: Austin Clements <austin@google.com>
Date:   Mon Jan 29 12:22:32 2018 -0500

    runtime: remove unused memlimit function
    
    Change-Id: Id057dcc85d64e5c670710fbab6cacd4b906cf594
    Reviewed-on: https://go-review.googlesource.com/93655
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 98e7f52b9e..2e442192cc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -324,38 +324,6 @@ func unminit() {
 	unminitSignals()
 }
 
-func memlimit() uintptr {
-	/*
-		TODO: Convert to Go when something actually uses the result.
-
-		Rlimit rl;
-		extern byte runtimetext[], runtimeend[];
-		uintptr used;
-
-		if(runtimegetrlimit(RLIMIT_AS, &rl) != 0)
-			return 0;
-		if(rl.rlim_cur >= 0x7fffffff)
-			return 0;
-
-		// Estimate our VM footprint excluding the heap.
-		// Not an exact science: use size of binary plus
-		// some room for thread stacks.
-		used = runtimeend - runtimetext + (64<<20);
-		if(used >= rl.rlim_cur)
-			return 0;
-
-		// If there's not at least 16 MB left, we're probably
-		// not going to be able to do much. Treat as no limit.
-		rl.rlim_cur -= used;
-		if(rl.rlim_cur < (16<<20))
-			return 0;
-
-		return rl.rlim_cur - used;
-	*/
-
-	return 0
-}
-
 //#ifdef GOARCH_386
 //#define sa_handler k_sa_handler
 //#endif

commit cbbcd20cfdc70e7b1221f4588cbfc6a908492812
Author: Cody Cutler <ccutler@csail.mit.edu>
Date:   Tue Feb 13 10:27:17 2018 -0500

    fix TLB flush regression
    
    biscuit on 1 CPU may fail to flush the TLB under certain thread schedules. our
    TLB flush test case in usertests caught this bug!
    
    the TLB flushing/pmap freeing code desperately needs a cleanup -- i shall do so
    soonish...

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b120ef6c58..3c91784424 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2030,6 +2030,15 @@ out:
 	return ret
 }
 
+func TLBflush() {
+	// maintaining coherency between cpu.shadowcr3 and cr3 requires that
+	// interrupts are cleared before reading cr3 until after loading that
+	// value into c3.
+	fl := Pushcli()
+	Lcr3(Rcr3())
+	Popcli(fl)
+}
+
 var Tlbshoot struct {
 	Waitfor	int64
 	P_pmap	uintptr

commit eaf603601bf79216772ba36a9b981f21b50ade96
Author: Zhengyu He <hzy@google.com>
Date:   Thu Nov 2 13:39:14 2017 -0700

    runtime: fix GNU/Linux getproccount if sched_getaffinity does not return a multiple of 8
    
    The current code can potentially return a smaller processor count on a
    linux kernel when its cpumask_size (controlled by both kernel config and
    boot parameter) is not a multiple of the pointer size, because
    r/sys.PtrSize will be rounded down. Since sched_getaffinity returns the
    size in bytes, we can just allocate the buf as a byte array to avoid the
    extra calculation with the pointer size and roundups.
    
    Change-Id: I0c21046012b88d8a56b5dd3dde1d158d94f8eea9
    Reviewed-on: https://go-review.googlesource.com/75591
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3157b21371..98e7f52b9e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -89,13 +89,13 @@ func getproccount() int32 {
 	// buffers, but we don't have a dynamic memory allocator at the
 	// moment, so that's a bit tricky and seems like overkill.
 	const maxCPUs = 64 * 1024
-	var buf [maxCPUs / (sys.PtrSize * 8)]uintptr
+	var buf [maxCPUs / 8]byte
 	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
 	if r < 0 {
 		return 1
 	}
 	n := int32(0)
-	for _, v := range buf[:r/sys.PtrSize] {
+	for _, v := range buf[:r] {
 		for v != 0 {
 			n += int32(v & 1)
 			v >>= 1
@@ -385,7 +385,7 @@ func raise(sig uint32)
 func raiseproc(sig uint32)
 
 //go:noescape
-func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+func sched_getaffinity(pid, len uintptr, buf *byte) int32
 func osyield()
 
 //go:nosplit

commit 193088b246f4bbe9a7d3a84ec7f4cc6786dac043
Author: Austin Clements <austin@google.com>
Date:   Mon Oct 16 20:28:29 2017 -0400

    runtime: separate error result for mmap
    
    Currently mmap returns an unsafe.Pointer that encodes OS errors as
    values less than 4096. In practice this is okay, but it borders on
    being really unsafe: for example, the value has to be checked
    immediately after return and if stack copying were ever to observe
    such a value, it would panic. It's also not remotely idiomatic.
    
    Fix this by making mmap return a separate pointer value and error,
    like a normal Go function.
    
    Updates #22218.
    
    Change-Id: Iefd965095ffc82cc91118872753a5d39d785c3a6
    Reviewed-on: https://go-review.googlesource.com/71270
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 83e35f4e27..3157b21371 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -220,8 +220,8 @@ func sysargs(argc int32, argv **byte) {
 		// try using mincore to detect the physical page size.
 		// mincore should return EINVAL when address is not a multiple of system page size.
 		const size = 256 << 10 // size of memory region to allocate
-		p := mmap(nil, size, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
-		if uintptr(p) < 4096 {
+		p, err := mmap(nil, size, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
+		if err != 0 {
 			return
 		}
 		var n uintptr

commit 59413d34c92cf5ce9b0e70e7105ed73a24849b3e
Author: Daniel Mart <mvdan@mvdan.cc>
Date:   Thu Aug 17 15:51:35 2017 +0100

    all: unindent some big chunks of code
    
    Found with mvdan.cc/unindent. Prioritized the ones with the biggest wins
    for now.
    
    Change-Id: I2b032e45cdd559fc9ed5b1ee4c4de42c4c92e07b
    Reviewed-on: https://go-review.googlesource.com/56470
    Run-TryBot: Daniel Mart <mvdan@mvdan.cc>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index dac4de4985..83e35f4e27 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -208,45 +208,46 @@ func sysargs(argc int32, argv **byte) {
 
 	// now argv+n is auxv
 	auxv := (*[1 << 28]uintptr)(add(unsafe.Pointer(argv), uintptr(n)*sys.PtrSize))
-	if sysauxv(auxv[:]) == 0 {
-		// In some situations we don't get a loader-provided
-		// auxv, such as when loaded as a library on Android.
-		// Fall back to /proc/self/auxv.
-		fd := open(&procAuxv[0], 0 /* O_RDONLY */, 0)
-		if fd < 0 {
-			// On Android, /proc/self/auxv might be unreadable (issue 9229), so we fallback to
-			// try using mincore to detect the physical page size.
-			// mincore should return EINVAL when address is not a multiple of system page size.
-			const size = 256 << 10 // size of memory region to allocate
-			p := mmap(nil, size, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
-			if uintptr(p) < 4096 {
-				return
-			}
-			var n uintptr
-			for n = 4 << 10; n < size; n <<= 1 {
-				err := mincore(unsafe.Pointer(uintptr(p)+n), 1, &addrspace_vec[0])
-				if err == 0 {
-					physPageSize = n
-					break
-				}
-			}
-			if physPageSize == 0 {
-				physPageSize = size
-			}
-			munmap(p, size)
+	if sysauxv(auxv[:]) != 0 {
+		return
+	}
+	// In some situations we don't get a loader-provided
+	// auxv, such as when loaded as a library on Android.
+	// Fall back to /proc/self/auxv.
+	fd := open(&procAuxv[0], 0 /* O_RDONLY */, 0)
+	if fd < 0 {
+		// On Android, /proc/self/auxv might be unreadable (issue 9229), so we fallback to
+		// try using mincore to detect the physical page size.
+		// mincore should return EINVAL when address is not a multiple of system page size.
+		const size = 256 << 10 // size of memory region to allocate
+		p := mmap(nil, size, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
+		if uintptr(p) < 4096 {
 			return
 		}
-		var buf [128]uintptr
-		n := read(fd, noescape(unsafe.Pointer(&buf[0])), int32(unsafe.Sizeof(buf)))
-		closefd(fd)
-		if n < 0 {
-			return
+		var n uintptr
+		for n = 4 << 10; n < size; n <<= 1 {
+			err := mincore(unsafe.Pointer(uintptr(p)+n), 1, &addrspace_vec[0])
+			if err == 0 {
+				physPageSize = n
+				break
+			}
+		}
+		if physPageSize == 0 {
+			physPageSize = size
 		}
-		// Make sure buf is terminated, even if we didn't read
-		// the whole file.
-		buf[len(buf)-2] = _AT_NULL
-		sysauxv(buf[:])
+		munmap(p, size)
+		return
+	}
+	var buf [128]uintptr
+	n = read(fd, noescape(unsafe.Pointer(&buf[0])), int32(unsafe.Sizeof(buf)))
+	closefd(fd)
+	if n < 0 {
+		return
 	}
+	// Make sure buf is terminated, even if we didn't read
+	// the whole file.
+	buf[len(buf)-2] = _AT_NULL
+	sysauxv(buf[:])
 }
 
 func sysauxv(auxv []uintptr) int {

commit e9348ab4e9c8c189036ef405d73528ca50a6f785
Author: Hiroshi Ioka <hirochachacha@gmail.com>
Date:   Tue Jul 25 21:44:11 2017 +0900

    runtime: move mincore from stubs.go to os_linux.go
    
    Although mincore is declared in stubs.go, mincore isn't used by any
    OSes except linux. Move it to os_linux.go and clean up unused code.
    
    Change-Id: I6cfb0fed85c0317a4d091a2722ac55fa79fc7c9a
    Reviewed-on: https://go-review.googlesource.com/54910
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 78899737b6..dac4de4985 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -193,6 +193,8 @@ const (
 
 var procAuxv = []byte("/proc/self/auxv\x00")
 
+func mincore(addr unsafe.Pointer, n uintptr, dst *byte) int32
+
 func sysargs(argc int32, argv **byte) {
 	n := argc + 1
 

commit 2673f9ed23348c634f6331ee589d489e4d9c7a9b
Author: Austin Clements <austin@google.com>
Date:   Wed Jul 12 10:12:50 2017 -0600

    runtime: pass CLONE_SYSVSEM to clone
    
    SysV semaphore undo lists should be shared by threads, just like
    several other resources listed in cloneFlags. Currently we don't do
    this, but it probably doesn't affect anything because 1) probably
    nobody uses SysV semaphores from Go and 2) Go-created threads never
    exit until the process does. Beyond being the right thing to do,
    user-level QEMU requires this flag because it depends on glibc to
    create new threads and glibc uses this flag.
    
    Fixes #20763.
    
    Change-Id: I1d1dafec53ed87e0f4d4d432b945e8e68bb72dcd
    Reviewed-on: https://go-review.googlesource.com/48170
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a6efc0e3d1..78899737b6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -132,6 +132,7 @@ const (
 		_CLONE_FS | /* share cwd, etc */
 		_CLONE_FILES | /* share fd table */
 		_CLONE_SIGHAND | /* share sig handler table */
+		_CLONE_SYSVSEM | /* share SysV semaphore undo lists (see issue #20763) */
 		_CLONE_THREAD /* revisit - okay for now */
 )
 

commit 4902bc93035162b878a40d9ed204e197db231659
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Apr 2 17:56:57 2017 -0400

    use 2MB pages to map kernel text
    
    greatly reduces iTLB misses, giving us the remaining 10% mailbench throughput!
    hooray!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 58bd8505c1..b120ef6c58 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1416,6 +1416,8 @@ const (
 	PTE_P		uintptr = 1 << 0
 	PTE_W		uintptr = 1 << 1
 	PTE_U		uintptr = 1 << 2
+	PTE_PS		uintptr = 1 << 7
+	PTE_G		uintptr = 1 << 8
 	PTE_PCD		uintptr = 1 << 4
 	PGSIZE		uintptr = 1 << 12
 	PGOFFMASK	uintptr = PGSIZE - 1
@@ -1498,6 +1500,9 @@ func pgdir_walk1(slot, van uintptr, create bool) *uintptr {
 		zero_phys(p_pg)
 		*sp = p_pg | PTE_P | PTE_W
 	}
+	if *sp & PTE_PS != 0 {
+		pancake("map in PS", *sp)
+	}
 	return pgdir_walk1(ns, slotnext(van), create)
 }
 
@@ -1945,10 +1950,15 @@ func proc_setup() {
 	Gscpu().mythread = &threads[0]
 }
 
+// XXX to prevent CPUs from calling zero_phys concurrently when allocating pmap
+// pages...
+var joinlock = &Spinlock_t{}
+
 //go:nosplit
 func Ap_setup(cpunum uint) {
 	// interrupts are probably already cleared
 	fl := Pushcli()
+	Splock(joinlock)
 
 	Splock(pmsglock)
 	_pmsg("cpu")
@@ -1973,6 +1983,7 @@ func Ap_setup(cpunum uint) {
 	Gscpu().num = cpunum
 	Gscpu().mythread = nil
 
+	Spunlock(joinlock)
 	Popcli(fl)
 }
 
@@ -2690,7 +2701,7 @@ func sigret(t *thread_t) {
 // construct siginfo_t and more of context.
 
 func find_empty(sz uintptr) uintptr {
-	v := caddr(0, 0, 0, 1, 0)
+	v := caddr(0, 0, 256, 0, 0)
 	cantuse := uintptr(0xf0)
 	for {
 		pte := pgdir_walk(v, false)

commit 55500ccb98ff52e9ed78871fb6e869afe7ad4a70
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 22 13:44:13 2017 -0400

    simplify TLB shootdown code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d378210115..58bd8505c1 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2019,36 +2019,16 @@ out:
 	return ret
 }
 
-var tlbshoot_wait uintptr
-var tlbshoot_pg uintptr
-var tlbshoot_count uintptr
-var tlbshoot_pmap uintptr
-var tlbshoot_gen uint64
-
-func Tlbadmit(p_pmap, cpuwait, pg, pgcount uintptr) uint64 {
-	for !atomic.Casuintptr(&tlbshoot_wait, 0, cpuwait) {
-		preemptok()
-	}
-	atomic.Xchguintptr(&tlbshoot_pg, pg)
-	atomic.Xchguintptr(&tlbshoot_count, pgcount)
-	atomic.Xchguintptr(&tlbshoot_pmap, p_pmap)
-	atomic.Xadd64(&tlbshoot_gen, 1)
-	return tlbshoot_gen
-}
-
-func Tlbwait(gen uint64) {
-	for atomic.Loaduintptr(&tlbshoot_wait) != 0 {
-		if atomic.Load64(&tlbshoot_gen) != gen {
-			break
-		}
-	}
+var Tlbshoot struct {
+	Waitfor	int64
+	P_pmap	uintptr
 }
 
 // must be nosplit since called at interrupt time
 //go:nosplit
 func tlb_shootdown() {
 	ct := Gscpu().mythread
-	if ct != nil && Rcr3() == tlbshoot_pmap {
+	if ct != nil && Rcr3() == Tlbshoot.P_pmap {
 		// lazy way for now
 		Lcr3(Rcr3())
 		//start := tlbshoot_pg
@@ -2057,42 +2037,13 @@ func tlb_shootdown() {
 		//	invlpg(start)
 		//}
 	}
-	dur := (*uint64)(unsafe.Pointer(&tlbshoot_wait))
-	v := atomic.Xadd64(dur, -1)
+	v := atomic.Xaddint64(&Tlbshoot.Waitfor, -1)
 	if v < 0 {
 		pancake("shootwait < 0", uintptr(v))
 	}
 	sched_resume(ct)
 }
 
-// this function checks to see if another thread is trying to preempt this
-// thread (perhaps to start a GC). this is called when go code is spinning on a
-// spinlock in order to avoid a deadlock where the thread that acquired the
-// spinlock starts a GC and waits forever for the spinning thread. (go code
-// should probably not use spinlocks. tlb shootdown code is the only code
-// protected by a spinlock since the lock must both be acquired in go code and
-// in interrupt context.)
-//
-// alternatively, we could make sure that no allocations are made while the
-// spinlock is acquired.
-func preemptok() {
-	gp := getg()
-	StackPreempt := uintptr(0xfffffffffffffade)
-	if gp.stackguard0 == StackPreempt {
-		pmsg("!")
-		// call function with stack splitting prologue
-		_dummy()
-	}
-}
-
-var _notdeadcode uint32
-func _dummy() {
-	if _notdeadcode != 0 {
-		_dummy()
-	}
-	_notdeadcode = 0
-}
-
 // cpu exception/interrupt vectors
 const (
 	TRAP_NMI	= 2

commit 34e00fe2d2b46cdb6b01652da71ab96d5b4eea01
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 20 09:29:51 2017 -0400

    ixgbe per-CPU TX queues
    
    in the old code, the lock on the single queue was the single most contended
    lock during the nginx experiment. per-CPU TX queues increase nginx throughput
    by 13%.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c168bb41ba..d378210115 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -516,6 +516,15 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
+// returns the logical CPU identifier on which the calling thread was executing
+// at some point.
+func CPUHint() int {
+	fl := Pushcli()
+	ret := int(_Gscpu().num)
+	Popcli(fl)
+	return ret
+}
+
 //go:nowritebarrierrec
 func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {

commit 3595a774046f172f07537a9310605b8c45bbbfc6
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 27 11:04:31 2017 -0500

    restore user SSE regs after servicing interrupts
    
    biscuit has always carefully saved user SSE regs, but until now didn't restore
    them after taking an interrupt. this was never detected until now since my user
    programs rarely use SSE instructions.
    
    i discovered the bug because lmbench calculates benchmark results with SSE
    double precision division and, for one particular benchmark, the compiler put
    fprintf on a new page. thus the benchmark tried to print the benchmark results
    via fprintf which caused a fault, lazily loaded fprintf's instructions, and
    finally clobbered the SSE regs holding the benchmark results. the benchmark
    then proceeded to print that biscuit has 128TB/s of read-file bandwidth.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c91a7241e7..c168bb41ba 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -560,6 +560,9 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
 		*(*uintptr)(unsafe.Pointer(&cpu.fxbuf)) = uintptr(unsafe.Pointer(fxbuf))
 	}
 
+	if !fastret {
+		fxrstor(fxbuf)
+	}
 	intno, aux := _Userrun(tf, fastret)
 
 	Sti()
@@ -1655,7 +1658,7 @@ func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	}
 }
 
-var fxinit [FXREGS]uintptr
+var Fxinit [FXREGS]uintptr
 
 // nosplit because APs call this function before FS is setup
 //go:nosplit
@@ -1674,8 +1677,8 @@ func fpuinit(amfirst bool) {
 	Lcr4(cr4);
 
 	if amfirst {
-		chkalign(unsafe.Pointer(&fxinit[0]), 16)
-		fxsave(&fxinit)
+		chkalign(unsafe.Pointer(&Fxinit[0]), 16)
+		fxsave(&Fxinit)
 
 		chksize(FXREGS*8, unsafe.Sizeof(threads[0].fx))
 		for i := range threads {
@@ -2206,10 +2209,10 @@ func trap(tf *[TFSIZE]uintptr) {
 		}
 	}
 
-	// don't add code before FPU/SSE/MMX context saving unless you've
-	// thought very carefully! it is easy to accidentally and silently
-	// corrupt SSE state (ie calling memmove indirectly by assignment of
-	// large datatypes) before it is saved below.
+	// don't add code before SSE context saving unless you've thought very
+	// carefully! it is easy to accidentally and silently corrupt SSE state
+	// (ie calling memmove indirectly by assignment of large datatypes)
+	// before it is saved below.
 
 	// save SSE state immediately before we clobber it
 	if ct != nil {
@@ -2921,7 +2924,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	mt.status = ST_RUNNABLE
 	mt.p_pmap = P_kpmap
 
-	mt.fx = fxinit
+	mt.fx = Fxinit
 
 	Spunlock(threadlock)
 	Popcli(fl)

commit 2c213bd4cdfdf4da90cc880b9594e2e55820368d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 23 17:10:07 2017 -0500

    remove threadinfo lock from syscall path

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 016a7ed8fc..c91a7241e7 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -516,6 +516,7 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
+//go:nowritebarrierrec
 func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
@@ -552,8 +553,11 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
 		// exception/interrupt, not during syscall exit/return. this is
 		// OK since sys5ABI defines the SSE registers to be
 		// caller-saved.
-		cpu.tf = tf
-		cpu.fxbuf = fxbuf
+
+		// disable these write barriers to prevent a CPU being
+		// preempted with interrupts cleared.
+		*(*uintptr)(unsafe.Pointer(&cpu.tf)) = uintptr(unsafe.Pointer(tf))
+		*(*uintptr)(unsafe.Pointer(&cpu.fxbuf)) = uintptr(unsafe.Pointer(fxbuf))
 	}
 
 	intno, aux := _Userrun(tf, fastret)

commit c7551fcab5f34b451342236b1d72fc28b135bf3e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 13 17:55:10 2017 -0500

    pad cpu_t to be a multiple of cacheline size
    
    otherwise cache thrashing due to false sharing of tf/fxbuf reads with sysrsp
    writes tanks performance by up 18%.
    
    calling getpid in a loop with 4 CPUs is now perfectly scalable!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1a6dd12a92..016a7ed8fc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -446,9 +446,9 @@ type cpu_t struct {
 	sysrsp		uintptr
 	shadowcr3	uintptr
 	shadowfs	uintptr
-	//pid		uintptr
 	tf	*[TFSIZE]uintptr
 	fxbuf	*[FXREGS]uintptr
+	_clpad		[56]uint8
 }
 
 var Cpumhz uint
@@ -2102,7 +2102,6 @@ const (
 	ST_INVALID	= 0
 	ST_RUNNABLE	= 1
 	ST_RUNNING	= 2
-	ST_WAITING	= 3
 	ST_SLEEPING	= 4
 	ST_WILLSLEEP	= 5
 )
@@ -2256,10 +2255,11 @@ func trap(tf *[TFSIZE]uintptr) {
 				ct.status = ST_RUNNABLE
 			}
 		}
+		wakeup()
 		if !yielding {
 			lap_eoi()
 			if cpu.num == 0 {
-				wakeup()
+				//wakeup()
 				proftick()
 			}
 		}
@@ -2394,12 +2394,15 @@ func _yieldy() {
 	_tchk()
 	cpu := Gscpu()
 	ct := cpu.mythread
-	_ti := (uintptr(unsafe.Pointer(ct)) -
-	    uintptr(unsafe.Pointer(&threads[0])))/unsafe.Sizeof(thread_t{})
-	ti := int(_ti)
-	start := (ti + 1) % maxthreads
-	if ct == nil {
-		start = 0
+	var start int
+	if ct != nil {
+		_ti := (uintptr(unsafe.Pointer(ct)) -
+		    uintptr(unsafe.Pointer(&threads[0])))/
+		    unsafe.Sizeof(thread_t{})
+		ti := int(_ti)
+		start = (ti + 1) % maxthreads
+	} else {
+		start = int(dumrand(0, uint(len(threads))))
 	}
 	for i := 0; i < maxthreads; i++ {
 		idx := (start + i) % maxthreads
@@ -3015,7 +3018,7 @@ func hack_syscall(trap, a1, a2, a3 int64) (int64, int64, int64) {
 var futexlock = &Spinlock_t{}
 
 // stack splitting prologue is not ok here, even though hack_futex is not
-// called during interrupt context, because the runtime thinks hack_futex makes
+// called during interrupt context, because notetsleepg thinks hack_futex makes
 // a system call and thus calls "entersyscallblock" which sets a flag that
 // causes a panic if the stack needs to be split or if the g is preempted
 // (though stackcheck() call below makes sure the stack is not overflowed).

commit 6844eb36923c95ddab3ba731b1018fba6114e492
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 8 13:25:43 2017 -0500

    cleanup/speedup system calls

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 88a338602d..1a6dd12a92 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -447,6 +447,8 @@ type cpu_t struct {
 	shadowcr3	uintptr
 	shadowfs	uintptr
 	//pid		uintptr
+	tf	*[TFSIZE]uintptr
+	fxbuf	*[FXREGS]uintptr
 }
 
 var Cpumhz uint
@@ -456,11 +458,6 @@ const MAXCPUS int = 32
 
 var cpus [MAXCPUS]cpu_t
 
-type tuser_t struct {
-	tf	*[TFSIZE]uintptr
-	fxbuf	*[FXREGS]uintptr
-}
-
 type prof_t struct {
 	enabled		int
 	totaltime	int
@@ -473,7 +470,6 @@ type thread_t struct {
 	tf		[TFSIZE]uintptr
 	//_pad		int
 	fx		[FXREGS]uintptr
-	user		tuser_t
 	sigtf		[TFSIZE]uintptr
 	sigfx		[FXREGS]uintptr
 	status		int
@@ -520,16 +516,15 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
-func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr, pmap *[512]int,
+func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
 	// while other cpus execute user programs.
 	//entersyscall(0)
-	fl := Pushcli()
+	Cli()
 	cpu := _Gscpu()
-	ct := cpu.mythread
 
 	var opmap uintptr
 	var dopdec bool
@@ -550,19 +545,20 @@ func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr, pmap *[512]int,
 		Wrmsr(ia32_fs_base, int(tf[TF_FSBASE]))
 		cpu.shadowfs = tf[TF_FSBASE]
 	}
-
-	// we only save/restore SSE registers on cpu exception/interrupt, not
-	// during syscall exit/return. this is OK since sys5ABI defines the SSE
-	// registers to be caller-saved.
-	// XXX types
-	ct.user.tf = tf
-	ct.user.fxbuf = fxbuf
+	// save the user buffers in case an interrupt arrives while in user
+	// mode
+	if cpu.tf != tf {
+		// we only save/restore SSE registers on cpu
+		// exception/interrupt, not during syscall exit/return. this is
+		// OK since sys5ABI defines the SSE registers to be
+		// caller-saved.
+		cpu.tf = tf
+		cpu.fxbuf = fxbuf
+	}
 
 	intno, aux := _Userrun(tf, fastret)
 
-	ct.user.tf = nil
-	ct.user.fxbuf = nil
-	Popcli(fl)
+	Sti()
 	//exitsyscall(0)
 	return intno, aux, opmap, dopdec
 }
@@ -992,7 +988,7 @@ var _cpuattrs [MAXCPUS]uint16
 func Cpuprint(n uint16, row uintptr) {
 	p := uintptr(0xb8000)
 	num := Gscpu().num
-	p += uintptr(num) * row*80*2
+	p += (uintptr(num) + row*80)*2
 	attr := _cpuattrs[num]
 	_cpuattrs[num] += 0x100
 	*(*uint16)(unsafe.Pointer(p)) = attr | n
@@ -2215,11 +2211,12 @@ func trap(tf *[TFSIZE]uintptr) {
 	// save SSE state immediately before we clobber it
 	if ct != nil {
 		// if in user mode, save to user buffers and make it look like
-		// Userrun returned.
-		if ct.user.tf != nil {
-			ufx := ct.user.fxbuf
+		// Userrun returned. did the interrupt occur while in user
+		// mode?
+		if tf[TF_CS] & 3 != 0 {
+			ufx := cpu.fxbuf
 			fxsave(ufx)
-			utf := ct.user.tf
+			utf := cpu.tf
 			*utf = *tf
 			ct.tf[TF_RIP] = _userintaddr
 			ct.tf[TF_RSP] = cpu.sysrsp
@@ -2229,20 +2226,10 @@ func trap(tf *[TFSIZE]uintptr) {
 			if trapno == TRAP_YIELD || trapno == TRAP_SIGRET {
 				pancake("nyet", trapno)
 			}
-			// XXX fix this using RIP method
-			// if we are unlucky enough for a timer int to come in
-			// before we execute the first instruction of the new
-			// rip, make sure the state we just saved isn't
-			// clobbered
-			*(*uintptr)(unsafe.Pointer(&ct.user.tf)) =
-			    uintptr(unsafe.Pointer(nil))
-			*(*uintptr)(unsafe.Pointer(&ct.user.fxbuf)) =
-			    uintptr(unsafe.Pointer(nil))
 		} else {
 			fxsave(&ct.fx)
 			ct.tf = *tf
 		}
-		//timetick(ct)
 	}
 
 	yielding := false

commit 57e98f91c075807f05307284a86db4aa20413f3e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 7 09:35:27 2017 -0500

    allow write barriers, cleanup more ancient silly types
    
    these write-barriers are now OK since runtime uses %gs instead of %fs

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 80fa44f66d..88a338602d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -426,7 +426,7 @@ func _trapret(*[TFSIZE]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
 func _userret()
-func _Userrun(*[TFSIZE]int, bool) (int, int)
+func _Userrun(*[TFSIZE]uintptr, bool) (int, int)
 func Wrmsr(int, int)
 
 // we have to carefully write go code that may be executed early (during boot)
@@ -520,7 +520,7 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
-func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
+func Userrun(tf *[TFSIZE]uintptr, fxbuf *[FXREGS]uintptr, pmap *[512]int,
     p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
@@ -545,23 +545,18 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
-	if cpu.shadowfs != uintptr(tf[TF_FSBASE]) {
+	if cpu.shadowfs != tf[TF_FSBASE] {
 		ia32_fs_base := 0xc0000100
-		Wrmsr(ia32_fs_base, tf[TF_FSBASE])
-		cpu.shadowfs = uintptr(tf[TF_FSBASE])
+		Wrmsr(ia32_fs_base, int(tf[TF_FSBASE]))
+		cpu.shadowfs = tf[TF_FSBASE]
 	}
 
 	// we only save/restore SSE registers on cpu exception/interrupt, not
 	// during syscall exit/return. this is OK since sys5ABI defines the SSE
 	// registers to be caller-saved.
 	// XXX types
-	//ct.user.tf = (*[TFSIZE]uintptr)(unsafe.Pointer(tf))
-	//ct.user.fxbuf = (*[FXREGS]uintptr)(unsafe.Pointer(fxbuf))
-
-	// avoid write barriers, see note above
-	// these write barriers are OK now that kernel uses %gs instead of %fs?
-	*(*uintptr)(unsafe.Pointer(&ct.user.tf)) = uintptr(unsafe.Pointer(tf))
-	*(*uintptr)(unsafe.Pointer(&ct.user.fxbuf)) = uintptr(unsafe.Pointer(fxbuf))
+	ct.user.tf = tf
+	ct.user.fxbuf = fxbuf
 
 	intno, aux := _Userrun(tf, fastret)
 

commit d6c5713ebb9f22d3e36b45fb112cc60fa2e9b973
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 2 17:01:31 2017 -0500

    poll sleeping Ms, too
    
    although i just modified the scheduler to poll for runnable threads, the old
    code wouldn't also check for sleeping threads that should be woken. thus these
    threads would not be awoken until the next timer interrupt (which is, on
    average, 500us away). a runtime thread sometimes sleeps for 100us when
    synchronizing with other runtime threads.
    
    this fixes our nginx and mailbench multicore performance. nginx with 1, 2, and
    4 cores have throughput within 5% of linux!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f7714424cb..80fa44f66d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1609,6 +1609,27 @@ func Get_phys() uintptr {
 	return get_pg()
 }
 
+func Tcount() (int, int) {
+	fl := Pushcli()
+	Splock(threadlock)
+
+	r := 0
+	nr := 0
+	for i := range threads {
+		switch (threads[i].status) {
+		case ST_RUNNING, ST_RUNNABLE:
+			r++
+		case ST_INVALID:
+		default:
+			nr++;
+		}
+	}
+
+	Spunlock(threadlock)
+	Popcli(fl)
+	return r, nr
+}
+
 //go:nosplit
 func get_pg() uintptr {
 	if pglast == 0 {
@@ -2311,12 +2332,20 @@ func sched_halt() {
 	// busy loop waiting for runnable thread without the threadlock
 	for {
 		Sti()
+		now := hack_nanotime()
 		sidx := int(dumrand(0, uint(len(threads))))
 		for n := 0; n < len(threads); n++ {
 			i := (sidx + n) % len(threads)
-			if threads[i].status == ST_RUNNABLE {
+			t := &threads[i]
+			if t.status == ST_RUNNABLE {
+				Cli()
+				Splock(threadlock)
+				_yieldy()
+				Sti()
+			} else if _waketimeout(now, t) {
 				Cli()
 				Splock(threadlock)
+				wakeup()
 				_yieldy()
 				Sti()
 			}
@@ -2357,8 +2386,7 @@ func wakeup() {
 	timedout := -110
 	for i := range threads {
 		t := &threads[i]
-		sf := t.sleepfor
-		if t.status == ST_SLEEPING && sf != -1 && sf < now {
+		if _waketimeout(now, t) {
 			t.status = ST_RUNNABLE
 			t.sleepfor = 0
 			t.futaddr = 0
@@ -2367,6 +2395,12 @@ func wakeup() {
 	}
 }
 
+//go:nosplit
+func _waketimeout(now int, t *thread_t) bool {
+	sf := t.sleepfor
+	return t.status == ST_SLEEPING && sf != -1 && sf < now
+}
+
 //go:nosplit
 func yieldy() {
 	_yieldy()
@@ -2576,6 +2610,8 @@ func proftick() {
 	}
 	_lastprof = n
 
+	// XXX this is broken; only deliver sigprof to thread that was
+	// executing when the timer expired.
 	for i := range threads {
 		// only do fake SIGPROF if we are already
 		t := &threads[i]
@@ -2996,7 +3032,11 @@ func hack_syscall(trap, a1, a2, a3 int64) (int64, int64, int64) {
 
 var futexlock = &Spinlock_t{}
 
-// XXX not sure why stack splitting prologue is not ok here
+// stack splitting prologue is not ok here, even though hack_futex is not
+// called during interrupt context, because the runtime thinks hack_futex makes
+// a system call and thus calls "entersyscallblock" which sets a flag that
+// causes a panic if the stack needs to be split or if the g is preempted
+// (though stackcheck() call below makes sure the stack is not overflowed).
 //go:nosplit
 func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
     val2 int32) int64 {

commit 4b1377d122d8f49e36dd4927bb01b88671bb10c8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 1 19:23:12 2017 -0500

    busy wait for runnable Ms
    
    the old code would occasionally execute the hlt instruction during brief
    periods when there are no runnable threads (like during runtime lock
    contention). the result is that the runtime thread is not awoken until the next
    timer interrupt which doesn't occur until 500 us later, on average.
    
    instead, busy wait for runnable threads with interrupts enabled.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index be97631712..f7714424cb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2305,7 +2305,23 @@ func _tchk() {
 
 //go:nosplit
 func sched_halt() {
-	cpu_halt(Gscpu().rsp)
+	if rflags() & TF_FL_IF != 0 {
+		pancake("must not be interruptible", 0)
+	}
+	// busy loop waiting for runnable thread without the threadlock
+	for {
+		Sti()
+		sidx := int(dumrand(0, uint(len(threads))))
+		for n := 0; n < len(threads); n++ {
+			i := (sidx + n) % len(threads)
+			if threads[i].status == ST_RUNNABLE {
+				Cli()
+				Splock(threadlock)
+				_yieldy()
+				Sti()
+			}
+		}
+	}
 }
 
 //go:nosplit
@@ -2353,6 +2369,12 @@ func wakeup() {
 
 //go:nosplit
 func yieldy() {
+	_yieldy()
+	sched_halt()
+}
+
+//go:nosplit
+func _yieldy() {
 	_tchk()
 	cpu := Gscpu()
 	ct := cpu.mythread
@@ -2375,7 +2397,6 @@ func yieldy() {
 	*(*uintptr)(unsafe.Pointer(&cpu.mythread)) =
 	    uintptr(unsafe.Pointer(nil))
 	Spunlock(threadlock)
-	sched_halt()
 }
 
 var _irqv struct {

commit ba6205f0411566e31e695ab202f7d83c0799881a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 31 16:59:18 2017 -0500

    use built-in funcPC to get function address

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ee12d2d30e..be97631712 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1318,9 +1318,7 @@ var _idt [idtsz]idte_t
 
 //go:nosplit
 func int_set(idx int, intentry func(), istn int) {
-	var f func()
-	f = intentry
-	entry := **(**uint)(unsafe.Pointer(&f))
+	entry := funcPC(intentry)
 
 	p := &_idt[idx]
 	p.baselow = uint16(entry)
@@ -1890,12 +1888,8 @@ func lapic_setup(calibrate bool) {
 }
 
 func proc_setup() {
-	var dur func()
-	dur = _userint
-	_userintaddr = **(**uintptr)(unsafe.Pointer(&dur))
-	var dur2 func(int32, unsafe.Pointer, *ucontext_t)
-	dur2 = sigsim
-	_sigsimaddr = **(**uintptr)(unsafe.Pointer(&dur2))
+	_userintaddr = funcPC(_userint)
+	_sigsimaddr = funcPC(sigsim)
 
 	chksize(TFSIZE*8, unsafe.Sizeof(threads[0].tf))
 	// initialize the first thread: us
@@ -1963,9 +1957,7 @@ func sysc_setup(myrsp uintptr) {
 
 	sysenter_eip := 0x176
 	// asm_amd64.s
-	var dur func()
-	dur = _sysentry
-	sysentryaddr := **(**uintptr)(unsafe.Pointer(&dur))
+	sysentryaddr := funcPC(_sysentry)
 	Wrmsr(sysenter_eip, int(sysentryaddr))
 
 	sysenter_esp := 0x175
@@ -2853,9 +2845,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	if flags != chk {
 		pancake("unexpected clone args", uintptr(flags))
 	}
-	var dur func(uintptr)
-	dur = clone_wrap
-	cloneaddr := **(**uintptr)(unsafe.Pointer(&dur))
+	cloneaddr := funcPC(clone_wrap)
 
 	fl := Pushcli()
 	Splock(threadlock)

commit 51a1efba44e4d91a0b89e931c2a24e34c6dca592
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 24 10:45:02 2017 -0500

    correctly report available physical memory

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 5646c35979..ee12d2d30e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1568,8 +1568,7 @@ func phys_init() {
 	found := false
 	base := sec.e820p
 	maxfound := uintptr(0)
-	// bootloader provides 15 e820 entries at most (it panicks if the PC
-	// provides more).
+	// bootloader provides e820 entries
 	e820sz := uintptr(28)
 	for i := uintptr(0); i < 4096/e820sz; i++ {
 		ep := (*e820_t)(unsafe.Pointer(base + i*28))
@@ -1605,7 +1604,7 @@ func phys_init() {
 var _eseg e820_t
 
 func Totalphysmem() int {
-	return int(_eseg.start + _eseg.len)
+	return int(pglast - pgfirst)
 }
 
 func Get_phys() uintptr {

commit 79a8617b45871bcc73577f511d3340ac306e7e45
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Jan 16 13:52:51 2017 -0500

    reduce "OS" quantum from 10ms to 1ms
    
    now runtime can more accurately preempt mutators from doing too much GC work.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1da014ec4e..5646c35979 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2106,7 +2106,7 @@ const (
 
 // scheduler constants
 const (
-	HZ	= 100
+	HZ	= 1000
 )
 
 var _userintaddr uintptr

commit 6a1cac27005f5e37c9c4acd0f63121e61b41ae3c
Author: Michael Marineau <mike@marineau.org>
Date:   Tue Jan 3 00:15:05 2017 -0800

    runtime: check sched_getaffinity return value
    
    Android on ChromeOS uses a restrictive seccomp filter that blocks
    sched_getaffinity, leading this code to index a slice by -errno.
    
    Change-Id: Iec09a4f79dfbc17884e24f39bcfdad305de75b37
    Reviewed-on: https://go-review.googlesource.com/34794
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 213b951a6b..a6efc0e3d1 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -91,6 +91,9 @@ func getproccount() int32 {
 	const maxCPUs = 64 * 1024
 	var buf [maxCPUs / (sys.PtrSize * 8)]uintptr
 	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
+	if r < 0 {
+		return 1
+	}
 	n := int32(0)
 	for _, v := range buf[:r/sys.PtrSize] {
 		for v != 0 {

commit a0667be8ef56c405d093cc0f5817b9932b4ca76c
Author: Shenghou Ma <minux@golang.org>
Date:   Mon Dec 19 09:48:07 2016 -0500

    runtime: use mincore to detect physical page size as last resort on Android
    
    Fixes #18041.
    
    Change-Id: Iad1439b2dd56b113c8829699eda467d1367b0e15
    Reviewed-on: https://go-review.googlesource.com/34610
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 320c1281c2..213b951a6b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -208,6 +208,26 @@ func sysargs(argc int32, argv **byte) {
 		// Fall back to /proc/self/auxv.
 		fd := open(&procAuxv[0], 0 /* O_RDONLY */, 0)
 		if fd < 0 {
+			// On Android, /proc/self/auxv might be unreadable (issue 9229), so we fallback to
+			// try using mincore to detect the physical page size.
+			// mincore should return EINVAL when address is not a multiple of system page size.
+			const size = 256 << 10 // size of memory region to allocate
+			p := mmap(nil, size, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, -1, 0)
+			if uintptr(p) < 4096 {
+				return
+			}
+			var n uintptr
+			for n = 4 << 10; n < size; n <<= 1 {
+				err := mincore(unsafe.Pointer(uintptr(p)+n), 1, &addrspace_vec[0])
+				if err == 0 {
+					physPageSize = n
+					break
+				}
+			}
+			if physPageSize == 0 {
+				physPageSize = size
+			}
+			munmap(p, size)
 			return
 		}
 		var buf [128]uintptr

commit a4117b2e2a0bc3e092e1c3258ec7d9eb4d11cf68
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 16:25:01 2016 -0500

    disable a couple unnecessary write barriers
    
    use the go compiler's nice new feature which asserts that a function neither
    contains nor ever calls a function containing write barriers.
    
    these pointer writes may be executed at any time (interrupt context) and the
    pointers they are modify are either to objects not allocated by the GC or must
    always be reachable (either through runtime.allgs or on the caller's stack). we
    need to disable them, otherwise they may be executed when certain invariants
    don't hold (namely when m.p == nil).

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 5af59f4b01..1da014ec4e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -559,6 +559,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	//ct.user.fxbuf = (*[FXREGS]uintptr)(unsafe.Pointer(fxbuf))
 
 	// avoid write barriers, see note above
+	// these write barriers are OK now that kernel uses %gs instead of %fs?
 	*(*uintptr)(unsafe.Pointer(&ct.user.tf)) = uintptr(unsafe.Pointer(tf))
 	*(*uintptr)(unsafe.Pointer(&ct.user.fxbuf)) = uintptr(unsafe.Pointer(fxbuf))
 
@@ -2166,6 +2167,7 @@ func kernel_fault(tf *[TFSIZE]uintptr) {
 // may want to only wakeup() on most timer ints since now there is more
 // overhead for timer ints during user time.
 //go:nosplit
+//go:nowritebarrierrec
 func trap(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 
@@ -2225,8 +2227,10 @@ func trap(tf *[TFSIZE]uintptr) {
 			// before we execute the first instruction of the new
 			// rip, make sure the state we just saved isn't
 			// clobbered
-			ct.user.tf = nil
-			ct.user.fxbuf = nil
+			*(*uintptr)(unsafe.Pointer(&ct.user.tf)) =
+			    uintptr(unsafe.Pointer(nil))
+			*(*uintptr)(unsafe.Pointer(&ct.user.fxbuf)) =
+			    uintptr(unsafe.Pointer(nil))
 		} else {
 			fxsave(&ct.fx)
 			ct.tf = *tf
@@ -2377,7 +2381,8 @@ func yieldy() {
 			sched_run(t)
 		}
 	}
-	cpu.mythread = nil
+	*(*uintptr)(unsafe.Pointer(&cpu.mythread)) =
+	    uintptr(unsafe.Pointer(nil))
 	Spunlock(threadlock)
 	sched_halt()
 }
@@ -2509,7 +2514,8 @@ func IRQcheck(pp *p) {
 			if gst &^ _Gscan != _Gwaiting {
 				pancake("bad igp status", uintptr(gst))
 			}
-			_irqv.handlers[i].igp = nil
+			*(*uintptr)(unsafe.Pointer(&_irqv.handlers[i].igp)) =
+			    uintptr(unsafe.Pointer(nil))
 			_irqv.handlers[i].started = true
 			// we cannot set gstatus or put to run queue before we
 			// release the spinlock since either operation may

commit fa2c4ef4a2941edebd0fc05fff6c1b2071029878
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 15:57:05 2016 -0500

    updates for runtime upgrade

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index acbbdab2da..5af59f4b01 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -246,6 +246,7 @@ func sysauxv(auxv []uintptr) int {
 func osinit() {
 	//ncpu = getproccount()
 	ncpu = 1
+	physPageSize = 4096
 }
 
 func Setncpu(n int32) {
@@ -271,7 +272,9 @@ func getRandomData(r []byte) {
 }
 
 func goenvs() {
-	goenvs_unix()
+	if hackmode == 0 {
+		goenvs_unix()
+	}
 }
 
 // Called to do synchronous initialization of Go code built with
@@ -900,6 +903,10 @@ func cls() {
 	sc_put('\n')
 }
 
+func Hmode() bool {
+	return hackmode != 0
+}
+
 var hackmode int64
 var Halt uint32
 
@@ -921,8 +928,8 @@ func Splock(l *Spinlock_t) {
 
 //go:nosplit
 func Spunlock(l *Spinlock_t) {
-	//atomic.Store(&l.v, 0)
-	l.v = 0
+	atomic.Store(&l.v, 0)
+	//l.v = 0
 }
 
 // since this lock may be taken during an interrupt (only under fatal error
@@ -1499,7 +1506,7 @@ func zero_phys(_phys uintptr) {
 	*pml4 = phys | PTE_P | PTE_W
 	_tva := caddr(VREC, VREC, VREC, VTEMP, 0)
 	tva := unsafe.Pointer(_tva)
-	memclr(tva, PGSIZE)
+	memclrNoHeapPointers(tva, PGSIZE)
 	*pml4 = 0
 	invlpg(_tva)
 }
@@ -2630,7 +2637,7 @@ func mksig(t *thread_t, signo int32) {
 	ctxt := (*ucontext_t)(unsafe.Pointer(_ctxt))
 
 	// the profiler only uses rip and rsp of the context...
-	memclr(unsafe.Pointer(_ctxt), ucsz)
+	memclrNoHeapPointers(unsafe.Pointer(_ctxt), ucsz)
 	ctxt.uc_mcontext.rip = t.tf[TF_RIP]
 	ctxt.uc_mcontext.rsp = t.tf[TF_RSP]
 
@@ -2857,7 +2864,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	*(*uintptr)(unsafe.Pointer(rsp)) = 0
 
 	mt := &threads[ti]
-	memclr(unsafe.Pointer(mt), unsafe.Sizeof(thread_t{}))
+	memclrNoHeapPointers(unsafe.Pointer(mt), unsafe.Sizeof(thread_t{}))
 	mt.tf[TF_CS] = KCODE64 << 3
 	mt.tf[TF_RSP] = rsp
 	mt.tf[TF_RIP] = cloneaddr
@@ -2897,7 +2904,7 @@ func hack_setitimer(timer uint32, new, old *itimerval) {
 	Popcli(fl)
 }
 
-func hack_sigaltstack(new, old *sigaltstackt) {
+func hack_sigaltstack(new, old *stackt) {
 	fl := Pushcli()
 	ct := Gscpu().mythread
 	SS_DISABLE := int32(2)

commit ec960a90d798404cb0a389a7bc60e6f9b5ad2da5
Merge: f1e22ef831 41908a5453
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 15:33:11 2016 -0500

    Merge commit '41908a54530120b68a79e0fd22b5e709d33cced0' into newmaster
    
    Conflicts:
            src/runtime/os_linux.go
            src/runtime/sys_linux_amd64.s

commit f1e22ef8310565aab9ae6d542ebade541d997f6d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 11:21:56 2016 -0500

    move yield int number from IRQ range to software int range

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ce142c6b73..23271678a9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2096,11 +2096,11 @@ const (
 	TRAP_SYSCALL	= 64
 	TRAP_TIMER	= 32
 	TRAP_DISK	= (32 + 14)
-	TRAP_YIELD	= 49
 	TRAP_SPUR	= 64
 	TRAP_TLBSHOOT	= 70
 	TRAP_SIGRET	= 71
 	TRAP_PERFMASK	= 72
+	TRAP_YIELD	= 73
 )
 
 var threadlock = &Spinlock_t{}

commit eaf3fec38b4f39b9e7ee9b48eb7164ee8634c983
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 11:05:24 2016 -0500

    manually set ncpu, no-op getRandomData

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 81ff30a3b8..ce142c6b73 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -81,29 +81,29 @@ func futexwakeup(addr *uint32, cnt uint32) {
 	*(*int32)(unsafe.Pointer(uintptr(0x1006))) = 0x1006
 }
 
-func getproccount() int32 {
-	// This buffer is huge (8 kB) but we are on the system stack
-	// and there should be plenty of space (64 kB).
-	// Also this is a leaf, so we're not holding up the memory for long.
-	// See golang.org/issue/11823.
-	// The suggested behavior here is to keep trying with ever-larger
-	// buffers, but we don't have a dynamic memory allocator at the
-	// moment, so that's a bit tricky and seems like overkill.
-	const maxCPUs = 64 * 1024
-	var buf [maxCPUs / (sys.PtrSize * 8)]uintptr
-	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
-	n := int32(0)
-	for _, v := range buf[:r/sys.PtrSize] {
-		for v != 0 {
-			n += int32(v & 1)
-			v >>= 1
-		}
-	}
-	if n == 0 {
-		n = 1
-	}
-	return n
-}
+//func getproccount() int32 {
+//	// This buffer is huge (8 kB) but we are on the system stack
+//	// and there should be plenty of space (64 kB).
+//	// Also this is a leaf, so we're not holding up the memory for long.
+//	// See golang.org/issue/11823.
+//	// The suggested behavior here is to keep trying with ever-larger
+//	// buffers, but we don't have a dynamic memory allocator at the
+//	// moment, so that's a bit tricky and seems like overkill.
+//	const maxCPUs = 64 * 1024
+//	var buf [maxCPUs / (sys.PtrSize * 8)]uintptr
+//	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
+//	n := int32(0)
+//	for _, v := range buf[:r/sys.PtrSize] {
+//		for v != 0 {
+//			n += int32(v & 1)
+//			v >>= 1
+//		}
+//	}
+//	if n == 0 {
+//		n = 1
+//	}
+//	return n
+//}
 
 // Clone, the Linux rfork.
 const (
@@ -226,12 +226,21 @@ func sysargs(argc int32, argv **byte) {
 }
 
 func osinit() {
-	ncpu = getproccount()
+	//ncpu = getproccount()
+	ncpu = 1
+}
+
+func Setncpu(n int32) {
+	ncpu = n
 }
 
 var urandom_dev = []byte("/dev/urandom\x00")
 
 func getRandomData(r []byte) {
+	if hackmode != 0 {
+		pmsg("no random!\n")
+		return
+	}
 	if startupRandomData != nil {
 		n := copy(r, startupRandomData)
 		extendRandom(r, n)
@@ -304,7 +313,9 @@ func minit() {
 	}
 
 	// for debuggers, in case cgo created the thread
-	_g_.m.procid = uint64(gettid())
+	if hackmode == 0 {
+		_g_.m.procid = uint64(gettid())
+	}
 
 	// restore signal mask from m.sigmask and unblock essential signals
 	nmask := _g_.m.sigmask
@@ -1229,6 +1240,7 @@ func hexdump(_p unsafe.Pointer, sz uintptr) {
 //go:nosplit
 func seg_setup() {
 	p := pdesc_t{}
+	chksize(unsafe.Sizeof(p), 16)
 	chksize(unsafe.Sizeof(seg64_t{}), 8)
 	pdsetup(&p, unsafe.Pointer(&_segs[0]), unsafe.Sizeof(_segs) - 1)
 	lgdt(p)

commit 0ac702e900e2199f6b40d3d7228a29c08d4e730c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Dec 16 11:03:53 2016 -0500

    initialize SSE earlier in boot

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 590e7eeeaf..81ff30a3b8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1674,6 +1674,11 @@ func fpuinit(amfirst bool) {
 	}
 }
 
+//go:nosplit
+func fpuinit0() {
+	fpuinit(true)
+}
+
 // LAPIC registers
 const (
 	LAPID		= 0x20/4
@@ -1897,7 +1902,6 @@ func proc_setup() {
 	_sigsimaddr = **(**uintptr)(unsafe.Pointer(&dur2))
 
 	chksize(TFSIZE*8, unsafe.Sizeof(threads[0].tf))
-	fpuinit(true)
 	// initialize the first thread: us
 	threads[0].status = ST_RUNNING
 	threads[0].p_pmap = P_kpmap

commit 106db54e283f176737d642420b2ff3ce71d84e32
Merge: e89ca7e2b0 7a62274065
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Dec 15 15:56:06 2016 -0500

    Merge commit '7a622740655bb5fcbd160eb96887032314842e6e' into newmaster
    
    Conflicts:
            src/runtime/os1_linux.go
            src/runtime/os_linux.go
            src/runtime/runtime2.go

commit 5de8ea145ac0d850a0921f9d37944267ef340a27
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Dec 14 08:47:32 2016 -0500

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f1bc0a4545..aba45dfbb3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1052,10 +1052,8 @@ func int_setup() {
 	int_set(63,  Xmsi7,  1)
 
 	int_set(64,  Xspur,    1)
-	// no longer used
 
 	int_set(70,  Xtlbshoot, 1)
-	// no longer used
 	int_set(72,  Xperfmask, 1)
 
 	p := pdesc_t{}

commit 6338f565c8aee15dae802b39e6337144cf3006fe
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Dec 1 10:40:09 2016 -0500

    fix TLB flushing
    
    broke it in the summer when i added better pmap freeing

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d4d6fb016e..f1bc0a4545 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1625,6 +1625,33 @@ func sysc_setup(myrsp uintptr) {
 	Wrmsr(sysenter_esp, 0)
 }
 
+func Condflush(_refp *int32, p_pmap, va uintptr, pgcount int) bool {
+	var refp *uint32
+	var refc uint32
+	fl := Pushcli()
+	cr3 := Rcr3()
+	ret := true
+	if cr3 != p_pmap {
+		ret = false
+		goto out
+	}
+	// runtime internal atomic doesn't have a load for int32...
+	refp = (*uint32)(unsafe.Pointer(_refp))
+	refc = atomic.Load(refp)
+	if refc > 2 {
+		ret = false
+		goto out
+	}
+	if pgcount == 1 {
+		invlpg(va)
+	} else {
+		Lcr3(cr3)
+	}
+out:
+	Popcli(fl)
+	return ret
+}
+
 var tlbshoot_wait uintptr
 var tlbshoot_pg uintptr
 var tlbshoot_count uintptr

commit 6f287fa2bb5b0b74506ecc586d036dcd11a761e2
Author: Austin Clements <austin@google.com>
Date:   Mon Nov 28 18:03:16 2016 -0500

    runtime: fall back to /proc/self/auxv in Android libs
    
    Android's libc doesn't provide access to auxv, so currently the Go
    runtime synthesizes a fake, minimal auxv when loaded as a library on
    Android. This used to be sufficient, but now we depend on auxv to
    retrieve the system physical page size and panic if we can't retrieve
    it.
    
    Fix this by falling back to reading auxv from /proc/self/auxv if the
    loader-provided auxv is empty and removing the synthetic auxv vectors.
    
    Fixes #18041.
    
    Change-Id: Ia2ec2c764a6609331494a5d359032c56cbb83482
    Reviewed-on: https://go-review.googlesource.com/33652
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: David Crawshaw <crawshaw@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 67c62bc18e..320c1281c2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -187,6 +187,8 @@ const (
 	_AT_HWCAP2 = 26 // hardware capability bit vector 2
 )
 
+var procAuxv = []byte("/proc/self/auxv\x00")
+
 func sysargs(argc int32, argv **byte) {
 	n := argc + 1
 
@@ -200,11 +202,30 @@ func sysargs(argc int32, argv **byte) {
 
 	// now argv+n is auxv
 	auxv := (*[1 << 28]uintptr)(add(unsafe.Pointer(argv), uintptr(n)*sys.PtrSize))
-	sysauxv(auxv[:])
+	if sysauxv(auxv[:]) == 0 {
+		// In some situations we don't get a loader-provided
+		// auxv, such as when loaded as a library on Android.
+		// Fall back to /proc/self/auxv.
+		fd := open(&procAuxv[0], 0 /* O_RDONLY */, 0)
+		if fd < 0 {
+			return
+		}
+		var buf [128]uintptr
+		n := read(fd, noescape(unsafe.Pointer(&buf[0])), int32(unsafe.Sizeof(buf)))
+		closefd(fd)
+		if n < 0 {
+			return
+		}
+		// Make sure buf is terminated, even if we didn't read
+		// the whole file.
+		buf[len(buf)-2] = _AT_NULL
+		sysauxv(buf[:])
+	}
 }
 
-func sysauxv(auxv []uintptr) {
-	for i := 0; auxv[i] != _AT_NULL; i += 2 {
+func sysauxv(auxv []uintptr) int {
+	var i int
+	for ; auxv[i] != _AT_NULL; i += 2 {
 		tag, val := auxv[i], auxv[i+1]
 		switch tag {
 		case _AT_RANDOM:
@@ -218,6 +239,7 @@ func sysauxv(auxv []uintptr) {
 
 		archauxv(tag, val)
 	}
+	return i / 2
 }
 
 func osinit() {

commit d39b7b5347a3e6cc9173224393e4cfaf874a272a
Author: Austin Clements <austin@google.com>
Date:   Mon Nov 28 14:54:38 2016 -0500

    runtime: extract Linux auxv handling
    
    This refactoring is in preparation for handling auxv differently in
    Android shared libraries.
    
    Updates #18041.
    
    Change-Id: If0458a309f9c804e7abd0a58b5a224d89f8da257
    Reviewed-on: https://go-review.googlesource.com/33651
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: David Crawshaw <crawshaw@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 72d17f549a..67c62bc18e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -200,6 +200,10 @@ func sysargs(argc int32, argv **byte) {
 
 	// now argv+n is auxv
 	auxv := (*[1 << 28]uintptr)(add(unsafe.Pointer(argv), uintptr(n)*sys.PtrSize))
+	sysauxv(auxv[:])
+}
+
+func sysauxv(auxv []uintptr) {
 	for i := 0; auxv[i] != _AT_NULL; i += 2 {
 		tag, val := auxv[i], auxv[i+1]
 		switch tag {

commit 1f605175b0044d69ac2364c24f515344c9866fd6
Author: Bryan C. Mills <bcmills@google.com>
Date:   Wed Nov 9 15:28:24 2016 -0500

    runtime/cgo: use libc for sigaction syscalls when possible
    
    This ensures that runtime's signal handlers pass through the TSAN and
    MSAN libc interceptors and subsequent calls to the intercepted
    sigaction function from C will correctly see them.
    
    Fixes #17753.
    
    Change-Id: I9798bb50291a4b8fa20caa39c02a4465ec40bb8d
    Reviewed-on: https://go-review.googlesource.com/33142
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 353522f69f..72d17f549a 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -311,9 +311,6 @@ func sigreturn()
 func sigtramp(sig uint32, info *siginfo, ctx unsafe.Pointer)
 func cgoSigtramp()
 
-//go:noescape
-func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
-
 //go:noescape
 func sigaltstack(new, old *stackt)
 

commit 40aaf283124de44d513ca086976194f0133faa82
Author: Carlos Eduardo Seo <cseo@linux.vnet.ibm.com>
Date:   Fri Oct 28 14:42:42 2016 -0200

    runtime: get ppc64x ISA level and hardware capabilities from HWCAP/HWCAP2
    
    This implements a check that can be done at runtime for the ISA level and
    hardware capability. It follows the same implementation as in s390x.
    
    These checks will be important as we enable new instructions and write go
    asm implementations using those.
    
    Updates #15403
    Fixes #16643
    
    Change-Id: Idfee374a3ffd7cf13a7d8cf0a6c83d247d3bee16
    Reviewed-on: https://go-review.googlesource.com/32330
    Reviewed-by: Lynn Boger <laboger@linux.vnet.ibm.com>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 4fae7aafcb..353522f69f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -184,6 +184,7 @@ const (
 	_AT_PAGESZ = 6  // System physical page size
 	_AT_HWCAP  = 16 // hardware capability bit vector
 	_AT_RANDOM = 25 // introduced in 2.6.29
+	_AT_HWCAP2 = 26 // hardware capability bit vector 2
 )
 
 func sysargs(argc int32, argv **byte) {

commit f1ad4863aae1fd5cd5d0e3e4e6cb6bfae62951a6
Author: Michael Munday <munday@ca.ibm.com>
Date:   Mon Oct 17 17:10:24 2016 -0400

    runtime: get s390x vector facility availability from AT_HWCAP
    
    This is a more robust method for obtaining the availability of vx.
    Since this variable may be checked frequently I've also now
    padded it so that it will be in its own cache line.
    
    I've kept the other check (in hash/crc32) the same for now until
    I can figure out the best way to update it.
    
    Updates #15403.
    
    Change-Id: I74eed651afc6f6a9c5fa3b88fa6a2b0c9ecf5875
    Reviewed-on: https://go-review.googlesource.com/31149
    Reviewed-by: Austin Clements <austin@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1adabe1a42..4fae7aafcb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -182,6 +182,7 @@ var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
 const (
 	_AT_NULL   = 0  // End of vector
 	_AT_PAGESZ = 6  // System physical page size
+	_AT_HWCAP  = 16 // hardware capability bit vector
 	_AT_RANDOM = 25 // introduced in 2.6.29
 )
 

commit a9355f9237a373dc4d5e7737bfdb82738389ab88
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 7 09:23:43 2016 -0400

    allow use of entire E820 page, not just first 15 entries
    
    the code used to put the E820 entries into the MBR, thus the limit. i moved
    them to their own page a long time ago but forgot to increase the limit.
    
    this is necessary for the new biscuit hardware.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 7fb667f1ce..d4d6fb016e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1226,7 +1226,8 @@ func phys_init() {
 	maxfound := uintptr(0)
 	// bootloader provides 15 e820 entries at most (it panicks if the PC
 	// provides more).
-	for i := uintptr(0); i < 15; i++ {
+	e820sz := uintptr(28)
+	for i := uintptr(0); i < 4096/e820sz; i++ {
 		ep := (*e820_t)(unsafe.Pointer(base + i*28))
 		if ep.len == 0 {
 			continue

commit 695238907d4702b9924369238f11c14bdafd3002
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Oct 5 10:07:18 2016 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 9dabea2062..7fb667f1ce 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1512,7 +1512,7 @@ func lapic_setup(calibrate bool) {
 	maskint := uint32(1 << 16)
 	// mask cmci, lint[01], error, perf counters, and thermal sensor
 	wlap(LVCMCI,    maskint)
-	// unmask LINT0 and LINT1. soon, i will use IO APIC instead.
+	// unmask LINT0 and LINT1
 	wlap(LVINT0,    rlap(LVINT0) &^ maskint)
 	wlap(LVINT1,    rlap(LVINT1) &^ maskint)
 	wlap(LVERROR,   maskint)

commit 6c13a1db2ebe146fec7cc7261146ca0e8420f011
Author: Ian Lance Taylor <iant@golang.org>
Date:   Mon Oct 3 16:58:34 2016 -0700

    runtime: don't call cgocallback from signal handler
    
    Calling cgocallback from a signal handler can fail when using the race
    detector. Calling cgocallback will lead to a call to newextram which
    will call oneNewExtraM which will call racegostart. The racegostart
    function will set up some race detector data structures, and doing that
    will sometimes call the C memory allocator. If we are running the signal
    handler from a signal that interrupted the C memory allocator, we will
    crash or hang.
    
    Instead, change the signal handler code to call needm and dropm. The
    needm function will grab allocated m and g structures and initialize the
    g to use the current stack--the signal stack. That is all we need to
    safely call code that allocates memory and checks whether it needs to
    split the stack. This may temporarily leave us with no m available to
    run a cgo callback, but that is OK in this case since the code we call
    will quickly either crash or call dropm to return the m.
    
    Implementing this required changing some of the setSignalstackSP
    functions to avoid a write barrier. These functions never need a write
    barrier but in some cases generated one anyhow because on some systems
    the ss_sp field is a pointer.
    
    Change-Id: I3893f47c3a66278f85eab7f94c1ab11d4f3be133
    Reviewed-on: https://go-review.googlesource.com/30218
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ad9c1894dc..1adabe1a42 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -384,7 +384,7 @@ func getsig(i uint32) uintptr {
 // setSignaltstackSP sets the ss_sp field of a stackt.
 //go:nosplit
 func setSignalstackSP(s *stackt, sp uintptr) {
-	s.ss_sp = (*byte)(unsafe.Pointer(sp))
+	*(*uintptr)(unsafe.Pointer(&s.ss_sp)) = sp
 }
 
 func (c *sigctxt) fixsigcode(sig uint32) {

commit be6e29ededdc2ffb612489bf6c29e75ca57bea02
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Oct 1 15:48:05 2016 -0400

    start ixgbe interrupt handler
    
    configure interrupt throttling too

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 39c37e48db..9dabea2062 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2120,7 +2120,7 @@ func IRQcheck(pp *p) {
 	}
 
 	// wakeup the goroutine for each received IRQ
-	for i := 0; i < 63; i++ {
+	for i := 0; i < 64; i++ {
 		ibit := uintptr(1 << uint(i))
 		if irqs & ibit != 0 {
 			gp := _irqv.handlers[i].igp

commit 15ab8373f959604fb52502ba7062c8b7ef953694
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Sep 30 12:34:11 2016 -0400

    non-overlapping MSI and IRQ ranges; interrupt cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1500108e75..39c37e48db 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -924,10 +924,7 @@ func Xfp()
 func Xve()
 func Xtimer()
 func Xspur()
-func Xyield()
-func Xsyscall()
 func Xtlbshoot()
-func Xsigret()
 func Xperfmask()
 func Xirq1()
 func Xirq2()
@@ -952,6 +949,14 @@ func Xirq20()
 func Xirq21()
 func Xirq22()
 func Xirq23()
+func Xmsi0()
+func Xmsi1()
+func Xmsi2()
+func Xmsi3()
+func Xmsi4()
+func Xmsi5()
+func Xmsi6()
+func Xmsi7()
 
 type idte_t struct {
 	baselow	uint16
@@ -1010,41 +1015,47 @@ func int_setup() {
 	int_set(19,  Xfp,  0)
 	int_set(20,  Xve,  0)
 
-	// interrupts
-	irqbase := 32
-	int_set(irqbase+ 0,  Xtimer,  1)
-	int_set(irqbase+ 1,  Xirq1,   1)
-	int_set(irqbase+ 2,  Xirq2,   1)
-	int_set(irqbase+ 3,  Xirq3,   1)
-	int_set(irqbase+ 4,  Xirq4,   1)
-	int_set(irqbase+ 5,  Xirq5,   1)
-	int_set(irqbase+ 6,  Xirq6,   1)
-	int_set(irqbase+ 7,  Xirq7,   1)
-	int_set(irqbase+ 8,  Xirq8,   1)
-	int_set(irqbase+ 9,  Xirq9,   1)
-	int_set(irqbase+10,  Xirq10,  1)
-	int_set(irqbase+11,  Xirq11,  1)
-	int_set(irqbase+12,  Xirq12,  1)
-	int_set(irqbase+13,  Xirq13,  1)
-	int_set(irqbase+14,  Xirq14,  1)
-	int_set(irqbase+15,  Xirq15,  1)
-	int_set(irqbase+16,  Xirq16,  1)
-	int_set(irqbase+17,  Xirq17,  1)
-	int_set(irqbase+18,  Xirq18,  1)
-	int_set(irqbase+19,  Xirq19,  1)
-	int_set(irqbase+20,  Xirq20,  1)
-	int_set(irqbase+21,  Xirq21,  1)
-	int_set(irqbase+22,  Xirq22,  1)
-	int_set(irqbase+23,  Xirq23,  1)
-
-	int_set(48,  Xspur,    1)
+	// IRQs
+	int_set(32,  Xtimer,  1)
+	int_set(33,  Xirq1,   1)
+	int_set(34,  Xirq2,   1)
+	int_set(35,  Xirq3,   1)
+	int_set(36,  Xirq4,   1)
+	int_set(37,  Xirq5,   1)
+	int_set(38,  Xirq6,   1)
+	int_set(39,  Xirq7,   1)
+	int_set(40,  Xirq8,   1)
+	int_set(41,  Xirq9,   1)
+	int_set(42,  Xirq10,  1)
+	int_set(43,  Xirq11,  1)
+	int_set(44,  Xirq12,  1)
+	int_set(45,  Xirq13,  1)
+	int_set(46,  Xirq14,  1)
+	int_set(47,  Xirq15,  1)
+	int_set(48,  Xirq16,  1)
+	int_set(49,  Xirq17,  1)
+	int_set(50,  Xirq18,  1)
+	int_set(51,  Xirq19,  1)
+	int_set(52,  Xirq20,  1)
+	int_set(53,  Xirq21,  1)
+	int_set(54,  Xirq22,  1)
+	int_set(55,  Xirq23,  1)
+
+	// MSI interrupts
+	int_set(56,  Xmsi0,  1)
+	int_set(57,  Xmsi1,  1)
+	int_set(58,  Xmsi2,  1)
+	int_set(59,  Xmsi3,  1)
+	int_set(60,  Xmsi4,  1)
+	int_set(61,  Xmsi5,  1)
+	int_set(62,  Xmsi6,  1)
+	int_set(63,  Xmsi7,  1)
+
+	int_set(64,  Xspur,    1)
 	// no longer used
-	//int_set(49,  Xyield,   1)
-	//int_set(64,  Xsyscall, 1)
 
 	int_set(70,  Xtlbshoot, 1)
 	// no longer used
-	//int_set(71,  Xsigret,   1)
 	int_set(72,  Xperfmask, 1)
 
 	p := pdesc_t{}
@@ -1694,12 +1705,11 @@ const (
 	TRAP_SYSCALL	= 64
 	TRAP_TIMER	= 32
 	TRAP_DISK	= (32 + 14)
-	TRAP_SPUR	= 48
 	TRAP_YIELD	= 49
+	TRAP_SPUR	= 64
 	TRAP_TLBSHOOT	= 70
 	TRAP_SIGRET	= 71
 	TRAP_PERFMASK	= 72
-	IRQ_BASE	= 32
 )
 
 var threadlock = &Spinlock_t{}
@@ -1882,16 +1892,6 @@ func trap(tf *[TFSIZE]uintptr) {
 		}
 		// yieldy doesn't return
 		yieldy()
-	} else if is_irq(trapno) {
-		if _newtrap != nil {
-			// catch kernel faults that occur while trying to
-			// handle user traps
-			_newtrap(tf)
-		} else {
-			pancake("IRQ without ntrap", trapno)
-		}
-		lap_eoi()
-		sched_resume(ct)
 	} else if is_cpuex(trapno) {
 		// we vet out kernel mode CPU exceptions above must be from
 		// user program. thus return from Userrun() to kernel.
@@ -1904,20 +1904,23 @@ func trap(tf *[TFSIZE]uintptr) {
 		perfmask()
 		sched_resume(ct)
 	} else {
-		pancake("unexpected int", trapno)
+		if _newtrap != nil {
+			// catch kernel faults that occur while trying to
+			// handle user traps
+			_newtrap(tf)
+		} else {
+			pancake("IRQ without ntrap", trapno)
+		}
+		lap_eoi()
+		sched_resume(ct)
 	}
 	// not reached
 	pancake("no returning", 0)
 }
 
-//go:nosplit
-func is_irq(trapno uintptr) bool {
-	return trapno > IRQ_BASE && trapno <= IRQ_BASE + 24
-}
-
 //go:nosplit
 func is_cpuex(trapno uintptr) bool {
-	return trapno < IRQ_BASE
+	return trapno < 32
 }
 
 //go:nosplit
@@ -2050,7 +2053,7 @@ func irqsched_m(gp *g) {
 		nstatus = _Gwaiting
 		gp.waitreason = "waiting for trap"
 		if _irqv.handlers[irq].igp != nil {
-			pancake("igp exists", 0)
+			pancake("igp exists", uintptr(irq))
 		}
 		_irqv.handlers[irq].igp = gp
 		start = false

commit 181dd4903156bc1df3e2a061f41c1ba60f7f4c2c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Sep 30 11:14:27 2016 -0400

    tie up loose end
    
    make sure performance counter interrupt handler doesn't clobber SSE regs

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index de69f31b40..1500108e75 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1784,10 +1784,14 @@ func kernel_fault(tf *[TFSIZE]uintptr) {
 func trap(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 
-	// XXX XXX XXX XXX make sure this can't clobber SSE regs.
 	if trapno == TRAP_NMI {
+		// prevent SSE corruption: set TS in cr0 to make sure SSE
+		// instructions generate a fault
+		ts := uintptr(1 << 3)
+		Lcr0(Rcr0() | ts)
 		perfgather(tf)
 		perfmask()
+		Lcr0(Rcr0() &^ ts)
 		_trapret(tf)
 	}
 
@@ -1809,12 +1813,12 @@ func trap(tf *[TFSIZE]uintptr) {
 		}
 	}
 
-	// don't add code before FPU context saving unless you've thought very
-	// carefully! it is easy to accidentally and silently corrupt FPU state
-	// (ie calling memmove indirectly by assignment of large datatypes)
-	// before it is saved below.
+	// don't add code before FPU/SSE/MMX context saving unless you've
+	// thought very carefully! it is easy to accidentally and silently
+	// corrupt SSE state (ie calling memmove indirectly by assignment of
+	// large datatypes) before it is saved below.
 
-	// save FPU state immediately before we clobber it
+	// save SSE state immediately before we clobber it
 	if ct != nil {
 		// if in user mode, save to user buffers and make it look like
 		// Userrun returned.

commit eb268cb321edf6e2bbaa832acb2e61db6b081f98
Author: Ian Lance Taylor <iant@golang.org>
Date:   Tue Sep 27 22:24:51 2016 -0700

    runtime: minor simplifications to signal code
    
    Change setsig, setsigstack, getsig, raise, raiseproc to take uint32 for
    signal number parameter, as that is the type mostly used for signal
    numbers.  Same for dieFromSignal, sigInstallGoHandler, raisebadsignal.
    
    Remove setsig restart parameter, as it is always either true or
    irrelevant.
    
    Don't check the handler in setsigstack, as the only caller does that
    anyhow.
    
    Don't bother to convert the handler from sigtramp to sighandler in
    getsig, as it will never be called when the handler is sigtramp or
    sighandler.
    
    Don't check the return value from rt_sigaction in the GNU/Linux version
    of setsigstack; no other setsigstack checks it, and it never fails.
    
    Change-Id: I6bbd677e048a77eddf974dd3d017bc3c560fbd48
    Reviewed-on: https://go-review.googlesource.com/29953
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 52b6b63868..ad9c1894dc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -329,8 +329,8 @@ func sigprocmask(how int32, new, old *sigset) {
 
 //go:noescape
 func getrlimit(kind int32, limit unsafe.Pointer) int32
-func raise(sig int32)
-func raiseproc(sig int32)
+func raise(sig uint32)
+func raiseproc(sig uint32)
 
 //go:noescape
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
@@ -338,12 +338,9 @@ func osyield()
 
 //go:nosplit
 //go:nowritebarrierrec
-func setsig(i int32, fn uintptr, restart bool) {
+func setsig(i uint32, fn uintptr) {
 	var sa sigactiont
-	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK | _SA_RESTORER
-	if restart {
-		sa.sa_flags |= _SA_RESTART
-	}
+	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK | _SA_RESTORER | _SA_RESTART
 	sigfillset(&sa.sa_mask)
 	// Although Linux manpage says "sa_restorer element is obsolete and
 	// should not be used". x86_64 kernel requires it. Only use it on
@@ -364,30 +361,23 @@ func setsig(i int32, fn uintptr, restart bool) {
 
 //go:nosplit
 //go:nowritebarrierrec
-func setsigstack(i int32) {
+func setsigstack(i uint32) {
 	var sa sigactiont
-	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
-		throw("rt_sigaction failure")
-	}
-	if sa.sa_handler == 0 || sa.sa_handler == _SIG_DFL || sa.sa_handler == _SIG_IGN || sa.sa_flags&_SA_ONSTACK != 0 {
+	rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask))
+	if sa.sa_flags&_SA_ONSTACK != 0 {
 		return
 	}
 	sa.sa_flags |= _SA_ONSTACK
-	if rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask)) != 0 {
-		throw("rt_sigaction failure")
-	}
+	rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask))
 }
 
 //go:nosplit
 //go:nowritebarrierrec
-func getsig(i int32) uintptr {
+func getsig(i uint32) uintptr {
 	var sa sigactiont
 	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
 		throw("rt_sigaction read failure")
 	}
-	if sa.sa_handler == funcPC(sigtramp) || sa.sa_handler == funcPC(cgoSigtramp) {
-		return funcPC(sighandler)
-	}
 	return sa.sa_handler
 }
 

commit e2e11f02a4627e4090083d433e6c66602b514ab7
Author: Ian Lance Taylor <iant@golang.org>
Date:   Mon Sep 26 11:35:55 2016 -0700

    runtime: unify Unix implementations of unminit
    
    Change-Id: I2cbb13eb85876ad05a52cbd498a9b86e7a28899c
    Reviewed-on: https://go-review.googlesource.com/29772
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 92c3db8616..52b6b63868 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -266,9 +266,7 @@ func minit() {
 // Called from dropm to undo the effect of an minit.
 //go:nosplit
 func unminit() {
-	if getg().m.newSigstack {
-		signalstack(nil)
-	}
+	unminitSignals()
 }
 
 func memlimit() uintptr {

commit ac24388e5e5bdc129451c074a349a982e1d55ffa
Author: Ian Lance Taylor <iant@golang.org>
Date:   Mon Sep 26 11:14:41 2016 -0700

    runtime: merge setting new signal mask in minit
    
    All the variants that sets the new signal mask in minit do the same
    thing, so merge them. This requires an OS-specific sigdelset function;
    the function already exists for linux, and is now added for other OS's.
    
    Change-Id: Ie96f6f02e2cf09c43005085985a078bd9581f670
    Reviewed-on: https://go-review.googlesource.com/29771
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index bc0d9f2027..92c3db8616 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -257,22 +257,10 @@ func gettid() uint32
 // Called to initialize a new m (including the bootstrap m).
 // Called on the new thread, cannot allocate memory.
 func minit() {
-	// Initialize signal handling.
-	_g_ := getg()
-
-	minitSignalStack()
+	minitSignals()
 
 	// for debuggers, in case cgo created the thread
-	_g_.m.procid = uint64(gettid())
-
-	// restore signal mask from m.sigmask and unblock essential signals
-	nmask := _g_.m.sigmask
-	for i := range sigtable {
-		if sigtable[i].flags&_SigUnblock != 0 {
-			sigdelset(&nmask, i)
-		}
-	}
-	sigprocmask(_SIG_SETMASK, &nmask, nil)
+	getg().m.procid = uint64(gettid())
 }
 
 // Called from dropm to undo the effect of an minit.

commit c2735039f3ea4e44a3c1df6ef6715e83bc5257b1
Author: Ian Lance Taylor <iant@golang.org>
Date:   Sun Sep 25 21:33:27 2016 -0700

    runtime: unify sigtrampgo
    
    Combine the various versions of sigtrampgo into a single function in
    signal_unix.go. This requires defining a fixsigcode method on sigctxt
    for all operating systems; it only does something on Darwin. This also
    requires changing the darwin/amd64 signal handler to call sigreturn
    itself, rather than relying on sigtrampgo to call sigreturn for it. We
    can then drop the Darwin sigreturn function, as it is no longer used.
    
    Change-Id: I5a0b9d2d2c141957e151b41e694efeb20e4b4b9a
    Reviewed-on: https://go-review.googlesource.com/29761
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 88139ae2fc..bc0d9f2027 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -410,3 +410,6 @@ func getsig(i int32) uintptr {
 func setSignalstackSP(s *stackt, sp uintptr) {
 	s.ss_sp = (*byte)(unsafe.Pointer(sp))
 }
+
+func (c *sigctxt) fixsigcode(sig uint32) {
+}

commit d15295c6790b70eba0e4a3aa7ddead251aa440da
Author: Ian Lance Taylor <iant@golang.org>
Date:   Sun Sep 25 13:38:54 2016 -0700

    runtime: unify handling of alternate signal stack
    
    Change all Unix systems to use stackt for the alternate signal
    stack (some were using sigaltstackt). Add OS-specific setSignalstackSP
    function to handle different types for ss_sp field, and unify all
    OS-specific signalstack functions into one. Unify handling of alternate
    signal stack in OS-specific minit and sigtrampgo functions via new
    functions minitSignalstack and setGsignalStack.
    
    Change-Id: Idc316dc69b1dd725717acdf61a1cd8b9f33ed174
    Reviewed-on: https://go-review.googlesource.com/29757
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 7b3ce71fea..88139ae2fc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -260,21 +260,7 @@ func minit() {
 	// Initialize signal handling.
 	_g_ := getg()
 
-	var st sigaltstackt
-	sigaltstack(nil, &st)
-	if st.ss_flags&_SS_DISABLE != 0 {
-		signalstack(&_g_.m.gsignal.stack)
-		_g_.m.newSigstack = true
-	} else {
-		// Use existing signal stack.
-		stsp := uintptr(unsafe.Pointer(st.ss_sp))
-		_g_.m.gsignal.stack.lo = stsp
-		_g_.m.gsignal.stack.hi = stsp + st.ss_size
-		_g_.m.gsignal.stackguard0 = stsp + _StackGuard
-		_g_.m.gsignal.stackguard1 = stsp + _StackGuard
-		_g_.m.gsignal.stackAlloc = st.ss_size
-		_g_.m.newSigstack = false
-	}
+	minitSignalStack()
 
 	// for debuggers, in case cgo created the thread
 	_g_.m.procid = uint64(gettid())
@@ -341,7 +327,7 @@ func cgoSigtramp()
 func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
 
 //go:noescape
-func sigaltstack(new, old *sigaltstackt)
+func sigaltstack(new, old *stackt)
 
 //go:noescape
 func setitimer(mode int32, new, old *itimerval)
@@ -419,15 +405,8 @@ func getsig(i int32) uintptr {
 	return sa.sa_handler
 }
 
+// setSignaltstackSP sets the ss_sp field of a stackt.
 //go:nosplit
-func signalstack(s *stack) {
-	var st sigaltstackt
-	if s == nil {
-		st.ss_flags = _SS_DISABLE
-	} else {
-		st.ss_sp = (*byte)(unsafe.Pointer(s.lo))
-		st.ss_size = s.hi - s.lo
-		st.ss_flags = 0
-	}
-	sigaltstack(&st, nil)
+func setSignalstackSP(s *stackt, sp uintptr) {
+	s.ss_sp = (*byte)(unsafe.Pointer(sp))
 }

commit ab552aa3b69deb208b38677880e86aa41c3a9e47
Author: Ian Lance Taylor <iant@golang.org>
Date:   Fri Sep 23 17:54:51 2016 -0700

    runtime: unify some signal handling functions
    
    Unify the OS-specific versions of msigsave, msigrestore, sigblock,
    updatesigmask, and unblocksig into single versions in signal_unix.go.
    To do this, make sigprocmask work the same way on all systems, which
    required adding a definition of sigprocmask for linux and openbsd.
    Also add a single OS-specific function sigmaskToSigset.
    
    Change-Id: I7cbf75131dddb57eeefe648ef845b0791404f785
    Reviewed-on: https://go-review.googlesource.com/29689
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: David Crawshaw <crawshaw@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 910455833f..7b3ce71fea 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -148,9 +148,9 @@ func newosproc(mp *m, stk unsafe.Pointer) {
 	// Disable signals during clone, so that the new thread starts
 	// with signals disabled. It will enable them in minit.
 	var oset sigset
-	rtsigprocmask(_SIG_SETMASK, &sigset_all, &oset, int32(unsafe.Sizeof(oset)))
+	sigprocmask(_SIG_SETMASK, &sigset_all, &oset)
 	ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart)))
-	rtsigprocmask(_SIG_SETMASK, &oset, nil, int32(unsafe.Sizeof(oset)))
+	sigprocmask(_SIG_SETMASK, &oset, nil)
 
 	if ret < 0 {
 		print("runtime: failed to create new OS thread (have ", mcount(), " already; errno=", -ret, ")\n")
@@ -252,22 +252,6 @@ func mpreinit(mp *m) {
 	mp.gsignal.m = mp
 }
 
-//go:nosplit
-func msigsave(mp *m) {
-	smask := &mp.sigmask
-	rtsigprocmask(_SIG_SETMASK, nil, smask, int32(unsafe.Sizeof(*smask)))
-}
-
-//go:nosplit
-func msigrestore(sigmask sigset) {
-	rtsigprocmask(_SIG_SETMASK, &sigmask, nil, int32(unsafe.Sizeof(sigmask)))
-}
-
-//go:nosplit
-func sigblock() {
-	rtsigprocmask(_SIG_SETMASK, &sigset_all, nil, int32(unsafe.Sizeof(sigset_all)))
-}
-
 func gettid() uint32
 
 // Called to initialize a new m (including the bootstrap m).
@@ -302,7 +286,7 @@ func minit() {
 			sigdelset(&nmask, i)
 		}
 	}
-	rtsigprocmask(_SIG_SETMASK, &nmask, nil, int32(unsafe.Sizeof(nmask)))
+	sigprocmask(_SIG_SETMASK, &nmask, nil)
 }
 
 // Called from dropm to undo the effect of an minit.
@@ -363,7 +347,13 @@ func sigaltstack(new, old *sigaltstackt)
 func setitimer(mode int32, new, old *itimerval)
 
 //go:noescape
-func rtsigprocmask(sig uint32, new, old *sigset, size int32)
+func rtsigprocmask(how int32, new, old *sigset, size int32)
+
+//go:nosplit
+//go:nowritebarrierrec
+func sigprocmask(how int32, new, old *sigset) {
+	rtsigprocmask(how, new, old, int32(unsafe.Sizeof(*new)))
+}
 
 //go:noescape
 func getrlimit(kind int32, limit unsafe.Pointer) int32
@@ -441,17 +431,3 @@ func signalstack(s *stack) {
 	}
 	sigaltstack(&st, nil)
 }
-
-//go:nosplit
-//go:nowritebarrierrec
-func updatesigmask(m sigmask) {
-	var mask sigset
-	sigcopyset(&mask, m)
-	rtsigprocmask(_SIG_SETMASK, &mask, nil, int32(unsafe.Sizeof(mask)))
-}
-
-func unblocksig(sig int32) {
-	var mask sigset
-	sigaddset(&mask, int(sig))
-	rtsigprocmask(_SIG_UNBLOCK, &mask, nil, int32(unsafe.Sizeof(mask)))
-}

commit 211db1ef73659ddec408e9aac0c56db8e82238c7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Sep 19 11:28:02 2016 -0400

    checkpoint baby ixgbe driver
    
    general initialization and MSI testing. MSI is amazing compared to PCI
    interrupts!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a1c5baec7f..de69f31b40 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2051,7 +2051,6 @@ func irqsched_m(gp *g) {
 		_irqv.handlers[irq].igp = gp
 		start = false
 	} else {
-		_pmsg("X")
 		nstatus = _Grunnable
 		start = true
 		// clear flag in order to detect any IRQs that occur before the

commit 682404b7fb59f9cbe186b27bb4b284594d58d80b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Sep 13 12:14:32 2016 -0400

    Store32() to guarantee 32bit writes without lock prefix
    
    all of the storing functions in go's atomic package use xchg, but asserting
    lock while storing to memory mapped registers seems like a bad idea.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c7eaa1fbe9..a1c5baec7f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -78,6 +78,7 @@ func Rdmsr(int) int
 func Rdtsc() uint64
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
+func Store32(*uint32, uint32)
 func stackcheck()
 func Sti()
 func _sysentry()

commit 6bd4bbe83eebe7f0f0609c13b2525ab9dcc2ded0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Sep 10 18:32:58 2016 -0400

    new IRQ subsystem
    
    new system is simpler and doesn't create a new goroutine for each IRQ. instead
    a driver creates an IRQ-handling goroutine which calls runtime.IRQsched(irqn)
    where irqn is the IRQ number that, when received, will wake the goroutine.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index bdb198aa25..c7eaa1fbe9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1999,157 +1999,156 @@ func yieldy() {
 	sched_halt()
 }
 
-// called only once to setup
-func Trapinit() {
-	mcall(trapinit_m)
-}
-
-func Trapsched() {
-	mcall(trapsched_m)
+var _irqv struct {
+	// slock protects everything in _irqv
+	slock		Spinlock_t
+	handlers	[64]struct {
+		igp	*g
+		started	bool
+	}
+	// flag indicating whether a thread in schedule() should check for
+	// runnable IRQ-handling goroutines
+	check		bool
+	// bitmask of IRQs that have requested service
+	irqs		uintptr
+}
+
+// IRQsched yields iff there have been no new IRQs since the last time the
+// calling goroutine was scheduled.
+func IRQsched(irq uint) {
+	gp := getg()
+	gp.m.irqn = irq
+	mcall(irqsched_m)
 }
 
-// called from the CPU interrupt handler. must only be called while interrupts
-// are disabled
- //go:nosplit
- func Trapwake() {
-	Splock(tlock);
-	_nints++
-	// only flag the Ps if a handler isn't currently active
-	if trapst == IDLE {
-		_ptrap = true
+func irqsched_m(gp *g) {
+	// have new IRQs arrived?
+	irq := gp.m.irqn
+	gp.m.irqn = 0
+	if irq > 63 {
+		throw("bad irq " + string(irq))
 	}
-	Spunlock(tlock)
-}
-// trap handling goroutines first call. it is not an error if there are no
-// interrupts when this is called.
-func trapinit_m(gp *g) {
-	_trapsched(gp, true)
-}
 
-func trapsched_m(gp *g) {
-	_trapsched(gp, false)
-}
-
-var tlock = &Spinlock_t{}
-
-var _initted bool
-var _trapsleeper *g
-var _nints int
-
-// i think it is ok for these trap sched/wake functions to be nosplit and call
-// nosplit functions even though they clear interrupts because we first switch
-// to the scheduler stack where preemptions are ignored.
-func _trapsched(gp *g, firsttime bool) {
 	fl := Pushcli()
-	Splock(tlock)
+	Splock(&_irqv.slock)
 
-	if firsttime {
-		if _initted {
-			pancake("two inits", 0)
-		}
-		_initted = true
-		// if there are traps already, let a P wake us up.
-		//goto bed
+	status := readgstatus(gp)
+	if((status&^_Gscan) != _Grunning){
+		pancake("bad g status", uintptr(status))
 	}
 
-	// decrement handled ints
-	if !firsttime {
-		if _nints <= 0 {
-			pancake("neg ints", uintptr(_nints))
+	var nstatus uint32
+	var start bool
+	bit := uintptr(1 << irq)
+	sleeping := _irqv.irqs & bit == 0
+	if sleeping {
+		nstatus = _Gwaiting
+		gp.waitreason = "waiting for trap"
+		if _irqv.handlers[irq].igp != nil {
+			pancake("igp exists", 0)
 		}
-		_nints--
-	}
-
-	// check if we are done
-	if !firsttime && _nints != 0 {
-		// keep processing
-		tprepsleep(gp, false)
-		_g_ := getg()
-		runqput(_g_.m.p.ptr(), gp, true)
+		_irqv.handlers[irq].igp = gp
+		start = false
 	} else {
-		tprepsleep(gp, true)
-		if _trapsleeper != nil {
-			pancake("trapsleeper set", 0)
-		}
-		_trapsleeper = gp
+		_pmsg("X")
+		nstatus = _Grunnable
+		start = true
+		// clear flag in order to detect any IRQs that occur before the
+		// next IRQsched.
+		_irqv.irqs &^= bit
+	}
+
+	_irqv.handlers[irq].started = start
+	// _Gscan shouldn't be set since gp's status is running
+	casgstatus(gp, _Grunning, nstatus)
+	pp := gp.m.p.ptr()
+	dropg()
+
+	if !sleeping {
+		// casgstatus must happen before runqput
+		runqput(pp, gp, true)
 	}
 
-	Spunlock(tlock)
+	Spunlock(&_irqv.slock)
 	Popcli(fl)
 
 	schedule()
 }
 
-var trapst int
-const (
-	IDLE	= iota
-	RUNNING	= iota
-)
-
-func tprepsleep(gp *g, done bool) {
-	if done {
-		trapst = IDLE
-	} else {
-		trapst = RUNNING
-	}
-
-	status := readgstatus(gp)
-	if((status&^_Gscan) != _Grunning){
-		pancake("bad g status", uintptr(status))
-	}
-	nst := uint32(_Grunnable)
-	if done {
-		nst = _Gwaiting
-		gp.waitreason = "waiting for trap"
+// called from the CPU interrupt handler. must only be called while interrupts
+// are disabled
+//go:nosplit
+func IRQwake(irq uint) {
+	if irq > 63 {
+		pancake("bad irq", uintptr(irq))
 	}
-	casgstatus(gp, _Grunning, nst)
-	dropg()
+	Splock(&_irqv.slock)
+	_irqv.irqs |= 1 << irq
+	_irqv.check = true
+	Spunlock(&_irqv.slock)
 }
 
-var _ptrap bool
-
-func trapcheck(pp *p) {
-	if !_ptrap {
+// puts IRQ-handling goroutines for outstanding IRQs on the runq. called by Ms
+// from findrunnable() before checking local run queue
+func IRQcheck(pp *p) {
+	if !_irqv.check {
 		return
 	}
+
 	fl := Pushcli()
-	Splock(tlock)
+	Splock(&_irqv.slock)
 
-	var trapgp *g
-	if !_ptrap {
-		goto out
-	}
-	// don't clear the start flag if the handler goroutine hasn't
-	// registered yet.
-	if !_initted {
+	var gps [64]*g
+	var gidx int
+	var newirqs uintptr
+	irqs := _irqv.irqs
+
+	if !_irqv.check {
 		goto out
 	}
-	if trapst == RUNNING {
+	_irqv.check = false
+
+	if irqs == 0 {
 		goto out
 	}
 
-	if trapst != IDLE {
-		pancake("bad trap status", uintptr(trapst))
+	// wakeup the goroutine for each received IRQ
+	for i := 0; i < 63; i++ {
+		ibit := uintptr(1 << uint(i))
+		if irqs & ibit != 0 {
+			gp := _irqv.handlers[i].igp
+			// the IRQ-handling goroutine has not yet called
+			// IRQsched; keep trying until it does.
+			if gp == nil {
+				newirqs |= ibit
+				continue
+			}
+			gst := readgstatus(gp)
+			if gst &^ _Gscan != _Gwaiting {
+				pancake("bad igp status", uintptr(gst))
+			}
+			_irqv.handlers[i].igp = nil
+			_irqv.handlers[i].started = true
+			// we cannot set gstatus or put to run queue before we
+			// release the spinlock since either operation may
+			// block
+			gps[gidx] = gp
+			gidx++
+		}
+	}
+	_irqv.irqs = newirqs
+	if newirqs != 0 {
+		_irqv.check = true
 	}
-
-	_ptrap = false
-	trapst = RUNNING
-
-	trapgp = _trapsleeper
-	_trapsleeper = nil
-
-	Spunlock(tlock)
-	Popcli(fl)
-
-	casgstatus(trapgp, _Gwaiting, _Grunnable)
-
-	// hopefully the trap goroutine is executed soon
-	runqput(pp, trapgp, true)
-	return
 out:
-	Spunlock(tlock)
+	Spunlock(&_irqv.slock)
 	Popcli(fl)
-	return
+
+	for i := 0; i < gidx; i++ {
+		gp := gps[i]
+		casgstatus(gp, _Gwaiting, _Grunnable)
+		runqput(pp, gp, true)
+	}
 }
 
 // goprofiling is implemented by simulating the SIGPROF signal. when proftick

commit 276a52de55fb48c4e56a778f1f7cac9292d8fad7
Author: Austin Clements <austin@google.com>
Date:   Mon Jul 18 21:40:02 2016 -0400

    runtime: fetch physical page size from the OS
    
    Currently the physical page size assumed by the runtime is hard-coded.
    On Linux the runtime at least fetches the OS page size during init and
    sanity checks against the hard-coded value, but they may still differ.
    On other OSes we wouldn't even notice.
    
    Add support on all OSes to fetch the actual OS physical page size
    during runtime init and lift the sanity check of PhysPageSize from the
    Linux init code to general malloc init. Currently this is the only use
    of the retrieved page size, but we'll add more shortly.
    
    Updates #12480 and #10180.
    
    Change-Id: I065f2834bc97c71d3208edc17fd990ec9058b6da
    Reviewed-on: https://go-review.googlesource.com/25050
    Run-TryBot: Austin Clements <austin@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 796e05a69e..910455833f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -207,17 +207,7 @@ func sysargs(argc int32, argv **byte) {
 			startupRandomData = (*[16]byte)(unsafe.Pointer(val))[:]
 
 		case _AT_PAGESZ:
-			// Check that the true physical page size is
-			// compatible with the runtime's assumed
-			// physical page size.
-			if sys.PhysPageSize < val {
-				print("runtime: kernel page size (", val, ") is larger than runtime page size (", sys.PhysPageSize, ")\n")
-				exit(1)
-			}
-			if sys.PhysPageSize%val != 0 {
-				print("runtime: runtime page size (", sys.PhysPageSize, ") is not a multiple of kernel page size (", val, ")\n")
-				exit(1)
-			}
+			physPageSize = val
 		}
 
 		archauxv(tag, val)

commit d8a4be6ab399396c8a6ea9ec9fc8d634b9d6b115
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Sep 6 14:45:25 2016 -0400

    support PCI memory bars, cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0097e9e0fc..bdb198aa25 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -54,7 +54,7 @@ func htpause()
 func invlpg(uintptr)
 func Inb(uint16) uint
 func Inl(int) int
-func Insl(int, unsafe.Pointer, int)
+func Insl(uint16, unsafe.Pointer, uint)
 func Invlpg(unsafe.Pointer)
 func Lcr0(uintptr)
 func Lcr3(uintptr)

commit 1762b976a717768353ca0fa1741ea40e6008fca8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Sep 6 10:51:33 2016 -0400

    use IOAPIC instead of 8259s
    
    no real functional change. mostly documentation about x86 interrupts.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e357678747..0097e9e0fc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1885,6 +1885,7 @@ func trap(tf *[TFSIZE]uintptr) {
 		} else {
 			pancake("IRQ without ntrap", trapno)
 		}
+		lap_eoi()
 		sched_resume(ct)
 	} else if is_cpuex(trapno) {
 		// we vet out kernel mode CPU exceptions above must be from
@@ -1906,7 +1907,7 @@ func trap(tf *[TFSIZE]uintptr) {
 
 //go:nosplit
 func is_irq(trapno uintptr) bool {
-	return trapno > IRQ_BASE && trapno <= IRQ_BASE + 15
+	return trapno > IRQ_BASE && trapno <= IRQ_BASE + 24
 }
 
 //go:nosplit

commit c4eaea40aa0fe5a50728a0a8a260efd68e1a96dc
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Sep 6 10:50:49 2016 -0400

    export spinlocks from runtime

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a6155ce1e5..e357678747 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -565,13 +565,12 @@ func cls() {
 var hackmode int64
 var Halt uint32
 
-// wait until remove definition from proc.c
-type spinlock_t struct {
+type Spinlock_t struct {
 	v	uint32
 }
 
 //go:nosplit
-func splock(l *spinlock_t) {
+func Splock(l *Spinlock_t) {
 	for {
 		if atomic.Xchg(&l.v, 1) == 0 {
 			break
@@ -583,14 +582,14 @@ func splock(l *spinlock_t) {
 }
 
 //go:nosplit
-func spunlock(l *spinlock_t) {
+func Spunlock(l *Spinlock_t) {
 	//atomic.Store(&l.v, 0)
 	l.v = 0
 }
 
 // since this lock may be taken during an interrupt (only under fatal error
 // conditions), interrupts must be cleared before attempting to take this lock.
-var pmsglock = &spinlock_t{}
+var pmsglock = &Spinlock_t{}
 
 //go:nosplit
 func _pmsg(msg string) {
@@ -606,9 +605,9 @@ func _pmsg(msg string) {
 //go:nosplit
 func pmsg(msg string) {
 	fl := Pushcli()
-	splock(pmsglock)
+	Splock(pmsglock)
 	_pmsg(msg)
-	spunlock(pmsglock)
+	Spunlock(pmsglock)
 	Popcli(fl)
 }
 
@@ -628,21 +627,21 @@ func _pnum(n uintptr) {
 //go:nosplit
 func pnum(n uintptr) {
 	fl := Pushcli()
-	splock(pmsglock)
+	Splock(pmsglock)
 	_pnum(n)
-	spunlock(pmsglock)
+	Spunlock(pmsglock)
 	Popcli(fl)
 }
 
 func Pmsga(_p *uint8, c int, attr int8) {
 	pn := uintptr(unsafe.Pointer(_p))
 	fl := Pushcli()
-	splock(pmsglock)
+	Splock(pmsglock)
 	for i := uintptr(0); i < uintptr(c); i++ {
 		p := (*int8)(unsafe.Pointer(pn+i))
 		putcha(*p, attr)
 	}
-	spunlock(pmsglock)
+	Spunlock(pmsglock)
 	Popcli(fl)
 }
 
@@ -1569,11 +1568,11 @@ func Ap_setup(cpunum uint) {
 	// interrupts are probably already cleared
 	fl := Pushcli()
 
-	splock(pmsglock)
+	Splock(pmsglock)
 	_pmsg("cpu")
 	_pnum(uintptr(cpunum))
 	_pmsg("joined\n")
-	spunlock(pmsglock)
+	Spunlock(pmsglock)
 
 	if cpunum >= uint(MAXCPUS) {
 		pancake("nice computer!", uintptr(cpunum))
@@ -1702,7 +1701,7 @@ const (
 	IRQ_BASE	= 32
 )
 
-var threadlock = &spinlock_t{}
+var threadlock = &Spinlock_t{}
 
 // maximum # of runtime "OS" threads
 const maxthreads = 64
@@ -1858,13 +1857,13 @@ func trap(tf *[TFSIZE]uintptr) {
 		// does not return
 		tlb_shootdown()
 	} else if trapno == TRAP_TIMER {
-		splock(threadlock)
+		Splock(threadlock)
 		if ct != nil {
 			if ct.status == ST_WILLSLEEP {
 				ct.status = ST_SLEEPING
 				// XXX set IF, unlock
 				ct.tf[TF_RFLAGS] |= TF_FL_IF
-				spunlock(futexlock)
+				Spunlock(futexlock)
 			} else {
 				ct.status = ST_RUNNABLE
 			}
@@ -1990,12 +1989,12 @@ func yieldy() {
 		t := &threads[idx]
 		if t.status == ST_RUNNABLE {
 			t.status = ST_RUNNING
-			spunlock(threadlock)
+			Spunlock(threadlock)
 			sched_run(t)
 		}
 	}
 	cpu.mythread = nil
-	spunlock(threadlock)
+	Spunlock(threadlock)
 	sched_halt()
 }
 
@@ -2012,13 +2011,13 @@ func Trapsched() {
 // are disabled
  //go:nosplit
  func Trapwake() {
-	splock(tlock);
+	Splock(tlock);
 	_nints++
 	// only flag the Ps if a handler isn't currently active
 	if trapst == IDLE {
 		_ptrap = true
 	}
-	spunlock(tlock)
+	Spunlock(tlock)
 }
 // trap handling goroutines first call. it is not an error if there are no
 // interrupts when this is called.
@@ -2030,7 +2029,7 @@ func trapsched_m(gp *g) {
 	_trapsched(gp, false)
 }
 
-var tlock = &spinlock_t{}
+var tlock = &Spinlock_t{}
 
 var _initted bool
 var _trapsleeper *g
@@ -2041,7 +2040,7 @@ var _nints int
 // to the scheduler stack where preemptions are ignored.
 func _trapsched(gp *g, firsttime bool) {
 	fl := Pushcli()
-	splock(tlock)
+	Splock(tlock)
 
 	if firsttime {
 		if _initted {
@@ -2074,7 +2073,7 @@ func _trapsched(gp *g, firsttime bool) {
 		_trapsleeper = gp
 	}
 
-	spunlock(tlock)
+	Spunlock(tlock)
 	Popcli(fl)
 
 	schedule()
@@ -2113,7 +2112,7 @@ func trapcheck(pp *p) {
 		return
 	}
 	fl := Pushcli()
-	splock(tlock)
+	Splock(tlock)
 
 	var trapgp *g
 	if !_ptrap {
@@ -2138,7 +2137,7 @@ func trapcheck(pp *p) {
 	trapgp = _trapsleeper
 	_trapsleeper = nil
 
-	spunlock(tlock)
+	Spunlock(tlock)
 	Popcli(fl)
 
 	casgstatus(trapgp, _Gwaiting, _Grunnable)
@@ -2147,7 +2146,7 @@ func trapcheck(pp *p) {
 	runqput(pp, trapgp, true)
 	return
 out:
-	spunlock(tlock)
+	Spunlock(tlock)
 	Popcli(fl)
 	return
 }
@@ -2336,7 +2335,7 @@ func prot_none(v, sz uintptr) {
 	}
 }
 
-var maplock = &spinlock_t{}
+var maplock = &Spinlock_t{}
 
 // this flag makes hack_mmap panic if a new pml4 entry is ever added to the
 // kernel's pmap. we want to make sure all kernel mappings added after bootup
@@ -2351,7 +2350,7 @@ func Pml4freeze() {
 func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
     fd int32, offset int32) uintptr {
 	fl := Pushcli()
-	splock(maplock)
+	Splock(maplock)
 
 	MAP_ANON := uintptr(0x20)
 	MAP_PRIVATE := uintptr(0x2)
@@ -2411,14 +2410,14 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 	}
 	ret = va
 out:
-	spunlock(maplock)
+	Spunlock(maplock)
 	Popcli(fl)
 	return ret
 }
 
 func hack_munmap(v, _sz uintptr) {
 	fl := Pushcli()
-	splock(maplock)
+	Splock(maplock)
 	sz := pgroundup(_sz)
 	cantuse := uintptr(0xf0)
 	for i := uintptr(0); i < sz; i += PGSIZE {
@@ -2436,7 +2435,7 @@ func hack_munmap(v, _sz uintptr) {
 		}
 	}
 	pmsg("POOF\n")
-	spunlock(maplock)
+	Spunlock(maplock)
 	Popcli(fl)
 }
 
@@ -2472,7 +2471,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	cloneaddr := **(**uintptr)(unsafe.Pointer(&dur))
 
 	fl := Pushcli()
-	splock(threadlock)
+	Splock(threadlock)
 
 	ti := thread_avail()
 	// provide fn as arg to clone_wrap
@@ -2501,7 +2500,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 
 	mt.fx = fxinit
 
-	spunlock(threadlock)
+	Spunlock(threadlock)
 	Popcli(fl)
 }
 
@@ -2553,13 +2552,13 @@ func hack_write(fd int, bufn uintptr, sz uint32) int64 {
 		pancake("unexpected fd", uintptr(fd))
 	}
 	fl := Pushcli()
-	splock(pmsglock)
+	Splock(pmsglock)
 	c := uintptr(sz)
 	for i := uintptr(0); i < c; i++ {
 		p := (*int8)(unsafe.Pointer(bufn + i))
 		putch(*p)
 	}
-	spunlock(pmsglock)
+	Spunlock(pmsglock)
 	Popcli(fl)
 	return int64(sz)
 }
@@ -2597,7 +2596,7 @@ func hack_syscall(trap, a1, a2, a3 int64) (int64, int64, int64) {
 	return 0, 0, -1
 }
 
-var futexlock = &spinlock_t{}
+var futexlock = &Spinlock_t{}
 
 // XXX not sure why stack splitting prologue is not ok here
 //go:nosplit
@@ -2611,7 +2610,7 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 	switch op {
 	case FUTEX_WAIT:
 		Cli()
-		splock(futexlock)
+		Splock(futexlock)
 		dosleep := *uaddr == val
 		if dosleep {
 			ct := Gscpu().mythread
@@ -2630,7 +2629,7 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 			ret = Gscpu().mythread.sleepret
 			Sti()
 		} else {
-			spunlock(futexlock)
+			Spunlock(futexlock)
 			Sti()
 			eagain := -11
 			ret = eagain
@@ -2638,8 +2637,8 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 	case FUTEX_WAKE:
 		woke := 0
 		Cli()
-		splock(futexlock)
-		splock(threadlock)
+		Splock(futexlock)
+		Splock(threadlock)
 		for i := 0; i < maxthreads && val > 0; i++ {
 			t := &threads[i]
 			st := t.status
@@ -2652,8 +2651,8 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 				woke++
 			}
 		}
-		spunlock(threadlock)
-		spunlock(futexlock)
+		Spunlock(threadlock)
+		Spunlock(futexlock)
 		Sti()
 		ret = woke
 	default:

commit 0dd6398408ce9631b6ebf53beead93b3f98d3754
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Aug 30 14:01:36 2016 -0400

    put all IO APIC interrupt vectors in IDT

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index dcb458bec1..a6155ce1e5 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -944,6 +944,14 @@ func Xirq12()
 func Xirq13()
 func Xirq14()
 func Xirq15()
+func Xirq16()
+func Xirq17()
+func Xirq18()
+func Xirq19()
+func Xirq20()
+func Xirq21()
+func Xirq22()
+func Xirq23()
 
 type idte_t struct {
 	baselow	uint16
@@ -1020,6 +1028,14 @@ func int_setup() {
 	int_set(irqbase+13,  Xirq13,  1)
 	int_set(irqbase+14,  Xirq14,  1)
 	int_set(irqbase+15,  Xirq15,  1)
+	int_set(irqbase+16,  Xirq16,  1)
+	int_set(irqbase+17,  Xirq17,  1)
+	int_set(irqbase+18,  Xirq18,  1)
+	int_set(irqbase+19,  Xirq19,  1)
+	int_set(irqbase+20,  Xirq20,  1)
+	int_set(irqbase+21,  Xirq21,  1)
+	int_set(irqbase+22,  Xirq22,  1)
+	int_set(irqbase+23,  Xirq23,  1)
 
 	int_set(48,  Xspur,    1)
 	// no longer used

commit 71ab9fa312f8266379dbb358b9ee9303cde7bd6b
Author: Josh Bleecher Snyder <josharian@gmail.com>
Date:   Mon Jul 11 16:05:57 2016 -0700

    all: fix assembly vet issues
    
    Add missing function prototypes.
    Fix function prototypes.
    Use FP references instead of SP references.
    Fix variable names.
    Update comments.
    Clean up whitespace. (Not for vet.)
    
    All fairly minor fixes to make vet happy.
    
    Updates #11041
    
    Change-Id: Ifab2cdf235ff61cdc226ab1d84b8467b5ac9446c
    Reviewed-on: https://go-review.googlesource.com/27713
    Run-TryBot: Josh Bleecher Snyder <josharian@gmail.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0131c6687d..796e05a69e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -133,7 +133,7 @@ const (
 )
 
 //go:noescape
-func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
+func clone(flags int32, stk, mp, gp, fn unsafe.Pointer) int32
 
 // May run with m.p==nil, so write barriers are not allowed.
 //go:nowritebarrier
@@ -360,7 +360,7 @@ func memlimit() uintptr {
 //#endif
 
 func sigreturn()
-func sigtramp()
+func sigtramp(sig uint32, info *siginfo, ctx unsafe.Pointer)
 func cgoSigtramp()
 
 //go:noescape

commit fa897643a18d71a62bade50f80171f5e58449f5a
Author: Michael Munday <munday@ca.ibm.com>
Date:   Sat Aug 20 18:36:27 2016 +0100

    runtime: remove unnecessary calls to memclr
    
    Go will have already cleared the structs (the original C wouldn't
    have).
    
    Change-Id: I4a5a0cfd73953181affc158d188aae2ce281bb33
    Reviewed-on: https://go-review.googlesource.com/27435
    Run-TryBot: Michael Munday <munday@ca.ibm.com>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 542f214a42..0131c6687d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -388,7 +388,6 @@ func osyield()
 //go:nowritebarrierrec
 func setsig(i int32, fn uintptr, restart bool) {
 	var sa sigactiont
-	memclr(unsafe.Pointer(&sa), unsafe.Sizeof(sa))
 	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK | _SA_RESTORER
 	if restart {
 		sa.sa_flags |= _SA_RESTART
@@ -431,8 +430,6 @@ func setsigstack(i int32) {
 //go:nowritebarrierrec
 func getsig(i int32) uintptr {
 	var sa sigactiont
-
-	memclr(unsafe.Pointer(&sa), unsafe.Sizeof(sa))
 	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
 		throw("rt_sigaction read failure")
 	}

commit 2dceafbbce01f88e98e5e6d3f397cf4bc2c2df18
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Aug 5 19:58:25 2016 -0400

    bugfix: correct file offset when middle of mapped file is unmapped
    
    and cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b496e3ece6..dcb458bec1 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -194,7 +194,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	var dopdec bool
 	if cpu.shadowcr3 != p_pmap {
 		sh := cpu.shadowcr3
-		dopdec = sh != 0 && sh != P_kpmap
+		dopdec = sh != 0
 		opmap = cpu.shadowcr3
 		dur := (*uint32)(unsafe.Pointer(pmap_ref))
 		atomic.Xadd(dur, 1)

commit c7ae41e5770b2258074eee68a6a3c4d0d71a251f
Author: Ian Lance Taylor <iant@golang.org>
Date:   Tue Jun 28 17:06:59 2016 -0700

    runtime: better error message for newosproc failure
    
    If creating a new thread fails with EAGAIN, point the user at ulimit.
    
    Fixes #15476.
    
    Change-Id: Ib36519614b5c72776ea7f218a0c62df1dd91a8ea
    Reviewed-on: https://go-review.googlesource.com/24570
    Run-TryBot: Ian Lance Taylor <iant@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 35b57d8a23..542f214a42 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -154,6 +154,9 @@ func newosproc(mp *m, stk unsafe.Pointer) {
 
 	if ret < 0 {
 		print("runtime: failed to create new OS thread (have ", mcount(), " already; errno=", -ret, ")\n")
+		if ret == -_EAGAIN {
+			println("runtime: may need to increase max user processes (ulimit -u)")
+		}
 		throw("newosproc")
 	}
 }

commit b9b33f8332a67ffd39872809f66394318bafd17c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Jun 11 18:43:36 2016 -0400

    remove some old crap

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c5ff2fadbd..b496e3ece6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -105,8 +105,6 @@ type cpu_t struct {
 	sysrsp		uintptr
 	shadowcr3	uintptr
 	shadowfs	uintptr
-	//pmap		*[512]int
-	//pms		[]*[512]int
 	//pid		uintptr
 }
 
@@ -182,8 +180,7 @@ func Gscpu() *cpu_t {
 }
 
 func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
-    p_pmap uintptr, pms []*[512]int, fastret bool,
-    pmap_ref *int32) (int, int, uintptr, bool) {
+    p_pmap uintptr, fastret bool, pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
@@ -204,15 +201,6 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 		Lcr3(p_pmap)
 		cpu.shadowcr3 = p_pmap
 	}
-	// set shadow pointers for user pmap so it isn't free'd out from under
-	// us if the process terminates soon.
-	//cpu.pmap = pmap
-	//cpu.pms = pms
-	//cpu.pid = uintptr(pid)
-	// avoid write barriers since we are uninterruptible. the caller must
-	// also have these references anyway, so skipping them is ok.
-	//*(*uintptr)(unsafe.Pointer(&cpu.pmap)) = uintptr(unsafe.Pointer(pmap))
-	//*(*[3]uintptr)(unsafe.Pointer(&cpu.pms)) = *(*[3]uintptr)(unsafe.Pointer(&pms))
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
@@ -242,14 +230,6 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	return intno, aux, opmap, dopdec
 }
 
-// caller must have interrupts cleared
-//go:nosplit
-func shadow_clear() {
-	//cpu := Gscpu()
-	//cpu.pmap = nil
-	//cpu.pms = nil
-}
-
 type nmiprof_t struct {
 	buf		[]uintptr
 	bufidx		uint64
@@ -1813,9 +1793,6 @@ func trap(tf *[TFSIZE]uintptr) {
 		}
 	}
 
-	// clear shadow pointers to user pmap
-	shadow_clear()
-
 	// don't add code before FPU context saving unless you've thought very
 	// carefully! it is easy to accidentally and silently corrupt FPU state
 	// (ie calling memmove indirectly by assignment of large datatypes)

commit 0f2f9cba43482fe3cb1f115da80d22a81cba0006
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jun 10 23:54:55 2016 -0400

    TLB shootdown bugfix
    
    thread_t.pmap is always the kernel pmap and has been for a while now.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 68ef582e33..c5ff2fadbd 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1646,8 +1646,7 @@ func Tlbwait(gen uint64) {
 //go:nosplit
 func tlb_shootdown() {
 	ct := Gscpu().mythread
-	// XXX XXX XXX this is wrong; should be Gscpu().shadowcr3
-	if ct != nil && ct.p_pmap == tlbshoot_pmap {
+	if ct != nil && Rcr3() == tlbshoot_pmap {
 		// lazy way for now
 		Lcr3(Rcr3())
 		//start := tlbshoot_pg

commit eebdc9f18d967944e8f9a7841020f5e71c8c6852
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jun 10 23:40:41 2016 -0400

    fix pmap freeing
    
    increase page map reference count when they are loaded/unloaded on each CPU
    too, that way we can safely free them.
    
    it's a real shame that allocating the page maps via the GC is prohibitively
    expensive for fork-intensive benchmarks; it was really convenient.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f71e7378b2..68ef582e33 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -182,7 +182,8 @@ func Gscpu() *cpu_t {
 }
 
 func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
-    p_pmap uintptr, pms []*[512]int, fastret bool) (int, int) {
+    p_pmap uintptr, pms []*[512]int, fastret bool,
+    pmap_ref *int32) (int, int, uintptr, bool) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
@@ -192,7 +193,14 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	cpu := _Gscpu()
 	ct := cpu.mythread
 
+	var opmap uintptr
+	var dopdec bool
 	if cpu.shadowcr3 != p_pmap {
+		sh := cpu.shadowcr3
+		dopdec = sh != 0 && sh != P_kpmap
+		opmap = cpu.shadowcr3
+		dur := (*uint32)(unsafe.Pointer(pmap_ref))
+		atomic.Xadd(dur, 1)
 		Lcr3(p_pmap)
 		cpu.shadowcr3 = p_pmap
 	}
@@ -231,7 +239,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	ct.user.fxbuf = nil
 	Popcli(fl)
 	//exitsyscall(0)
-	return intno, aux
+	return intno, aux, opmap, dopdec
 }
 
 // caller must have interrupts cleared
@@ -1068,7 +1076,7 @@ const (
 )
 
 // physical address of kernel's pmap, given to us by bootloader
-var p_kpmap uintptr
+var P_kpmap uintptr
 
 //go:nosplit
 func pml4x(va uintptr) uintptr {
@@ -1537,7 +1545,7 @@ func proc_setup() {
 	fpuinit(true)
 	// initialize the first thread: us
 	threads[0].status = ST_RUNNING
-	threads[0].p_pmap = p_kpmap
+	threads[0].p_pmap = P_kpmap
 
 	// initialize GS pointers
 	for i := range cpus {
@@ -1638,10 +1646,10 @@ func Tlbwait(gen uint64) {
 //go:nosplit
 func tlb_shootdown() {
 	ct := Gscpu().mythread
+	// XXX XXX XXX this is wrong; should be Gscpu().shadowcr3
 	if ct != nil && ct.p_pmap == tlbshoot_pmap {
-		// the TLB was already invalidated since trap() currently
-		// switches to kernel pmap on any exception/interrupt other
-		// than NMI.
+		// lazy way for now
+		Lcr3(Rcr3())
 		//start := tlbshoot_pg
 		//end := tlbshoot_pg + tlbshoot_count * PGSIZE
 		//for ; start < end; start += PGSIZE {
@@ -1781,16 +1789,14 @@ func kernel_fault(tf *[TFSIZE]uintptr) {
 func trap(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 
-	// XXX doesn't this clobber SSE regs?
+	// XXX XXX XXX XXX make sure this can't clobber SSE regs.
 	if trapno == TRAP_NMI {
 		perfgather(tf)
 		perfmask()
 		_trapret(tf)
 	}
 
-	Lcr3(p_kpmap)
 	cpu := Gscpu()
-	cpu.shadowcr3 = p_kpmap
 
 	// CPU exceptions in kernel mode are fatal errors
 	if trapno < TRAP_TIMER && (tf[TF_CS] & 3) == 0 {
@@ -1943,7 +1949,10 @@ func sched_run(t *thread_t) {
 	//Gscpu().mythread = t
 	*(*uintptr)(unsafe.Pointer(&Gscpu().mythread)) = uintptr(unsafe.Pointer(t))
 	fxrstor(&t.fx)
-	trapret(&t.tf, t.p_pmap)
+	// flush the TLB, otherwise the cpu may use a TLB entry for a page that
+	// has since been unmapped
+	Lcr3(Rcr3())
+	_trapret(&t.tf)
 }
 
 //go:nosplit
@@ -2496,7 +2505,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))
 	mp.procid = uint64(ti)
 	mt.status = ST_RUNNABLE
-	mt.p_pmap = p_kpmap
+	mt.p_pmap = P_kpmap
 
 	mt.fx = fxinit
 
@@ -2720,10 +2729,6 @@ func Pnum(n int) {
 	pnum(uintptr(n))
 }
 
-func Kpmap_p() uintptr {
-	return p_kpmap
-}
-
 func GCmarktime() int {
 	return int(work.totaltime)
 }

commit e9ca64775997f65cbff166988fdfdcb38df3d51e
Merge: 1de551172d c50ee39286
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon May 9 20:45:30 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit c50ee39286fa603a3cfae882d09116ae931fc796
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon May 9 20:44:16 2016 -0400

    wrap long serial console lines

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 140989e864..8f7e9af3b7 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -466,16 +466,30 @@ func sc_put_(c int8) {
 	Outb(com1, uint8(c))
 }
 
+var _scx int
+
 //go:nosplit
 func sc_put(c int8) {
 	if c == '\n' {
 		sc_put_('\r')
-	}
-	sc_put_(c)
-	if c == '\b' {
-		// clear the previous character
-		sc_put_(' ')
-		sc_put_('\b')
+		sc_put_(c)
+		_scx = 0
+	} else if _scx == 80 {
+		sc_put_(c)
+		sc_put_('\r')
+		sc_put_('\n')
+		_scx = 0
+	} else if c == '\b' {
+		if _scx > 0 {
+			// clear the previous character
+			sc_put_('\b')
+			sc_put_(' ')
+			sc_put_('\b')
+			_scx--
+		}
+	} else {
+		sc_put_(c)
+		_scx++
 	}
 }
 

commit a96d19335f1c7d45fd6bf8a1ab4a76fce1175f97
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon May 9 20:43:39 2016 -0400

    use LBR to precisely detect branch mispredicts

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 760e707402..140989e864 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -300,35 +300,41 @@ func dumrand(low, high uint) uint {
 
 //go:nosplit
 func _consumelbr() {
-	lastbranch_tos := 0x1c9
+	//lastbranch_tos := 0x1c9
 	lastbranch_0_from_ip := 0x680
-	lastbranch_0_to_ip := 0x6c0
+	//lastbranch_0_to_ip := 0x6c0
 
-	last := Rdmsr(lastbranch_tos) & 0xf
+	// top of stack
+	//last := Rdmsr(lastbranch_tos) & 0xf
 	// XXX stacklen
-	l := 16 * 2
-	l++
+	lbrlen := 16
+	//l := 16 * 2
+	l := 0
+	for i := 0; i < lbrlen; i++ {
+		from := uintptr(Rdmsr(lastbranch_0_from_ip + i))
+		mispred := uintptr(1 << 63)
+		if from & mispred != 0 {
+			l++
+		}
+	}
+	if int(nmiprof.bufidx) + l >= len(nmiprof.buf) {
+		return
+	}
 	idx := int(atomic.Xadd64(&nmiprof.bufidx, int64(l)))
 	idx -= l
 	for i := 0; i < 16; i++ {
-		cur := (last - i)
-		if cur < 0 {
-			cur += 16
+		from := uintptr(Rdmsr(lastbranch_0_from_ip + i))
+		Wrmsr(lastbranch_0_from_ip + i, 0)
+		mispred := uintptr(1 << 63)
+		if from & mispred == 0 {
+			continue
 		}
-		from := uintptr(Rdmsr(lastbranch_0_from_ip + cur))
-		to := uintptr(Rdmsr(lastbranch_0_to_ip + cur))
-		Wrmsr(lastbranch_0_from_ip + cur, 0)
-		Wrmsr(lastbranch_0_to_ip + cur, 0)
-		if idx + 2*i + 1 >= len(nmiprof.buf) {
+		if idx >= len(nmiprof.buf) {
 			Cpuprint('!', 1)
 			break
 		}
-		nmiprof.buf[idx+2*i] = from
-		nmiprof.buf[idx+2*i+1] = to
-	}
-	idx += l - 1
-	if idx < len(nmiprof.buf) {
-		nmiprof.buf[idx] = ^uintptr(0)
+		nmiprof.buf[idx] = from
+		idx++
 	}
 }
 
@@ -343,11 +349,16 @@ func _lbrreset(en bool) {
 	// enable last branch records. filter every branch but to direct
 	// calls/jmps (sandybridge onward has better filtering)
 	lbr_select := 0x1c8
-	jcc := 1 << 2
-	indjmp := 1 << 6
-	//reljmp := 1 << 7
-	farbr := 1 << 8
-	dv := jcc | farbr | indjmp
+	nocplgt0 := 1 << 1
+	//nojcc := 1 << 2
+	norelcall := 1 << 3
+	noindcall := 1 << 4
+	nonearret := 1 << 5
+	noindjmp := 1 << 6
+	noreljmp := 1 << 7
+	nofarbr := 1 << 8
+	dv := nocplgt0 | norelcall | noindcall | nonearret | noindjmp |
+	    noreljmp | nofarbr
 	Wrmsr(lbr_select, dv)
 
 	freeze_lbrs_on_pmi := 1 << 11
@@ -1752,6 +1763,7 @@ func kernel_fault(tf *[TFSIZE]uintptr) {
 func trap(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
 
+	// XXX doesn't this clobber SSE regs?
 	if trapno == TRAP_NMI {
 		perfgather(tf)
 		perfmask()

commit 7ffd55248c85c4ecce8925c2c2a07fca6f02e60f
Merge: 28e260d131 002ce084b0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Apr 27 22:16:25 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit 002ce084b0bacf92d0ab5dc8043191969042d7d3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Apr 27 22:15:47 2016 -0400

    gc cpu time counting and alloc benchmark hacks

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 527011aaf7..760e707402 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2694,10 +2694,18 @@ func Kpmap_p() uintptr {
 	return p_kpmap
 }
 
-func GCworktime() int {
+func GCmarktime() int {
 	return int(work.totaltime)
 }
 
+func GCbgsweeptime() int {
+	return int(work.bgsweeptime)
+}
+
+func GCwbenabledtime() int {
+	return int(wbenabledtime)
+}
+
 func Heapsz() int {
 	return int(memstats.next_gc)
 }

commit 3b0208af22172fcb286ddb312106b665c90b6068
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Apr 24 09:19:40 2016 -0400

    don't need pmap shadow pointers for refpgs

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 56db72bffc..88b4244602 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -105,8 +105,8 @@ type cpu_t struct {
 	sysrsp		uintptr
 	shadowcr3	uintptr
 	shadowfs	uintptr
-	pmap		*[512]int
-	pms		[]*[512]int
+	//pmap		*[512]int
+	//pms		[]*[512]int
 	//pid		uintptr
 }
 
@@ -203,8 +203,8 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	//cpu.pid = uintptr(pid)
 	// avoid write barriers since we are uninterruptible. the caller must
 	// also have these references anyway, so skipping them is ok.
-	*(*uintptr)(unsafe.Pointer(&cpu.pmap)) = uintptr(unsafe.Pointer(pmap))
-	*(*[3]uintptr)(unsafe.Pointer(&cpu.pms)) = *(*[3]uintptr)(unsafe.Pointer(&pms))
+	//*(*uintptr)(unsafe.Pointer(&cpu.pmap)) = uintptr(unsafe.Pointer(pmap))
+	//*(*[3]uintptr)(unsafe.Pointer(&cpu.pms)) = *(*[3]uintptr)(unsafe.Pointer(&pms))
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
@@ -237,9 +237,9 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 // caller must have interrupts cleared
 //go:nosplit
 func shadow_clear() {
-	cpu := Gscpu()
-	cpu.pmap = nil
-	cpu.pms = nil
+	//cpu := Gscpu()
+	//cpu.pmap = nil
+	//cpu.pms = nil
 }
 
 type nmiprof_t struct {

commit da19be2a966e52d34427588c1bff9ffbb72f3773
Merge: 21005eb3c2 4354980481
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 22 23:47:31 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit 4354980481920ac314e3e3260cd4cce0665df681
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 22 23:17:02 2016 -0400

    reduce system call overhead
    
    avoid all {rd,wr}msrs during syscalls. there are several necesary steps to
    achieve this:
    
    1) rewrite the kernel binary to use %gs instead of %fs (which is defined as the
    TLS indirect pointer in the amd64 ELF TLS ABI), that way we don't need to
    context switch %fs on every syscall. i tried modifying the go runtime/compiler
    to simply use %gs, but the runtime still panic'ed and the build system requires
    running go binaries natively on the host platform. thus i was unable to build
    my modified runtime fully. instead, i wrote a small script to rewrite my kernel
    binary to use %gs while the runtime build system is left unchanged.
    
    2) since the kernel now uses %gs for TLS, store the per-CPU indirect pointer in
    IA32_KERNEL_GS_BASE MSR. now we simply use swapgs to load the per-CPU indirect
    pointer.
    
    3) expand trap frame to save/restore %gs since it is per thread now. context
    switches never save/restore %fs now; %fs is explicitly loaded in Userrun(). in
    order to avoid a rdmsr to detect whether %fs is correct, we shadow the %fs
    register in a per-CPU variable. thus checking %fs is a memory load instead of
    rdmsr.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 59e2c47542..527011aaf7 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -49,6 +49,7 @@ func fxrstor(*[FXREGS]uintptr)
 func fxsave(*[FXREGS]uintptr)
 func _Gscpu() *cpu_t
 func gs_null()
+func gs_set(*cpu_t)
 func htpause()
 func invlpg(uintptr)
 func Inb(uint16) uint
@@ -83,6 +84,7 @@ func _sysentry()
 func _trapret(*[TFSIZE]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
+func _userret()
 func _Userrun(*[TFSIZE]int, bool) (int, int)
 func Wrmsr(int, int)
 
@@ -101,6 +103,8 @@ type cpu_t struct {
 	rsp		uintptr
 	num		uint
 	sysrsp		uintptr
+	shadowcr3	uintptr
+	shadowfs	uintptr
 	pmap		*[512]int
 	pms		[]*[512]int
 	//pid		uintptr
@@ -128,7 +132,7 @@ type prof_t struct {
 // near front
 type thread_t struct {
 	tf		[TFSIZE]uintptr
-	_pad		int
+	//_pad		int
 	fx		[FXREGS]uintptr
 	user		tuser_t
 	sigtf		[TFSIZE]uintptr
@@ -142,23 +146,24 @@ type thread_t struct {
 	sleepret	int
 	futaddr		uintptr
 	p_pmap		uintptr
-	//_pad2		int
+	_pad2		int
 }
 
 // XXX fix these misleading names
 const(
-  TFSIZE       = 23
+  TFSIZE       = 24
   FXREGS       = 64
-  TFREGS       = 16
-  TF_FSBASE    = 0
-  TF_R8        = 8
-  TF_RBP       = 9
-  TF_RSI       = 10
-  TF_RDI       = 11
-  TF_RDX       = 12
-  TF_RCX       = 13
-  TF_RBX       = 14
-  TF_RAX       = 15
+  TFREGS       = 17
+  TF_GSBASE    = 0
+  TF_FSBASE    = 1
+  TF_R8        = 9
+  TF_RBP       = 10
+  TF_RSI       = 11
+  TF_RDI       = 12
+  TF_RDX       = 13
+  TF_RCX       = 14
+  TF_RBX       = 15
+  TF_RAX       = 16
   TF_TRAPNO    = TFREGS
   TF_RIP       = TFREGS + 2
   TF_CS        = TFREGS + 3
@@ -184,11 +189,12 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	// while other cpus execute user programs.
 	//entersyscall(0)
 	fl := Pushcli()
-	cpu := Gscpu()
+	cpu := _Gscpu()
 	ct := cpu.mythread
 
-	if Rcr3() != p_pmap {
+	if cpu.shadowcr3 != p_pmap {
 		Lcr3(p_pmap)
+		cpu.shadowcr3 = p_pmap
 	}
 	// set shadow pointers for user pmap so it isn't free'd out from under
 	// us if the process terminates soon.
@@ -202,14 +208,11 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
-	// XXX to avoid {rd,wr}msr on int/syscall context switch, we can move
-	// each cpu's cpu_t into it's M's tls memory, thus freeing up %gs. %gs
-	// would then be storage for %fs of the opposite privilege level (%gs
-	// holds kernels %fs in usermode, user's %fs is kernel mode). then we
-	// can use swapgs, like other kernels.
-	ia32_fs_base := 0xc0000100
-	kfsbase := Rdmsr(ia32_fs_base)
-	Wrmsr(ia32_fs_base, tf[TF_FSBASE])
+	if cpu.shadowfs != uintptr(tf[TF_FSBASE]) {
+		ia32_fs_base := 0xc0000100
+		Wrmsr(ia32_fs_base, tf[TF_FSBASE])
+		cpu.shadowfs = uintptr(tf[TF_FSBASE])
+	}
 
 	// we only save/restore SSE registers on cpu exception/interrupt, not
 	// during syscall exit/return. this is OK since sys5ABI defines the SSE
@@ -224,7 +227,6 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 
 	intno, aux := _Userrun(tf, fastret)
 
-	Wrmsr(ia32_fs_base, kfsbase)
 	ct.user.tf = nil
 	ct.user.fxbuf = nil
 	Popcli(fl)
@@ -880,8 +882,9 @@ func seg_setup() {
 	// descriptor cache, the writes to the MSRs are thrown out (presumably
 	// because the caches are thought to be invalid).
 	fs_null()
-	ia32_fs_base := 0xc0000100
-	Wrmsr(ia32_fs_base, tlsaddr)
+	gs_null()
+	ia32_gs_base := 0xc0000101
+	Wrmsr(ia32_gs_base, tlsaddr)
 }
 
 // interrupt entries, defined in runtime/asm_amd64.s
@@ -1551,6 +1554,7 @@ func Ap_setup(cpunum uint) {
 		pancake("cpu id conflict", uintptr(mycpu.num))
 	}
 	fs_null()
+	gs_null()
 	gs_set(mycpu)
 	Gscpu().num = cpunum
 	Gscpu().mythread = nil
@@ -1558,18 +1562,6 @@ func Ap_setup(cpunum uint) {
 	Popcli(fl)
 }
 
-//go:nosplit
-func gs_set(c *cpu_t) {
-	// we must set fs/gs at least once before we use the MSRs to change
-	// their base address. the MSRs write directly to hidden segment
-	// descriptor cache, and if we don't explicitly fill the segment
-	// descriptor cache, the writes to the MSRs are thrown out (presumably
-	// because the caches are thought to be invalid).
-	gs_null()
-	ia32_gs_base := 0xc0000101
-	Wrmsr(ia32_gs_base, int(uintptr(unsafe.Pointer(c))))
-}
-
 //go:nosplit
 func sysc_setup(myrsp uintptr) {
 	// lowest 2 bits are ignored for sysenter, but used for sysexit
@@ -1767,13 +1759,15 @@ func trap(tf *[TFSIZE]uintptr) {
 	}
 
 	Lcr3(p_kpmap)
+	cpu := Gscpu()
+	cpu.shadowcr3 = p_kpmap
 
 	// CPU exceptions in kernel mode are fatal errors
 	if trapno < TRAP_TIMER && (tf[TF_CS] & 3) == 0 {
 		kernel_fault(tf)
 	}
 
-	ct := Gscpu().mythread
+	ct := cpu.mythread
 
 	if rflags() & TF_FL_IF != 0 {
 		pancake("ints enabled in trap", 0)
@@ -1802,7 +1796,7 @@ func trap(tf *[TFSIZE]uintptr) {
 			utf := ct.user.tf
 			*utf = *tf
 			ct.tf[TF_RIP] = _userintaddr
-			ct.tf[TF_RSP] = Gscpu().sysrsp
+			ct.tf[TF_RSP] = cpu.sysrsp
 			ct.tf[TF_RAX] = trapno
 			ct.tf[TF_RBX] = Rcr2()
 			// XXXPANIC
@@ -1849,7 +1843,7 @@ func trap(tf *[TFSIZE]uintptr) {
 		}
 		if !yielding {
 			lap_eoi()
-			if Gscpu().num == 0 {
+			if cpu.num == 0 {
 				wakeup()
 				proftick()
 			}
@@ -2463,7 +2457,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	mt.tf[TF_RSP] = rsp
 	mt.tf[TF_RIP] = cloneaddr
 	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
-	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 8
+	mt.tf[TF_GSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 8
 
 	// avoid write barrier for mp here since we have interrupts clear. Ms
 	// are always reachable from allm anyway. see comments in runtime2.go

commit 21005eb3c27132902387f94dcf234d5706657abb
Merge: c34f2c3813 d3b976c147
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 19 19:50:03 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit d3b976c14798556f5973821b59899b39dcb0a1a8
Merge: 9891544c69 e6305e3425
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 19 19:46:47 2016 -0400

    Merge branch 'master' into g1.6
    
    Conflicts:
            src/runtime/asm_amd64.s
            src/runtime/os_linux.go

commit e6305e34251d6b647ecf90efa038fa2cdec27756
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 19 19:40:27 2016 -0400

    use per-cpu GS to track sysenter stack
    
    that way we avoid using IA32_SYSENTER_ESP which requires wrmsr.
    
    significantly reduces syscall overhead.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a61face674..744d650c93 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -62,7 +62,7 @@ func _sysentry()
 func _trapret(*[TFSIZE]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
-func _Userrun(*[24]int, bool) (int, int)
+func _Userrun(*[TFSIZE]int, bool) (int, int)
 func Wrmsr(int, int)
 
 // proc.c
@@ -84,6 +84,7 @@ type cpu_t struct {
 	mythread	*thread_t
 	rsp		uintptr
 	num		uint
+	sysrsp		uintptr
 	pmap		*[512]int
 	pms		[]*[512]int
 	//pid		uintptr
@@ -111,6 +112,7 @@ type prof_t struct {
 // near front
 type thread_t struct {
 	tf		[TFSIZE]uintptr
+	_pad		int
 	fx		[FXREGS]uintptr
 	user		tuser_t
 	sigtf		[TFSIZE]uintptr
@@ -123,24 +125,23 @@ type thread_t struct {
 	sleepret	int
 	futaddr		uintptr
 	p_pmap		uintptr
-	//_pad		int
+	_pad2		int
 }
 
 // XXX fix these misleading names
 const(
-  TFSIZE       = 24
+  TFSIZE       = 23
   FXREGS       = 64
-  TFREGS       = 17
-  TF_SYSRSP    = 0
-  TF_FSBASE    = 1
-  TF_R8        = 9
-  TF_RBP       = 10
-  TF_RSI       = 11
-  TF_RDI       = 12
-  TF_RDX       = 13
-  TF_RCX       = 14
-  TF_RBX       = 15
-  TF_RAX       = 16
+  TFREGS       = 16
+  TF_FSBASE    = 0
+  TF_R8        = 8
+  TF_RBP       = 9
+  TF_RSI       = 10
+  TF_RDI       = 11
+  TF_RDX       = 12
+  TF_RCX       = 13
+  TF_RBX       = 14
+  TF_RAX       = 15
   TF_TRAPNO    = TFREGS
   TF_RIP       = TFREGS + 2
   TF_CS        = TFREGS + 3
@@ -1563,7 +1564,7 @@ func sysc_setup(myrsp uintptr) {
 	Wrmsr(sysenter_eip, int(sysentryaddr))
 
 	sysenter_esp := 0x175
-	Wrmsr(sysenter_esp, int(myrsp))
+	Wrmsr(sysenter_esp, 0)
 }
 
 var tlbshoot_wait uintptr
@@ -1780,7 +1781,7 @@ func trap(tf *[TFSIZE]uintptr) {
 			utf := ct.user.tf
 			*utf = *tf
 			ct.tf[TF_RIP] = _userintaddr
-			ct.tf[TF_RSP] = utf[TF_SYSRSP]
+			ct.tf[TF_RSP] = Gscpu().sysrsp
 			ct.tf[TF_RAX] = trapno
 			ct.tf[TF_RBX] = Rcr2()
 			// XXXPANIC

commit c34f2c381382bffe2a34f8359608be1ae494bd3c
Merge: a37151c9ef 9891544c69
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Apr 18 14:22:24 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit 9891544c69fd06881d11afcee6826af09093c1ff
Merge: c21321b93c 8c004279e4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Apr 18 14:20:32 2016 -0400

    Merge branch 'master' into g1.6
    
    Conflicts:
            src/runtime/os_linux.go

commit afaed0d78e3be4007eb52632ceed3576295ec47a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Apr 18 14:06:25 2016 -0400

    don't yield Ps before returning to userspace
    
    has considerable overhead and seems like a waste of time since the user process
    can, at worst, keep the P "busy" for one scheduling quantum.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d281d58ac6..a61face674 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -164,7 +164,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
 	// while other cpus execute user programs.
-	entersyscall()
+	//entersyscall()
 	fl := Pushcli()
 	cpu := Gscpu()
 	ct := cpu.mythread
@@ -201,7 +201,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	ct.user.tf = nil
 	ct.user.fxbuf = nil
 	Popcli(fl)
-	exitsyscall()
+	//exitsyscall()
 	return intno, aux
 }
 

commit 8ce844e88ed8c16bef7febea05b003b50bd0609e
Author: Austin Clements <austin@google.com>
Date:   Thu Apr 14 13:27:36 2016 -0400

    runtime: check kernel physical page size during init
    
    The runtime hard-codes an assumed physical page size. If this is
    smaller than the kernel's page size or not a multiple of it, sysUnused
    may incorrectly release more memory to the system than intended.
    
    Add a runtime startup check that the runtime's assumed physical page
    is compatible with the kernel's physical page size.
    
    For #9993.
    
    Change-Id: Ida9d07f93c00ca9a95dd55fc59bf0d8a607f6728
    Reviewed-on: https://go-review.googlesource.com/22064
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 4645f1c33d..35b57d8a23 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -178,6 +178,7 @@ var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
 
 const (
 	_AT_NULL   = 0  // End of vector
+	_AT_PAGESZ = 6  // System physical page size
 	_AT_RANDOM = 25 // introduced in 2.6.29
 )
 
@@ -201,7 +202,21 @@ func sysargs(argc int32, argv **byte) {
 			// The kernel provides a pointer to 16-bytes
 			// worth of random data.
 			startupRandomData = (*[16]byte)(unsafe.Pointer(val))[:]
+
+		case _AT_PAGESZ:
+			// Check that the true physical page size is
+			// compatible with the runtime's assumed
+			// physical page size.
+			if sys.PhysPageSize < val {
+				print("runtime: kernel page size (", val, ") is larger than runtime page size (", sys.PhysPageSize, ")\n")
+				exit(1)
+			}
+			if sys.PhysPageSize%val != 0 {
+				print("runtime: runtime page size (", sys.PhysPageSize, ") is not a multiple of kernel page size (", val, ")\n")
+				exit(1)
+			}
 		}
+
 		archauxv(tag, val)
 	}
 }

commit 90addd3d41852192ba697d33c9b1660988b82ed7
Author: Austin Clements <austin@google.com>
Date:   Thu Apr 14 12:32:28 2016 -0400

    runtime: common handling of _AT_RANDOM auxv
    
    The Linux kernel provides 16 bytes of random data via the auxv vector
    at startup. Currently we consume this separately on 386, amd64, arm,
    and arm64. Now that we have a common auxv parser, handle _AT_RANDOM in
    the common path.
    
    Change-Id: Ib69549a1d37e2d07a351cf0f44007bcd24f0d20d
    Reviewed-on: https://go-review.googlesource.com/22062
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index eeb30c7dd9..4645f1c33d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -177,7 +177,8 @@ var failallocatestack = []byte("runtime: failed to allocate stack for the new OS
 var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
 
 const (
-	_AT_NULL = 0 // End of vector
+	_AT_NULL   = 0  // End of vector
+	_AT_RANDOM = 25 // introduced in 2.6.29
 )
 
 func sysargs(argc int32, argv **byte) {
@@ -195,6 +196,12 @@ func sysargs(argc int32, argv **byte) {
 	auxv := (*[1 << 28]uintptr)(add(unsafe.Pointer(argv), uintptr(n)*sys.PtrSize))
 	for i := 0; auxv[i] != _AT_NULL; i += 2 {
 		tag, val := auxv[i], auxv[i+1]
+		switch tag {
+		case _AT_RANDOM:
+			// The kernel provides a pointer to 16-bytes
+			// worth of random data.
+			startupRandomData = (*[16]byte)(unsafe.Pointer(val))[:]
+		}
 		archauxv(tag, val)
 	}
 }

commit c955bb2040e601c474e547b8badbe44677c9fbdf
Author: Austin Clements <austin@google.com>
Date:   Thu Apr 14 12:12:45 2016 -0400

    runtime: common auxv parser
    
    Currently several different Linux architectures have separate copies
    of the auxv parser. Bring these all together into a single copy of the
    parser that calls out to a per-arch handler for each tag/value pair.
    This is in preparation for handling common auxv tags in one place.
    
    For #9993.
    
    Change-Id: Iceebc3afad6b4133b70fca7003561ae370445c10
    Reviewed-on: https://go-review.googlesource.com/22061
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: Brad Fitzpatrick <bradfitz@golang.org>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Michael Hudson-Doyle <michael.hudson@canonical.com>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 7d8cc7e5c4..eeb30c7dd9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -176,6 +176,29 @@ func newosproc0(stacksize uintptr, fn unsafe.Pointer) {
 var failallocatestack = []byte("runtime: failed to allocate stack for the new OS thread\n")
 var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
 
+const (
+	_AT_NULL = 0 // End of vector
+)
+
+func sysargs(argc int32, argv **byte) {
+	n := argc + 1
+
+	// skip over argv, envp to get to auxv
+	for argv_index(argv, n) != nil {
+		n++
+	}
+
+	// skip NULL separator
+	n++
+
+	// now argv+n is auxv
+	auxv := (*[1 << 28]uintptr)(add(unsafe.Pointer(argv), uintptr(n)*sys.PtrSize))
+	for i := 0; auxv[i] != _AT_NULL; i += 2 {
+		tag, val := auxv[i], auxv[i+1]
+		archauxv(tag, val)
+	}
+}
+
 func osinit() {
 	ncpu = getproccount()
 }

commit e1d7c27f14f93f157ada689f93dcae06a7e24aa0
Merge: 05a1ae9d23 6b499932b4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Apr 9 15:33:17 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs

commit 6b499932b4c6c7a02ff51806181c00fe56b7da00
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Apr 9 15:32:29 2016 -0400

    1.6 fixes

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f7a491be4d..0b24ae3057 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2702,3 +2702,11 @@ func Kpmap_p() uintptr {
 func GCworktime() int {
 	return int(work.totaltime)
 }
+
+func Heapsz() int {
+	return int(memstats.next_gc)
+}
+
+func Setheap(n int) {
+	heapminimum = uint64(n)
+}

commit 34c58065e54e0ac2d610b4a550bdba8f1db90ec6
Author: Brad Fitzpatrick <bradfitz@golang.org>
Date:   Wed Apr 6 02:52:17 2016 +0000

    runtime: rename os1_linux.go to os_linux.go
    
    Change-Id: I938f61763c3256a876d62aeb54ef8c25cc4fc90e
    Reviewed-on: https://go-review.googlesource.com/21567
    Reviewed-by: Minux Ma <minux@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
new file mode 100644
index 0000000000..7d8cc7e5c4
--- /dev/null
+++ b/src/runtime/os_linux.go
@@ -0,0 +1,422 @@
+// Copyright 2009 The Go Authors. All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import (
+	"runtime/internal/sys"
+	"unsafe"
+)
+
+type mOS struct{}
+
+//go:noescape
+func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer, val3 uint32) int32
+
+// Linux futex.
+//
+//	futexsleep(uint32 *addr, uint32 val)
+//	futexwakeup(uint32 *addr)
+//
+// Futexsleep atomically checks if *addr == val and if so, sleeps on addr.
+// Futexwakeup wakes up threads sleeping on addr.
+// Futexsleep is allowed to wake up spuriously.
+
+const (
+	_FUTEX_WAIT = 0
+	_FUTEX_WAKE = 1
+)
+
+// Atomically,
+//	if(*addr == val) sleep
+// Might be woken up spuriously; that's allowed.
+// Don't sleep longer than ns; ns < 0 means forever.
+//go:nosplit
+func futexsleep(addr *uint32, val uint32, ns int64) {
+	var ts timespec
+
+	// Some Linux kernels have a bug where futex of
+	// FUTEX_WAIT returns an internal error code
+	// as an errno. Libpthread ignores the return value
+	// here, and so can we: as it says a few lines up,
+	// spurious wakeups are allowed.
+	if ns < 0 {
+		futex(unsafe.Pointer(addr), _FUTEX_WAIT, val, nil, nil, 0)
+		return
+	}
+
+	// It's difficult to live within the no-split stack limits here.
+	// On ARM and 386, a 64-bit divide invokes a general software routine
+	// that needs more stack than we can afford. So we use timediv instead.
+	// But on real 64-bit systems, where words are larger but the stack limit
+	// is not, even timediv is too heavy, and we really need to use just an
+	// ordinary machine instruction.
+	if sys.PtrSize == 8 {
+		ts.set_sec(ns / 1000000000)
+		ts.set_nsec(int32(ns % 1000000000))
+	} else {
+		ts.tv_nsec = 0
+		ts.set_sec(int64(timediv(ns, 1000000000, (*int32)(unsafe.Pointer(&ts.tv_nsec)))))
+	}
+	futex(unsafe.Pointer(addr), _FUTEX_WAIT, val, unsafe.Pointer(&ts), nil, 0)
+}
+
+// If any procs are sleeping on addr, wake up at most cnt.
+//go:nosplit
+func futexwakeup(addr *uint32, cnt uint32) {
+	ret := futex(unsafe.Pointer(addr), _FUTEX_WAKE, cnt, nil, nil, 0)
+	if ret >= 0 {
+		return
+	}
+
+	// I don't know that futex wakeup can return
+	// EAGAIN or EINTR, but if it does, it would be
+	// safe to loop and call futex again.
+	systemstack(func() {
+		print("futexwakeup addr=", addr, " returned ", ret, "\n")
+	})
+
+	*(*int32)(unsafe.Pointer(uintptr(0x1006))) = 0x1006
+}
+
+func getproccount() int32 {
+	// This buffer is huge (8 kB) but we are on the system stack
+	// and there should be plenty of space (64 kB).
+	// Also this is a leaf, so we're not holding up the memory for long.
+	// See golang.org/issue/11823.
+	// The suggested behavior here is to keep trying with ever-larger
+	// buffers, but we don't have a dynamic memory allocator at the
+	// moment, so that's a bit tricky and seems like overkill.
+	const maxCPUs = 64 * 1024
+	var buf [maxCPUs / (sys.PtrSize * 8)]uintptr
+	r := sched_getaffinity(0, unsafe.Sizeof(buf), &buf[0])
+	n := int32(0)
+	for _, v := range buf[:r/sys.PtrSize] {
+		for v != 0 {
+			n += int32(v & 1)
+			v >>= 1
+		}
+	}
+	if n == 0 {
+		n = 1
+	}
+	return n
+}
+
+// Clone, the Linux rfork.
+const (
+	_CLONE_VM             = 0x100
+	_CLONE_FS             = 0x200
+	_CLONE_FILES          = 0x400
+	_CLONE_SIGHAND        = 0x800
+	_CLONE_PTRACE         = 0x2000
+	_CLONE_VFORK          = 0x4000
+	_CLONE_PARENT         = 0x8000
+	_CLONE_THREAD         = 0x10000
+	_CLONE_NEWNS          = 0x20000
+	_CLONE_SYSVSEM        = 0x40000
+	_CLONE_SETTLS         = 0x80000
+	_CLONE_PARENT_SETTID  = 0x100000
+	_CLONE_CHILD_CLEARTID = 0x200000
+	_CLONE_UNTRACED       = 0x800000
+	_CLONE_CHILD_SETTID   = 0x1000000
+	_CLONE_STOPPED        = 0x2000000
+	_CLONE_NEWUTS         = 0x4000000
+	_CLONE_NEWIPC         = 0x8000000
+
+	cloneFlags = _CLONE_VM | /* share memory */
+		_CLONE_FS | /* share cwd, etc */
+		_CLONE_FILES | /* share fd table */
+		_CLONE_SIGHAND | /* share sig handler table */
+		_CLONE_THREAD /* revisit - okay for now */
+)
+
+//go:noescape
+func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
+
+// May run with m.p==nil, so write barriers are not allowed.
+//go:nowritebarrier
+func newosproc(mp *m, stk unsafe.Pointer) {
+	/*
+	 * note: strace gets confused if we use CLONE_PTRACE here.
+	 */
+	if false {
+		print("newosproc stk=", stk, " m=", mp, " g=", mp.g0, " clone=", funcPC(clone), " id=", mp.id, " ostk=", &mp, "\n")
+	}
+
+	// Disable signals during clone, so that the new thread starts
+	// with signals disabled. It will enable them in minit.
+	var oset sigset
+	rtsigprocmask(_SIG_SETMASK, &sigset_all, &oset, int32(unsafe.Sizeof(oset)))
+	ret := clone(cloneFlags, stk, unsafe.Pointer(mp), unsafe.Pointer(mp.g0), unsafe.Pointer(funcPC(mstart)))
+	rtsigprocmask(_SIG_SETMASK, &oset, nil, int32(unsafe.Sizeof(oset)))
+
+	if ret < 0 {
+		print("runtime: failed to create new OS thread (have ", mcount(), " already; errno=", -ret, ")\n")
+		throw("newosproc")
+	}
+}
+
+// Version of newosproc that doesn't require a valid G.
+//go:nosplit
+func newosproc0(stacksize uintptr, fn unsafe.Pointer) {
+	stack := sysAlloc(stacksize, &memstats.stacks_sys)
+	if stack == nil {
+		write(2, unsafe.Pointer(&failallocatestack[0]), int32(len(failallocatestack)))
+		exit(1)
+	}
+	ret := clone(cloneFlags, unsafe.Pointer(uintptr(stack)+stacksize), nil, nil, fn)
+	if ret < 0 {
+		write(2, unsafe.Pointer(&failthreadcreate[0]), int32(len(failthreadcreate)))
+		exit(1)
+	}
+}
+
+var failallocatestack = []byte("runtime: failed to allocate stack for the new OS thread\n")
+var failthreadcreate = []byte("runtime: failed to create new OS thread\n")
+
+func osinit() {
+	ncpu = getproccount()
+}
+
+var urandom_dev = []byte("/dev/urandom\x00")
+
+func getRandomData(r []byte) {
+	if startupRandomData != nil {
+		n := copy(r, startupRandomData)
+		extendRandom(r, n)
+		return
+	}
+	fd := open(&urandom_dev[0], 0 /* O_RDONLY */, 0)
+	n := read(fd, unsafe.Pointer(&r[0]), int32(len(r)))
+	closefd(fd)
+	extendRandom(r, int(n))
+}
+
+func goenvs() {
+	goenvs_unix()
+}
+
+// Called to do synchronous initialization of Go code built with
+// -buildmode=c-archive or -buildmode=c-shared.
+// None of the Go runtime is initialized.
+//go:nosplit
+//go:nowritebarrierrec
+func libpreinit() {
+	initsig(true)
+}
+
+// Called to initialize a new m (including the bootstrap m).
+// Called on the parent thread (main thread in case of bootstrap), can allocate memory.
+func mpreinit(mp *m) {
+	mp.gsignal = malg(32 * 1024) // Linux wants >= 2K
+	mp.gsignal.m = mp
+}
+
+//go:nosplit
+func msigsave(mp *m) {
+	smask := &mp.sigmask
+	rtsigprocmask(_SIG_SETMASK, nil, smask, int32(unsafe.Sizeof(*smask)))
+}
+
+//go:nosplit
+func msigrestore(sigmask sigset) {
+	rtsigprocmask(_SIG_SETMASK, &sigmask, nil, int32(unsafe.Sizeof(sigmask)))
+}
+
+//go:nosplit
+func sigblock() {
+	rtsigprocmask(_SIG_SETMASK, &sigset_all, nil, int32(unsafe.Sizeof(sigset_all)))
+}
+
+func gettid() uint32
+
+// Called to initialize a new m (including the bootstrap m).
+// Called on the new thread, cannot allocate memory.
+func minit() {
+	// Initialize signal handling.
+	_g_ := getg()
+
+	var st sigaltstackt
+	sigaltstack(nil, &st)
+	if st.ss_flags&_SS_DISABLE != 0 {
+		signalstack(&_g_.m.gsignal.stack)
+		_g_.m.newSigstack = true
+	} else {
+		// Use existing signal stack.
+		stsp := uintptr(unsafe.Pointer(st.ss_sp))
+		_g_.m.gsignal.stack.lo = stsp
+		_g_.m.gsignal.stack.hi = stsp + st.ss_size
+		_g_.m.gsignal.stackguard0 = stsp + _StackGuard
+		_g_.m.gsignal.stackguard1 = stsp + _StackGuard
+		_g_.m.gsignal.stackAlloc = st.ss_size
+		_g_.m.newSigstack = false
+	}
+
+	// for debuggers, in case cgo created the thread
+	_g_.m.procid = uint64(gettid())
+
+	// restore signal mask from m.sigmask and unblock essential signals
+	nmask := _g_.m.sigmask
+	for i := range sigtable {
+		if sigtable[i].flags&_SigUnblock != 0 {
+			sigdelset(&nmask, i)
+		}
+	}
+	rtsigprocmask(_SIG_SETMASK, &nmask, nil, int32(unsafe.Sizeof(nmask)))
+}
+
+// Called from dropm to undo the effect of an minit.
+//go:nosplit
+func unminit() {
+	if getg().m.newSigstack {
+		signalstack(nil)
+	}
+}
+
+func memlimit() uintptr {
+	/*
+		TODO: Convert to Go when something actually uses the result.
+
+		Rlimit rl;
+		extern byte runtimetext[], runtimeend[];
+		uintptr used;
+
+		if(runtimegetrlimit(RLIMIT_AS, &rl) != 0)
+			return 0;
+		if(rl.rlim_cur >= 0x7fffffff)
+			return 0;
+
+		// Estimate our VM footprint excluding the heap.
+		// Not an exact science: use size of binary plus
+		// some room for thread stacks.
+		used = runtimeend - runtimetext + (64<<20);
+		if(used >= rl.rlim_cur)
+			return 0;
+
+		// If there's not at least 16 MB left, we're probably
+		// not going to be able to do much. Treat as no limit.
+		rl.rlim_cur -= used;
+		if(rl.rlim_cur < (16<<20))
+			return 0;
+
+		return rl.rlim_cur - used;
+	*/
+
+	return 0
+}
+
+//#ifdef GOARCH_386
+//#define sa_handler k_sa_handler
+//#endif
+
+func sigreturn()
+func sigtramp()
+func cgoSigtramp()
+
+//go:noescape
+func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
+
+//go:noescape
+func sigaltstack(new, old *sigaltstackt)
+
+//go:noescape
+func setitimer(mode int32, new, old *itimerval)
+
+//go:noescape
+func rtsigprocmask(sig uint32, new, old *sigset, size int32)
+
+//go:noescape
+func getrlimit(kind int32, limit unsafe.Pointer) int32
+func raise(sig int32)
+func raiseproc(sig int32)
+
+//go:noescape
+func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+func osyield()
+
+//go:nosplit
+//go:nowritebarrierrec
+func setsig(i int32, fn uintptr, restart bool) {
+	var sa sigactiont
+	memclr(unsafe.Pointer(&sa), unsafe.Sizeof(sa))
+	sa.sa_flags = _SA_SIGINFO | _SA_ONSTACK | _SA_RESTORER
+	if restart {
+		sa.sa_flags |= _SA_RESTART
+	}
+	sigfillset(&sa.sa_mask)
+	// Although Linux manpage says "sa_restorer element is obsolete and
+	// should not be used". x86_64 kernel requires it. Only use it on
+	// x86.
+	if GOARCH == "386" || GOARCH == "amd64" {
+		sa.sa_restorer = funcPC(sigreturn)
+	}
+	if fn == funcPC(sighandler) {
+		if iscgo {
+			fn = funcPC(cgoSigtramp)
+		} else {
+			fn = funcPC(sigtramp)
+		}
+	}
+	sa.sa_handler = fn
+	rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask))
+}
+
+//go:nosplit
+//go:nowritebarrierrec
+func setsigstack(i int32) {
+	var sa sigactiont
+	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
+		throw("rt_sigaction failure")
+	}
+	if sa.sa_handler == 0 || sa.sa_handler == _SIG_DFL || sa.sa_handler == _SIG_IGN || sa.sa_flags&_SA_ONSTACK != 0 {
+		return
+	}
+	sa.sa_flags |= _SA_ONSTACK
+	if rt_sigaction(uintptr(i), &sa, nil, unsafe.Sizeof(sa.sa_mask)) != 0 {
+		throw("rt_sigaction failure")
+	}
+}
+
+//go:nosplit
+//go:nowritebarrierrec
+func getsig(i int32) uintptr {
+	var sa sigactiont
+
+	memclr(unsafe.Pointer(&sa), unsafe.Sizeof(sa))
+	if rt_sigaction(uintptr(i), nil, &sa, unsafe.Sizeof(sa.sa_mask)) != 0 {
+		throw("rt_sigaction read failure")
+	}
+	if sa.sa_handler == funcPC(sigtramp) || sa.sa_handler == funcPC(cgoSigtramp) {
+		return funcPC(sighandler)
+	}
+	return sa.sa_handler
+}
+
+//go:nosplit
+func signalstack(s *stack) {
+	var st sigaltstackt
+	if s == nil {
+		st.ss_flags = _SS_DISABLE
+	} else {
+		st.ss_sp = (*byte)(unsafe.Pointer(s.lo))
+		st.ss_size = s.hi - s.lo
+		st.ss_flags = 0
+	}
+	sigaltstack(&st, nil)
+}
+
+//go:nosplit
+//go:nowritebarrierrec
+func updatesigmask(m sigmask) {
+	var mask sigset
+	sigcopyset(&mask, m)
+	rtsigprocmask(_SIG_SETMASK, &mask, nil, int32(unsafe.Sizeof(mask)))
+}
+
+func unblocksig(sig int32) {
+	var mask sigset
+	sigaddset(&mask, int(sig))
+	rtsigprocmask(_SIG_UNBLOCK, &mask, nil, int32(unsafe.Sizeof(mask)))
+}

commit 5103fbfdb29278533c666163a9d56f85408224d9
Author: Brad Fitzpatrick <bradfitz@golang.org>
Date:   Wed Apr 6 02:51:55 2016 +0000

    runtime: merge os_linux.go into os1_linux.go
    
    Change-Id: I791c47014fe69e8529c7b2f0b9a554e47902d46c
    Reviewed-on: https://go-review.googlesource.com/21566
    Reviewed-by: Minux Ma <minux@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
deleted file mode 100644
index dd69743e10..0000000000
--- a/src/runtime/os_linux.go
+++ /dev/null
@@ -1,36 +0,0 @@
-// Copyright 2014 The Go Authors. All rights reserved.
-// Use of this source code is governed by a BSD-style
-// license that can be found in the LICENSE file.
-
-package runtime
-
-import "unsafe"
-
-type mOS struct{}
-
-//go:noescape
-func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer, val3 uint32) int32
-
-//go:noescape
-func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
-
-//go:noescape
-func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
-
-//go:noescape
-func sigaltstack(new, old *sigaltstackt)
-
-//go:noescape
-func setitimer(mode int32, new, old *itimerval)
-
-//go:noescape
-func rtsigprocmask(sig uint32, new, old *sigset, size int32)
-
-//go:noescape
-func getrlimit(kind int32, limit unsafe.Pointer) int32
-func raise(sig int32)
-func raiseproc(sig int32)
-
-//go:noescape
-func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
-func osyield()

commit 270ff99522fe4bdc5f45fc48fa2bb6809a79893e
Merge: 5d73957d24 5c449bd30b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 1 16:22:27 2016 -0400

    Merge branch 'g1.6' into g1.6refpgs
    
    Conflicts:
            biscuit/main.go
            src/runtime/os_linux.go

commit 470361b6983f0afc73a089857c9b1fedef3a8d2e
Merge: e06f912f02 23d0c6b74a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 1 15:24:46 2016 -0400

    Merge branch 'master' into g1.6
    
    Conflicts:
            biscuit/syscall.go
            biscuit/user/c/time.c

commit c12b1e36fcc40adeccd77dfbc7c0c76e87dedb4d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Apr 1 15:07:46 2016 -0400

    use largest E820 segment
    
    gives me 17GB of physical memory instead of 1.3GB on my test hardware

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 31f8ad053d..d281d58ac6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1154,9 +1154,11 @@ var pglast uintptr
 
 //go:nosplit
 func phys_init() {
+	origfirst := pgfirst
 	sec := (*secret_t)(unsafe.Pointer(uintptr(0x7c00)))
 	found := false
 	base := sec.e820p
+	maxfound := uintptr(0)
 	// bootloader provides 15 e820 entries at most (it panicks if the PC
 	// provides more).
 	for i := uintptr(0); i < 15; i++ {
@@ -1164,11 +1166,22 @@ func phys_init() {
 		if ep.len == 0 {
 			continue
 		}
-		endpg := ep.start + ep.len
-		if pgfirst >= ep.start && pgfirst < endpg {
-			pglast = endpg
+		// use largest segment
+		if ep.len > maxfound {
+			maxfound = ep.len
+			_eseg = *ep
 			found = true
-			break
+			// initialize pgfirst/pglast. if the segment contains
+			// origfirst, then the bootloader already allocated the
+			// the pages from [ep.start, origfirst). thus set
+			// pgfirst to origfirst.
+			endpg := ep.start + ep.len
+			pglast = endpg
+			if origfirst >= ep.start && origfirst < endpg {
+				pgfirst = origfirst
+			} else {
+				pgfirst = ep.start
+			}
 		}
 	}
 	if !found {
@@ -1179,6 +1192,12 @@ func phys_init() {
 	}
 }
 
+var _eseg e820_t
+
+func Totalphysmem() int {
+	return int(_eseg.start + _eseg.len)
+}
+
 //go:nosplit
 func get_pg() uintptr {
 	if pglast == 0 {

commit b3c876b13ae0a95fbed94f6e7825699798d07016
Merge: 1e639341ec bbe8bd23a4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 30 17:49:23 2016 -0400

    Merge branch 'master' into g1.6

commit 44287673b2d2fbfdbcbf2d9bb0fb47120bcbbc60
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 30 17:44:40 2016 -0400

    after taking NMI due to PMU, re-enable all counters
    
    not just the one that overflowed

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0b1bd11852..31f8ad053d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -220,6 +220,7 @@ type nmiprof_t struct {
 	evtsel		int
 	evtmin		uint
 	evtmax		uint
+	gctrl		int
 }
 
 var _nmibuf [4096]uintptr
@@ -230,6 +231,13 @@ func SetNMI(mask bool, evtsel int, min, max uint) {
 	nmiprof.evtsel = evtsel
 	nmiprof.evtmin = min
 	nmiprof.evtmax = max
+	// create value for ia32_perf_global_ctrl, to easily enable pmcs. does
+	// not enable fixed function counters.
+	ax, _, _, _ := Cpuid(0xa, 0)
+	npmc := (ax >> 8) & 0xff
+	for i := uint32(0); i < npmc; i++ {
+		nmiprof.gctrl |= 1 << i
+	}
 }
 
 func TakeNMIBuf() ([]uintptr, bool) {
@@ -376,7 +384,7 @@ func _pmcreset(en bool) {
 	Wrmsr(ia32_debugctl, freeze_pmc_on_pmi)
 	// cpu clears global_ctrl on PMI if freeze-on-pmi is set.
 	// re-enable
-	Wrmsr(ia32_global_ctrl, 1)
+	Wrmsr(ia32_global_ctrl, nmiprof.gctrl)
 
 	v := nmiprof.evtsel
 	Wrmsr(ia32_perfevtsel0, v)

commit 5d73957d24a2eb170242fa2b0ef4ca915e1cd1d7
Merge: e666b52176 25fbb28fd9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 28 13:52:03 2016 -0400

    Merge branch 'refpgs' into g1.6refpgs

commit e666b5217681eeb9544b7c558f51abf5b767933b
Merge: ac4fd1e2ef 74b9cf7ec2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 28 09:36:10 2016 -0400

    Merge branch 'master' into g1.6

commit 74b9cf7ec29e97b74361f0548bffa4e454925023
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 28 09:35:49 2016 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c7e706efde..0b1bd11852 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -359,27 +359,27 @@ func _pmcreset(en bool) {
 	ia32_global_ctrl := 0x38f
 	//ia32_perf_global_status := uint32(0x38e)
 
-	if en {
-		// disable perf counter before clearing
-		Wrmsr(ia32_perfevtsel0, 0)
+	// disable perf counter before clearing
+	Wrmsr(ia32_perfevtsel0, 0)
 
-		// clear overflow
-		Wrmsr(ia32_perf_global_ovf_ctrl, 1)
+	if !en {
+		return
+	}
 
-		r := dumrand(nmiprof.evtmin, nmiprof.evtmax)
-		Wrmsr(ia32_pmc0, -int(r))
+	// clear overflow
+	Wrmsr(ia32_perf_global_ovf_ctrl, 1)
 
-		freeze_pmc_on_pmi := 1 << 12
-		Wrmsr(ia32_debugctl, freeze_pmc_on_pmi)
-		// cpu clears global_ctrl on PMI if freeze-on-pmi is set.
-		// re-enable
-		Wrmsr(ia32_global_ctrl, 1)
+	r := dumrand(nmiprof.evtmin, nmiprof.evtmax)
+	Wrmsr(ia32_pmc0, -int(r))
 
-		v := nmiprof.evtsel
-		Wrmsr(ia32_perfevtsel0, v)
-	} else {
-		Wrmsr(ia32_perfevtsel0, 0)
-	}
+	freeze_pmc_on_pmi := 1 << 12
+	Wrmsr(ia32_debugctl, freeze_pmc_on_pmi)
+	// cpu clears global_ctrl on PMI if freeze-on-pmi is set.
+	// re-enable
+	Wrmsr(ia32_global_ctrl, 1)
+
+	v := nmiprof.evtsel
+	Wrmsr(ia32_perfevtsel0, v)
 
 	// the write to debugctl enabling LBR must come after clearing overflow
 	// via global_ovf_ctrl; otherwise the processor instantly clears lbr...

commit 25fbb28fd9756a46f0493be877f7fe664ef92007
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 25 12:02:56 2016 -0400

    allocate pmap and user pages manually via reference counting

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c7e706efde..db87228f07 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1171,6 +1171,10 @@ func phys_init() {
 	}
 }
 
+func Get_phys() uintptr {
+	return get_pg()
+}
+
 //go:nosplit
 func get_pg() uintptr {
 	if pglast == 0 {

commit 95976b6c289e752b1e38f69928598291404c522e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 22 13:30:22 2016 -0400

    update sys_info(2) for go1.6 GC stats

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a44204b767..3cd2ee3ddb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -2671,3 +2671,7 @@ func Pnum(n int) {
 func Kpmap_p() uintptr {
 	return p_kpmap
 }
+
+func GCworktime() int {
+	return int(work.totaltime)
+}

commit a4597a0d0c30278be6922739d50c07eaaccf76d4
Merge: 59edf3b5ab 7bfde1c2a7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 18 15:51:04 2016 -0400

    Merge branch 'master' into g1.6

commit 7bfde1c2a79c854799b7f9c3ec68c29c4996bbff
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 18 15:50:23 2016 -0400

    fix Cpuprint

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fefc04d7f8..c7e706efde 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -610,7 +610,7 @@ var _cpuattrs [MAXCPUS]uint16
 func Cpuprint(n uint16, row uintptr) {
 	p := uintptr(0xb8000)
 	num := Gscpu().num
-	p += uintptr(num) + row*80
+	p += uintptr(num) * row*80*2
 	attr := _cpuattrs[num]
 	_cpuattrs[num] += 0x100
 	*(*uint16)(unsafe.Pointer(p)) = attr | n

commit 2127140d40c07fbfb7b306413974738a6513d301
Merge: 5e688d3bc1 1378634a88
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 17 14:41:27 2016 -0400

    Merge branch 'master' into g1.6

commit ed705e5d5685b44117883ef35a316f2715970817
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 17 11:06:10 2016 -0400

    fix pthread_self(3)
    
    i forgot to make it return the thread id instead of pid after adding pthread
    support, nearly a year ago.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 2de8c32822..fefc04d7f8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -180,6 +180,11 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
+	// XXX to avoid {rd,wr}msr on int/syscall context switch, we can move
+	// each cpu's cpu_t into it's M's tls memory, thus freeing up %gs. %gs
+	// would then be storage for %fs of the opposite privilege level (%gs
+	// holds kernels %fs in usermode, user's %fs is kernel mode). then we
+	// can use swapgs, like other kernels.
 	ia32_fs_base := 0xc0000100
 	kfsbase := Rdmsr(ia32_fs_base)
 	Wrmsr(ia32_fs_base, tf[TF_FSBASE])

commit 9e2d502d74cec4bdfb9184688f47c19546c1873d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 14 18:31:04 2016 -0400

    whoops; make biscuit under go1.4 compile again

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 251853fbd8..2de8c32822 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -516,7 +516,6 @@ func Trapinit() {
 	mcall(trapinit_m)
 }
 
-var hackmode int64
 var Halt uint32
 
 // wait until remove definition from proc.c

commit 6aab455cac150466c54183621f099381a7ead533
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 14 09:15:29 2016 -0400

    go1.6 updates
    
    only small changes: they renamed atomic functions, also use sigaltstack(2) to
    query current signal stack instead of only specifying a new one, and made m0's
    struct statically allocated.
    
    go1.6 now works too!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e7295e4c9a..89b7101931 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -5,6 +5,7 @@
 package runtime
 
 import "unsafe"
+import "runtime/internal/atomic"
 
 type mOS struct{}
 
@@ -133,12 +134,13 @@ type thread_t struct {
 	status		int
 	doingsig	int
 	sigstack	uintptr
+	sigsize		uintptr
 	prof		prof_t
 	sleepfor	int
 	sleepret	int
 	futaddr		uintptr
 	p_pmap		uintptr
-	//_pad		int
+	_pad		int
 }
 
 // XXX fix these misleading names
@@ -290,7 +292,7 @@ func _consumelbr() {
 	// XXX stacklen
 	l := 16 * 2
 	l++
-	idx := int(xadd64(&nmiprof.bufidx, int64(l)))
+	idx := int(atomic.Xadd64(&nmiprof.bufidx, int64(l)))
 	idx -= l
 	for i := 0; i < 16; i++ {
 		cur := (last - i)
@@ -340,7 +342,7 @@ func _lbrreset(en bool) {
 
 //go:nosplit
 func perfgather(tf *[TFSIZE]uintptr) {
-	idx := xadd64(&nmiprof.bufidx, 1) - 1
+	idx := atomic.Xadd64(&nmiprof.bufidx, 1) - 1
 	if idx < uint64(len(nmiprof.buf)) {
 		//nmiprof.buf[idx] = tf[TF_RIP]
 		//pid := Gscpu().pid
@@ -531,7 +533,7 @@ func cls() {
 	sc_put('\n')
 }
 
-var hackmode int32
+var hackmode int64
 var Halt uint32
 
 // wait until remove definition from proc.c
@@ -542,7 +544,7 @@ type spinlock_t struct {
 //go:nosplit
 func splock(l *spinlock_t) {
 	for {
-		if xchg(&l.v, 1) == 0 {
+		if atomic.Xchg(&l.v, 1) == 0 {
 			break
 		}
 		for l.v != 0 {
@@ -553,7 +555,7 @@ func splock(l *spinlock_t) {
 
 //go:nosplit
 func spunlock(l *spinlock_t) {
-	//atomicstore(&l.v, 0)
+	//atomic.Store(&l.v, 0)
 	l.v = 0
 }
 
@@ -643,10 +645,12 @@ func cpupnum(rip uintptr) {
 //go:nosplit
 func pancake(msg string, addr uintptr) {
 	Pushcli()
-	atomicstore(&Halt, 1)
+	atomic.Store(&Halt, 1)
 	_pmsg(msg)
 	_pnum(addr)
 	_pmsg("PANCAKE")
+	a := 0
+	stack_dump(uintptr(unsafe.Pointer(&a)))
 	for {
 		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
 		*p = 0x1400 | 'F'
@@ -854,7 +858,7 @@ func seg_setup() {
 	// now that we have a GDT, setup tls for the first thread.
 	// elf tls specification defines user tls at -16(%fs). go1.5 uses
 	// -8(%fs) though.
-	t := uintptr(unsafe.Pointer(&tls0[0]))
+	t := uintptr(unsafe.Pointer(&m0.tls[0]))
 	tlsaddr := int(t + 8)
 	// we must set fs/gs at least once before we use the MSRs to change
 	// their base address. the MSRs write directly to hidden segment
@@ -1268,7 +1272,7 @@ func rlap(reg uint) uint32 {
 		pancake("lapic not init", 0)
 	}
 	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
-	return atomicload(&lpg[reg])
+	return atomic.Load(&lpg[reg])
 }
 
 //go:nosplit
@@ -1558,19 +1562,19 @@ var tlbshoot_pmap uintptr
 var tlbshoot_gen uint64
 
 func Tlbadmit(p_pmap, cpuwait, pg, pgcount uintptr) uint64 {
-	for !casuintptr(&tlbshoot_wait, 0, cpuwait) {
+	for !atomic.Casuintptr(&tlbshoot_wait, 0, cpuwait) {
 		preemptok()
 	}
-	xchguintptr(&tlbshoot_pg, pg)
-	xchguintptr(&tlbshoot_count, pgcount)
-	xchguintptr(&tlbshoot_pmap, p_pmap)
-	xadd64(&tlbshoot_gen, 1)
+	atomic.Xchguintptr(&tlbshoot_pg, pg)
+	atomic.Xchguintptr(&tlbshoot_count, pgcount)
+	atomic.Xchguintptr(&tlbshoot_pmap, p_pmap)
+	atomic.Xadd64(&tlbshoot_gen, 1)
 	return tlbshoot_gen
 }
 
 func Tlbwait(gen uint64) {
-	for atomicloaduintptr(&tlbshoot_wait) != 0 {
-		if atomicload64(&tlbshoot_gen) != gen {
+	for atomic.Loaduintptr(&tlbshoot_wait) != 0 {
+		if atomic.Load64(&tlbshoot_gen) != gen {
 			break
 		}
 	}
@@ -1591,7 +1595,7 @@ func tlb_shootdown() {
 		//}
 	}
 	dur := (*uint64)(unsafe.Pointer(&tlbshoot_wait))
-	v := xadd64(dur, -1)
+	v := atomic.Xadd64(dur, -1)
 	if v < 0 {
 		pancake("shootwait < 0", uintptr(v))
 	}
@@ -2187,7 +2191,7 @@ func mksig(t *thread_t, signo int32) {
 	t.status = ST_RUNNABLE
 	t.doingsig = 1
 
-	rsp := t.sigstack
+	rsp := t.sigstack + t.sigsize
 	ucsz := unsafe.Sizeof(ucontext_t{})
 	rsp -= ucsz
 	_ctxt := rsp
@@ -2465,11 +2469,23 @@ func hack_sigaltstack(new, old *sigaltstackt) {
 	fl := Pushcli()
 	ct := Gscpu().mythread
 	SS_DISABLE := int32(2)
-	if new.ss_flags & SS_DISABLE != 0 {
-		ct.sigstack = 0
-	} else {
-		ct.sigstack = uintptr(unsafe.Pointer(new.ss_sp)) +
-		    uintptr(new.ss_size)
+	if old != nil {
+		if ct.sigstack == 0 {
+			old.ss_flags = SS_DISABLE
+		} else {
+			old.ss_sp = (*byte)(unsafe.Pointer(ct.sigstack))
+			old.ss_size = ct.sigsize
+			old.ss_flags = 0
+		}
+	}
+	if new != nil {
+		if new.ss_flags & SS_DISABLE != 0 {
+			ct.sigstack = 0
+			ct.sigsize = 0
+		} else {
+			ct.sigstack = uintptr(unsafe.Pointer(new.ss_sp))
+			ct.sigsize = new.ss_size
+		}
 	}
 	Popcli(fl)
 }
@@ -2603,7 +2619,7 @@ func hack_exit(code int32) {
 	pmsg("exit with code")
 	pnum(uintptr(code))
 	pmsg(".\nhalting\n")
-	atomicstore(&Halt, 1)
+	atomic.Store(&Halt, 1)
 	for {
 	}
 }
@@ -2635,7 +2651,7 @@ func Nanotime() int {
 // useful for basic tests of filesystem durability
 func Crash() {
 	pmsg("CRASH!\n")
-	atomicstore(&Halt, 1)
+	atomic.Store(&Halt, 1)
 	for {
 	}
 }

commit 93ec45cbb498d55871ebaf82ac1a7147f892e3f4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Mar 13 12:24:28 2016 -0400

    correct hackmode type

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 87916f7c48..251853fbd8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -516,7 +516,7 @@ func Trapinit() {
 	mcall(trapinit_m)
 }
 
-var hackmode int32
+var hackmode int64
 var Halt uint32
 
 // wait until remove definition from proc.c

commit 0d4fb800de4704a94b0306fb7e6bb051c06e70b6
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Mar 13 10:08:16 2016 -0400

    convert trap sched/wake code
    
    Conflicts:
            src/runtime/proc1.go

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 98e702e548..e7295e4c9a 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -85,11 +85,6 @@ func _userint()
 func _Userrun(*[24]int, bool) (int, int)
 func Wrmsr(int, int)
 
-// proc.c
-func Trapwake()
-func trapsched_m(*g)
-func trapinit_m(*g)
-
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
 // that have the stack splitting prologue. the following is a list of go code
@@ -536,15 +531,6 @@ func cls() {
 	sc_put('\n')
 }
 
-//func Trapsched() {
-//	mcall(trapsched_m)
-//}
-
-// called only once to setup
-//func Trapinit() {
-//	mcall(trapinit_m)
-//}
-
 var hackmode int32
 var Halt uint32
 
@@ -1951,6 +1937,159 @@ func yieldy() {
 	sched_halt()
 }
 
+// called only once to setup
+func Trapinit() {
+	mcall(trapinit_m)
+}
+
+func Trapsched() {
+	mcall(trapsched_m)
+}
+
+// called from the CPU interrupt handler. must only be called while interrupts
+// are disabled
+ //go:nosplit
+ func Trapwake() {
+	splock(tlock);
+	_nints++
+	// only flag the Ps if a handler isn't currently active
+	if trapst == IDLE {
+		_ptrap = true
+	}
+	spunlock(tlock)
+}
+// trap handling goroutines first call. it is not an error if there are no
+// interrupts when this is called.
+func trapinit_m(gp *g) {
+	_trapsched(gp, true)
+}
+
+func trapsched_m(gp *g) {
+	_trapsched(gp, false)
+}
+
+var tlock = &spinlock_t{}
+
+var _initted bool
+var _trapsleeper *g
+var _nints int
+
+// i think it is ok for these trap sched/wake functions to be nosplit and call
+// nosplit functions even though they clear interrupts because we first switch
+// to the scheduler stack where preemptions are ignored.
+func _trapsched(gp *g, firsttime bool) {
+	fl := Pushcli()
+	splock(tlock)
+
+	if firsttime {
+		if _initted {
+			pancake("two inits", 0)
+		}
+		_initted = true
+		// if there are traps already, let a P wake us up.
+		//goto bed
+	}
+
+	// decrement handled ints
+	if !firsttime {
+		if _nints <= 0 {
+			pancake("neg ints", uintptr(_nints))
+		}
+		_nints--
+	}
+
+	// check if we are done
+	if !firsttime && _nints != 0 {
+		// keep processing
+		tprepsleep(gp, false)
+		_g_ := getg()
+		runqput(_g_.m.p.ptr(), gp, true)
+	} else {
+		tprepsleep(gp, true)
+		if _trapsleeper != nil {
+			pancake("trapsleeper set", 0)
+		}
+		_trapsleeper = gp
+	}
+
+	spunlock(tlock)
+	Popcli(fl)
+
+	schedule()
+}
+
+var trapst int
+const (
+	IDLE	= iota
+	RUNNING	= iota
+)
+
+func tprepsleep(gp *g, done bool) {
+	if done {
+		trapst = IDLE
+	} else {
+		trapst = RUNNING
+	}
+
+	status := readgstatus(gp)
+	if((status&^_Gscan) != _Grunning){
+		pancake("bad g status", uintptr(status))
+	}
+	nst := uint32(_Grunnable)
+	if done {
+		nst = _Gwaiting
+		gp.waitreason = "waiting for trap"
+	}
+	casgstatus(gp, _Grunning, nst)
+	dropg()
+}
+
+var _ptrap bool
+
+func trapcheck(pp *p) {
+	if !_ptrap {
+		return
+	}
+	fl := Pushcli()
+	splock(tlock)
+
+	var trapgp *g
+	if !_ptrap {
+		goto out
+	}
+	// don't clear the start flag if the handler goroutine hasn't
+	// registered yet.
+	if !_initted {
+		goto out
+	}
+	if trapst == RUNNING {
+		goto out
+	}
+
+	if trapst != IDLE {
+		pancake("bad trap status", uintptr(trapst))
+	}
+
+	_ptrap = false
+	trapst = RUNNING
+
+	trapgp = _trapsleeper
+	_trapsleeper = nil
+
+	spunlock(tlock)
+	Popcli(fl)
+
+	casgstatus(trapgp, _Gwaiting, _Grunnable)
+
+	// hopefully the trap goroutine is executed soon
+	runqput(pp, trapgp, true)
+	return
+out:
+	spunlock(tlock)
+	Popcli(fl)
+	return
+}
+
 // goprofiling is implemented by simulating the SIGPROF signal. when proftick
 // observes that enough time has elapsed, mksig() is used to deliver SIGPROF to
 // the runtime and things are setup so the runtime returns to sigsim().

commit 28f5e5bec78489d6944748d1fae28b3aa5a54c2c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 12 12:15:04 2016 -0500

    temporarily disable trap sleep/wake
    
    for easier testing. need to write trap sleep/wake for 1.5.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 673a330771..98e702e548 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -536,22 +536,22 @@ func cls() {
 	sc_put('\n')
 }
 
-func Trapsched() {
-	mcall(trapsched_m)
-}
+//func Trapsched() {
+//	mcall(trapsched_m)
+//}
 
 // called only once to setup
-func Trapinit() {
-	mcall(trapinit_m)
-}
+//func Trapinit() {
+//	mcall(trapinit_m)
+//}
 
 var hackmode int32
 var Halt uint32
 
 // wait until remove definition from proc.c
-//type spinlock_t struct {
-//	v	uint32
-//}
+type spinlock_t struct {
+	v	uint32
+}
 
 //go:nosplit
 func splock(l *spinlock_t) {

commit 57a9973bd03f6d002292cad5848253f3e1a90ebb
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 12 22:38:42 2016 -0500

    remove invariant-breaking write barriers
    
    concurrent collectors require write barriers. we need to be careful with them
    in uninterruptible code though. almost all of biscuit's runtime pointer writes
    do not reference heap allocated objects, so write barriers can be elided.
    
    go 1.5 works! that was much less painful than i thought it would be.
    
    still need to convert trap wake/sleep. then i will investigate our fancy new
    gc.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 09e0993c71..673a330771 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -193,10 +193,14 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 		Lcr3(p_pmap)
 	}
 	// set shadow pointers for user pmap so it isn't free'd out from under
-	// us if the process terminates soon
-	cpu.pmap = pmap
-	cpu.pms = pms
+	// us if the process terminates soon.
+	//cpu.pmap = pmap
+	//cpu.pms = pms
 	//cpu.pid = uintptr(pid)
+	// avoid write barriers since we are uninterruptible. the caller must
+	// also have these references anyway, so skipping them is ok.
+	*(*uintptr)(unsafe.Pointer(&cpu.pmap)) = uintptr(unsafe.Pointer(pmap))
+	*(*[3]uintptr)(unsafe.Pointer(&cpu.pms)) = *(*[3]uintptr)(unsafe.Pointer(&pms))
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually
@@ -208,8 +212,13 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	// during syscall exit/return. this is OK since sys5ABI defines the SSE
 	// registers to be caller-saved.
 	// XXX types
-	ct.user.tf = (*[TFSIZE]uintptr)(unsafe.Pointer(tf))
-	ct.user.fxbuf = (*[FXREGS]uintptr)(unsafe.Pointer(fxbuf))
+	//ct.user.tf = (*[TFSIZE]uintptr)(unsafe.Pointer(tf))
+	//ct.user.fxbuf = (*[FXREGS]uintptr)(unsafe.Pointer(fxbuf))
+
+	// avoid write barriers, see note above
+	*(*uintptr)(unsafe.Pointer(&ct.user.tf)) = uintptr(unsafe.Pointer(tf))
+	*(*uintptr)(unsafe.Pointer(&ct.user.fxbuf)) = uintptr(unsafe.Pointer(fxbuf))
+
 	intno, aux := _Userrun(tf, fastret)
 
 	Wrmsr(ia32_fs_base, kfsbase)
@@ -1881,7 +1890,11 @@ func sched_run(t *thread_t) {
 	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
 		pancake("thread not interurptible", 0)
 	}
-	Gscpu().mythread = t
+	// mythread never references a heap allocated object. avoid
+	// writebarrier since sched_run can be executed at any time, even when
+	// GC invariants do not hold (like when g.m.p == nil).
+	//Gscpu().mythread = t
+	*(*uintptr)(unsafe.Pointer(&Gscpu().mythread)) = uintptr(unsafe.Pointer(t))
 	fxrstor(&t.fx)
 	trapret(&t.tf, t.p_pmap)
 }
@@ -2276,7 +2289,10 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
 	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 8
 
-	gp.m = mp
+	// avoid write barrier for mp here since we have interrupts clear. Ms
+	// are always reachable from allm anyway. see comments in runtime2.go
+	//gp.m = mp
+	*(*uintptr)(unsafe.Pointer(&gp.m)) = uintptr(unsafe.Pointer(mp))
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))
 	mp.procid = uint64(ti)
 	mt.status = ST_RUNNABLE

commit b193f491121140c325fed031f424c3330a043829
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 12 12:20:38 2016 -0500

    1.5 updates
    
    one notable change: go1.5 no longer uses amd64 ELF TLS address of -16(%fs), but
    -8(%fs).

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f9fcfd2e03..09e0993c71 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -184,7 +184,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
 	// while other cpus execute user programs.
-	entersyscall()
+	entersyscall(0)
 	fl := Pushcli()
 	cpu := Gscpu()
 	ct := cpu.mythread
@@ -216,7 +216,7 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	ct.user.tf = nil
 	ct.user.fxbuf = nil
 	Popcli(fl)
-	exitsyscall()
+	exitsyscall(0)
 	return intno, aux
 }
 
@@ -857,9 +857,10 @@ func seg_setup() {
 	lgdt(p)
 
 	// now that we have a GDT, setup tls for the first thread.
-	// elf tls specification defines user tls at -16(%fs)
+	// elf tls specification defines user tls at -16(%fs). go1.5 uses
+	// -8(%fs) though.
 	t := uintptr(unsafe.Pointer(&tls0[0]))
-	tlsaddr := int(t + 16)
+	tlsaddr := int(t + 8)
 	// we must set fs/gs at least once before we use the MSRs to change
 	// their base address. the MSRs write directly to hidden segment
 	// descriptor cache, and if we don't explicitly fill the segment
@@ -2273,7 +2274,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	mt.tf[TF_RSP] = rsp
 	mt.tf[TF_RIP] = cloneaddr
 	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
-	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 16
+	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 8
 
 	gp.m = mp
 	mp.tls[0] = uintptr(unsafe.Pointer(gp))

commit d2eb6433fbeb4c4e81dbda541959e71e03c244c6
Merge: e2f500e61c 7bc40ffb05
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Mar 13 11:21:11 2016 -0400

    Merge tag 'go1.6' into tmerge
    
    Conflicts:
            .gitignore
            src/cmd/dist/build.c
            src/cmd/ld/lib.c
            src/liblink/obj6.c
            src/runtime/mgc0.c
            src/runtime/os_linux.c
            src/runtime/os_linux.go
            src/runtime/proc.c
            src/runtime/runtime.c
            src/runtime/stack.h
            src/runtime/sys_linux_amd64.s
            src/runtime/traceback.go

commit e2f500e61c3c4075737c89bcab37cdf66cc167df
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 12 15:41:30 2016 -0500

    avoid pmap freeing race
    
    a cpu's currently loaded pmap could be GC'ed after it sets the shadow pointers
    to a new pmap but before it loads the new pmap in cr3.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f251d53f5c..87916f7c48 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -169,14 +169,14 @@ func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
 	cpu := Gscpu()
 	ct := cpu.mythread
 
+	if Rcr3() != p_pmap {
+		Lcr3(p_pmap)
+	}
 	// set shadow pointers for user pmap so it isn't free'd out from under
 	// us if the process terminates soon
 	cpu.pmap = pmap
 	cpu.pms = pms
 	//cpu.pid = uintptr(pid)
-	if Rcr3() != p_pmap {
-		Lcr3(p_pmap)
-	}
 
 	// if doing a fast return after a syscall, we need to restore some user
 	// state manually

commit 9dc22ee5cffa389a199e69a2756c6c6e4fbc4db5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 11 15:32:55 2016 -0500

    final cleanup
    
    i cannot convert the trap sleep/wake code to go since the 1.4 scheduler is in
    C.
    
    now for the merge!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index cc529ce45f..f251d53f5c 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -21,6 +21,7 @@ func Cli()
 func clone_call(uintptr)
 func cpu_halt(uintptr)
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
+func fakesig(int32, unsafe.Pointer, *ucontext_t)
 func finit()
 func fs_null()
 func fxrstor(*[FXREGS]uintptr)
@@ -28,6 +29,7 @@ func fxsave(*[FXREGS]uintptr)
 func _Gscpu() *cpu_t
 func gs_null()
 func htpause()
+func invlpg(uintptr)
 func Inb(uint16) uint
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
@@ -45,6 +47,7 @@ func Outsl(int, unsafe.Pointer, int)
 func Outw(int, int)
 func Popcli(int)
 func Pushcli() int
+func rflags() uintptr
 func Rcr0() uintptr
 func Rcr2() uintptr
 func Rcr3() uintptr
@@ -150,7 +153,7 @@ const(
 //go:nosplit
 func Gscpu() *cpu_t {
 	if rflags() & TF_FL_IF != 0 {
-		G_pancake("must not be interruptible", 0)
+		pancake("must not be interruptible", 0)
 	}
 	return _Gscpu()
 }
@@ -513,18 +516,9 @@ func Trapinit() {
 	mcall(trapinit_m)
 }
 
-// G_ prefix means a function had to have both C and Go versions while the
-// conversion is underway. remove prefix afterwards. we need two versions of
-// functions that take a string as an argument since string literals are
-// different data types in C and Go.
+var hackmode int32
 var Halt uint32
 
-// TEMPORARY CRAP
-func _pmsg(*int8)
-func invlpg(uintptr)
-func rflags() uintptr
-func fakesig(int32, unsafe.Pointer, *ucontext_t)
-
 // wait until remove definition from proc.c
 //type spinlock_t struct {
 //	v	uint32
@@ -553,7 +547,7 @@ func spunlock(l *spinlock_t) {
 var pmsglock = &spinlock_t{}
 
 //go:nosplit
-func _G_pmsg(msg string) {
+func _pmsg(msg string) {
 	putch(' ');
 	// can't use range since it results in calls stringiter2 which has the
 	// stack splitting proglogue
@@ -564,10 +558,10 @@ func _G_pmsg(msg string) {
 
 // msg must be utf-8 string
 //go:nosplit
-func G_pmsg(msg string) {
+func pmsg(msg string) {
 	fl := Pushcli()
 	splock(pmsglock)
-	_G_pmsg(msg)
+	_pmsg(msg)
 	spunlock(pmsglock)
 	Popcli(fl)
 }
@@ -631,28 +625,13 @@ func cpupnum(rip uintptr) {
 	}
 }
 
-func pmsg(msg *int8)
-
 //go:nosplit
-func pancake(msg *int8, addr uintptr) {
+func pancake(msg string, addr uintptr) {
 	Pushcli()
 	atomicstore(&Halt, 1)
 	_pmsg(msg)
 	_pnum(addr)
-	_G_pmsg("PANCAKE")
-	for {
-		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
-		*p = 0x1400 | 'F'
-	}
-}
-
-//go:nosplit
-func G_pancake(msg string, addr uintptr) {
-	Pushcli()
-	atomicstore(&Halt, 1)
-	_G_pmsg(msg)
-	_pnum(addr)
-	_G_pmsg("PANCAKE")
+	_pmsg("PANCAKE")
 	for {
 		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
 		*p = 0x1400 | 'F'
@@ -665,14 +644,14 @@ var gostr = []int8{'g', 'o', 0}
 func chkalign(_p unsafe.Pointer, n uintptr) {
 	p := uintptr(_p)
 	if p & (n - 1) != 0 {
-		G_pancake("not aligned", p)
+		pancake("not aligned", p)
 	}
 }
 
 //go:nosplit
 func chksize(n uintptr, exp uintptr) {
 	if n != exp {
-		G_pancake("size mismatch", n)
+		pancake("size mismatch", n)
 	}
 }
 
@@ -1067,11 +1046,11 @@ func caddr(l4 uintptr, ppd uintptr, pd uintptr, pt uintptr,
 func pgdir_walk(_va uintptr, create bool) *uintptr {
 	v := pgrounddown(_va)
 	if v == 0 && create {
-		G_pancake("map zero pg", _va)
+		pancake("map zero pg", _va)
 	}
 	slot0 := pml4x(v)
 	if slot0 == VREC {
-		G_pancake("map in VREC", _va)
+		pancake("map in VREC", _va)
 	}
 	pml4 := caddr(VREC, VREC, VREC, VREC, slot0)
 	return pgdir_walk1(pml4, slotnext(v), create)
@@ -1101,7 +1080,7 @@ func zero_phys(_phys uintptr) {
 	rec := caddr(VREC, VREC, VREC, VREC, VTEMP)
 	pml4 := (*uintptr)(unsafe.Pointer(rec))
 	if *pml4 & PTE_P != 0 {
-		G_pancake("vtemp in use", *pml4)
+		pancake("vtemp in use", *pml4)
 	}
 	phys := pgrounddown(_phys)
 	*pml4 = phys | PTE_P | PTE_W
@@ -1181,10 +1160,10 @@ func phys_init() {
 		}
 	}
 	if !found {
-		G_pancake("e820 problems", pgfirst)
+		pancake("e820 problems", pgfirst)
 	}
 	if pgfirst & PGOFFMASK != 0 {
-		G_pancake("pgfist not aligned", pgfirst)
+		pancake("pgfist not aligned", pgfirst)
 	}
 }
 
@@ -1195,7 +1174,7 @@ func get_pg() uintptr {
 	}
 	pgfirst = skip_bad(pgfirst)
 	if pgfirst >= pglast {
-		G_pancake("oom", pglast)
+		pancake("oom", pglast)
 	}
 	ret := pgfirst
 	pgfirst += PGSIZE
@@ -1207,7 +1186,7 @@ func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	pte := pgdir_walk(va, true)
 	old := *pte
 	if old & PTE_P != 0 && fempty {
-		G_pancake("expected empty pte", old)
+		pancake("expected empty pte", old)
 	}
 	p_pg := get_pg()
 	zero_phys(p_pg)
@@ -1270,7 +1249,7 @@ var _lapaddr uintptr
 //go:nosplit
 func rlap(reg uint) uint32 {
 	if _lapaddr == 0 {
-		G_pancake("lapic not init", 0)
+		pancake("lapic not init", 0)
 	}
 	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
 	return atomicload(&lpg[reg])
@@ -1279,7 +1258,7 @@ func rlap(reg uint) uint32 {
 //go:nosplit
 func wlap(reg uint, val uint32) {
 	if _lapaddr == 0 {
-		G_pancake("lapic not init", 0)
+		pancake("lapic not init", 0)
 	}
 	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
 	lpg[reg] = val
@@ -1288,10 +1267,10 @@ func wlap(reg uint, val uint32) {
 //go:nosplit
 func lap_id() uint32 {
 	if rflags() & TF_FL_IF != 0 {
-		G_pancake("interrupts must be cleared", 0)
+		pancake("interrupts must be cleared", 0)
 	}
 	if _lapaddr == 0 {
-		G_pancake("lapic not init", 0)
+		pancake("lapic not init", 0)
 	}
 	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
 	return lpg[LAPID] >> 24
@@ -1300,7 +1279,7 @@ func lap_id() uint32 {
 //go:nosplit
 func lap_eoi() {
 	if _lapaddr == 0 {
-		G_pancake("lapic not init", 0)
+		pancake("lapic not init", 0)
 	}
 	wlap(LAPEOI, 0)
 }
@@ -1365,7 +1344,7 @@ func lapic_setup(calibrate bool) {
 		// map lapic IO mem
 		pte := pgdir_walk(la, false)
 		if pte != nil && *pte & PTE_P != 0 {
-			G_pancake("lapic mem already mapped", 0)
+			pancake("lapic mem already mapped", 0)
 		}
 	}
 
@@ -1375,7 +1354,7 @@ func lapic_setup(calibrate bool) {
 
 	lver := rlap(LAPVER)
 	if lver < 0x10 {
-		G_pancake("82489dx not supported", uintptr(lver))
+		pancake("82489dx not supported", uintptr(lver))
 	}
 
 	// enable lapic, set spurious int vector
@@ -1408,19 +1387,19 @@ func lapic_setup(calibrate bool) {
 
 		lapend := rlap(LAPCCNT)
 		if lapend > lapstart {
-			G_pancake("lapic timer wrapped?", uintptr(lapend))
+			pancake("lapic timer wrapped?", uintptr(lapend))
 		}
 		lapelapsed := (lapstart - lapend)*uint32(frac)
 		cycelapsed := (Rdtsc() - cycstart)*uint64(frac)
-		G_pmsg("LAPIC Mhz:")
+		pmsg("LAPIC Mhz:")
 		pnum(uintptr(lapelapsed/(1000 * 1000)))
-		G_pmsg("\n")
+		pmsg("\n")
 		_lapic_quantum = lapelapsed / HZ
 
-		G_pmsg("CPU Mhz:")
+		pmsg("CPU Mhz:")
 		Cpumhz = uint(cycelapsed/(1000 * 1000))
 		pnum(uintptr(Cpumhz))
-		G_pmsg("\n")
+		pmsg("\n")
 		Pspercycle = uint(1000000000000/cycelapsed)
 
 		pit_disable()
@@ -1443,21 +1422,21 @@ func lapic_setup(calibrate bool) {
 	ia32_apic_base := 0x1b
 	reg := uintptr(Rdmsr(ia32_apic_base))
 	if reg & (1 << 11) == 0 {
-		G_pancake("lapic disabled?", reg)
+		pancake("lapic disabled?", reg)
 	}
 	if (reg >> 12) != 0xfee00 {
-		G_pancake("weird base addr?", reg >> 12)
+		pancake("weird base addr?", reg >> 12)
 	}
 
 	lreg := rlap(LVSPUR)
 	if lreg & (1 << 12) != 0 {
-		G_pmsg("EOI broadcast surpression\n")
+		pmsg("EOI broadcast surpression\n")
 	}
 	if lreg & (1 << 9) != 0 {
-		G_pmsg("focus processor checking\n")
+		pmsg("focus processor checking\n")
 	}
 	if lreg & (1 << 8) == 0 {
-		G_pmsg("apic disabled\n")
+		pmsg("apic disabled\n")
 	}
 }
 
@@ -1502,13 +1481,13 @@ func Ap_setup(cpunum uint) {
 	fl := Pushcli()
 
 	splock(pmsglock)
-	_G_pmsg("cpu")
+	_pmsg("cpu")
 	_pnum(uintptr(cpunum))
-	_G_pmsg("joined\n")
+	_pmsg("joined\n")
 	spunlock(pmsglock)
 
 	if cpunum >= uint(MAXCPUS) {
-		G_pancake("nice computer!", uintptr(cpunum))
+		pancake("nice computer!", uintptr(cpunum))
 	}
 	fpuinit(false)
 	lapic_setup(false)
@@ -1516,7 +1495,7 @@ func Ap_setup(cpunum uint) {
 	sysc_setup(myrsp)
 	mycpu := &cpus[cpunum]
 	if mycpu.num != 0 {
-		G_pancake("cpu id conflict", uintptr(mycpu.num))
+		pancake("cpu id conflict", uintptr(mycpu.num))
 	}
 	fs_null()
 	gs_set(mycpu)
@@ -1598,7 +1577,7 @@ func tlb_shootdown() {
 	dur := (*uint64)(unsafe.Pointer(&tlbshoot_wait))
 	v := xadd64(dur, -1)
 	if v < 0 {
-		G_pancake("shootwait < 0", uintptr(v))
+		pancake("shootwait < 0", uintptr(v))
 	}
 	sched_resume(ct)
 }
@@ -1617,7 +1596,7 @@ func preemptok() {
 	gp := getg()
 	StackPreempt := uintptr(0xfffffffffffffade)
 	if gp.stackguard0 == StackPreempt {
-		G_pmsg("!")
+		pmsg("!")
 		// call function with stack splitting prologue
 		_dummy()
 	}
@@ -1679,7 +1658,7 @@ func Install_traphandler(newtrap func(*[TFSIZE]uintptr)) {
 //go:nosplit
 func stack_dump(rsp uintptr) {
 	pte := pgdir_walk(rsp, false)
-	_G_pmsg("STACK DUMP\n")
+	_pmsg("STACK DUMP\n")
 	if pte != nil && *pte & PTE_P != 0 {
 		pc := 0
 		p := rsp
@@ -1690,13 +1669,13 @@ func stack_dump(rsp uintptr) {
 				p += 8
 				_pnum(n)
 				if (pc % 4) == 0 {
-					_G_pmsg("\n")
+					_pmsg("\n")
 				}
 				pc++
 			}
 		}
 	} else {
-		_G_pmsg("bad stack")
+		_pmsg("bad stack")
 		_pnum(rsp)
 	}
 }
@@ -1704,21 +1683,21 @@ func stack_dump(rsp uintptr) {
 //go:nosplit
 func kernel_fault(tf *[TFSIZE]uintptr) {
 	trapno := tf[TF_TRAPNO]
-	_G_pmsg("trap frame at")
+	_pmsg("trap frame at")
 	_pnum(uintptr(unsafe.Pointer(tf)))
-	_G_pmsg("trapno")
+	_pmsg("trapno")
 	_pnum(trapno)
 	rip := tf[TF_RIP]
-	_G_pmsg("rip")
+	_pmsg("rip")
 	_pnum(rip)
 	if trapno == TRAP_PGFAULT {
 		cr2 := Rcr2()
-		_G_pmsg("cr2")
+		_pmsg("cr2")
 		_pnum(cr2)
 	}
 	rsp := tf[TF_RSP]
 	stack_dump(rsp)
-	G_pancake("kernel fault", trapno)
+	pancake("kernel fault", trapno)
 }
 
 // XXX
@@ -1744,7 +1723,7 @@ func trap(tf *[TFSIZE]uintptr) {
 	ct := Gscpu().mythread
 
 	if rflags() & TF_FL_IF != 0 {
-		G_pancake("ints enabled in trap", 0)
+		pancake("ints enabled in trap", 0)
 	}
 
 	if Halt != 0 {
@@ -1775,7 +1754,7 @@ func trap(tf *[TFSIZE]uintptr) {
 			ct.tf[TF_RBX] = Rcr2()
 			// XXXPANIC
 			if trapno == TRAP_YIELD || trapno == TRAP_SIGRET {
-				G_pancake("nyet", trapno)
+				pancake("nyet", trapno)
 			}
 			// XXX fix this using RIP method
 			// if we are unlucky enough for a timer int to come in
@@ -1830,7 +1809,7 @@ func trap(tf *[TFSIZE]uintptr) {
 			// handle user traps
 			_newtrap(tf)
 		} else {
-			G_pancake("IRQ without ntrap", trapno)
+			pancake("IRQ without ntrap", trapno)
 		}
 		sched_resume(ct)
 	} else if is_cpuex(trapno) {
@@ -1845,10 +1824,10 @@ func trap(tf *[TFSIZE]uintptr) {
 		perfmask()
 		sched_resume(ct)
 	} else {
-		G_pancake("unexpected int", trapno)
+		pancake("unexpected int", trapno)
 	}
 	// not reached
-	G_pancake("no returning", 0)
+	pancake("no returning", 0)
 }
 
 //go:nosplit
@@ -1864,10 +1843,10 @@ func is_cpuex(trapno uintptr) bool {
 //go:nosplit
 func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
-		G_pancake("must not be interruptible", 0)
+		pancake("must not be interruptible", 0)
 	}
 	if threadlock.v == 0 {
-		G_pancake("must hold threadlock", 0)
+		pancake("must hold threadlock", 0)
 	}
 }
 
@@ -1879,7 +1858,7 @@ func sched_halt() {
 //go:nosplit
 func sched_run(t *thread_t) {
 	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
-		G_pancake("thread not interurptible", 0)
+		pancake("thread not interurptible", 0)
 	}
 	Gscpu().mythread = t
 	fxrstor(&t.fx)
@@ -2024,11 +2003,11 @@ type ucontext_t struct {
 //go:nosplit
 func mksig(t *thread_t, signo int32) {
 	if t.sigstack == 0 {
-		G_pancake("no sig stack", t.sigstack)
+		pancake("no sig stack", t.sigstack)
 	}
 	// save old context for sigret
 	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
-		G_pancake("thread uninterruptible?", 0)
+		pancake("thread uninterruptible?", 0)
 	}
 	t.sigtf = t.tf
 	t.sigfx = t.fx
@@ -2074,13 +2053,13 @@ func sigsim(signo int32, si unsafe.Pointer, ctx *ucontext_t) {
 //go:nosplit
 func sigret(t *thread_t) {
 	if t.status != ST_RUNNING {
-		G_pancake("uh oh!", uintptr(t.status))
+		pancake("uh oh!", uintptr(t.status))
 	}
 	t.tf = t.sigtf
 	t.fx = t.sigfx
 	t.doingsig = 0
 	if t.status != ST_RUNNING {
-		G_pancake("wut", uintptr(t.status))
+		pancake("wut", uintptr(t.status))
 	}
 	sched_run(t)
 }
@@ -2163,12 +2142,12 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 	}
 	vaend = caddr(VUEND, 0, 0, 0, 0)
 	if va >= vaend || va + sz >= vaend {
-		G_pancake("va space exhausted", va)
+		pancake("va space exhausted", va)
 	}
 
 	t = MAP_ANON | MAP_PRIVATE
 	if flags & t != t {
-		G_pancake("unexpected flags", flags)
+		pancake("unexpected flags", flags)
 	}
 	perms = PTE_P
 	if prot == PROT_NONE {
@@ -2187,7 +2166,7 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 			pml4 := caddr(VREC, VREC, VREC, VREC, sidx)
 			pml4e := (*uintptr)(unsafe.Pointer(pml4))
 			if *pml4e & PTE_P == 0 {
-				G_pancake("new pml4 entry to kernel pmap", va)
+				pancake("new pml4 entry to kernel pmap", va)
 			}
 		}
 	}
@@ -2211,7 +2190,7 @@ func hack_munmap(v, _sz uintptr) {
 		va := v + i
 		pte := pgdir_walk(va, false)
 		if pml4x(va) >= VUEND {
-			G_pancake("high unmap", va)
+			pancake("high unmap", va)
 		}
 		// XXX goodbye, memory
 		if pte != nil && *pte & PTE_P != 0 {
@@ -2221,7 +2200,7 @@ func hack_munmap(v, _sz uintptr) {
 			invlpg(va)
 		}
 	}
-	G_pmsg("POOF\n")
+	pmsg("POOF\n")
 	spunlock(maplock)
 	Popcli(fl)
 }
@@ -2233,13 +2212,13 @@ func thread_avail() int {
 			return i
 		}
 	}
-	G_pancake("no available threads", maxthreads)
+	pancake("no available threads", maxthreads)
 	return -1
 }
 
 func clone_wrap(rip uintptr) {
 	clone_call(rip)
-	G_pancake("clone_wrap returned", 0)
+	pancake("clone_wrap returned", 0)
 }
 
 func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
@@ -2251,7 +2230,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 	chk := uint32(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
 	    CLONE_THREAD)
 	if flags != chk {
-		G_pancake("unexpected clone args", uintptr(flags))
+		pancake("unexpected clone args", uintptr(flags))
 	}
 	var dur func(uintptr)
 	dur = clone_wrap
@@ -2291,7 +2270,7 @@ func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
 func hack_setitimer(timer uint32, new, old *itimerval) {
 	TIMER_PROF := uint32(2)
 	if timer != TIMER_PROF {
-		G_pancake("weird timer", uintptr(timer))
+		pancake("weird timer", uintptr(timer))
 	}
 
 	fl := Pushcli()
@@ -2321,7 +2300,7 @@ func hack_sigaltstack(new, old *sigaltstackt) {
 
 func hack_write(fd int, bufn uintptr, sz uint32) int64 {
 	if fd != 1 && fd != 2 {
-		G_pancake("unexpected fd", uintptr(fd))
+		pancake("unexpected fd", uintptr(fd))
 	}
 	fl := Pushcli()
 	splock(pmsglock)
@@ -2358,11 +2337,11 @@ func hack_syscall(trap, a1, a2, a3 int64) (int64, int64, int64) {
 	case 2:
 		enoent := int64(-2)
 		if !cstrmatch(uintptr(a1), fnwhite) {
-			G_pancake("unexpected open", 0)
+			pancake("unexpected open", 0)
 		}
 		return 0, 0, enoent
 	default:
-		G_pancake("unexpected syscall", uintptr(trap))
+		pancake("unexpected syscall", uintptr(trap))
 	}
 	// not reached
 	return 0, 0, -1
@@ -2428,7 +2407,7 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 		Sti()
 		ret = woke
 	default:
-		G_pancake("unexpected futex op", uintptr(op))
+		pancake("unexpected futex op", uintptr(op))
 	}
 	return int64(ret)
 }
@@ -2445,9 +2424,9 @@ func hack_usleep(delay int64) {
 func hack_exit(code int32) {
 	Cli()
 	Gscpu().mythread.status = ST_INVALID
-	G_pmsg("exit with code")
+	pmsg("exit with code")
 	pnum(uintptr(code))
-	G_pmsg(".\nhalting\n")
+	pmsg(".\nhalting\n")
 	atomicstore(&Halt, 1)
 	for {
 	}
@@ -2479,7 +2458,7 @@ func Nanotime() int {
 
 // useful for basic tests of filesystem durability
 func Crash() {
-	G_pmsg("CRASH!\n")
+	pmsg("CRASH!\n")
 	atomicstore(&Halt, 1)
 	for {
 	}

commit 458bebe13f7cf51f41d66388d18ddeac670212f5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 11 11:23:57 2016 -0500

    finish and cleanup of main C file
    
    just trap{sched,wake} C code remains.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fa23dd05f2..cc529ce45f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -15,64 +15,57 @@ func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
 func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
-func trapsched_m(*g)
-func trapinit_m(*g)
 
+// runtime/asm_amd64.s
 func Cli()
+func clone_call(uintptr)
+func cpu_halt(uintptr)
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
-func Invlpg(unsafe.Pointer)
-func Kpmap() *[512]int
-func Kpmap_p() int
-func Lcr3(uintptr)
+func finit()
+func fs_null()
+func fxrstor(*[FXREGS]uintptr)
+func fxsave(*[FXREGS]uintptr)
+func _Gscpu() *cpu_t
+func gs_null()
+func htpause()
+func Inb(uint16) uint
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
+func Invlpg(unsafe.Pointer)
+func Lcr0(uintptr)
+func Lcr3(uintptr)
+func Lcr4(uintptr)
+func lgdt(pdesc_t)
+func lidt(pdesc_t)
+func ltr(uint)
+func mktrap(int)
 func Outb(uint16, uint8)
-func Inb(uint16) uint
-func Outw(int, int)
 func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
-func Pmsga(*uint8, int, int8)
-func Pnum(int)
-func Kreset()
-func Ktime() int
-func Rdtsc() uint64
+func Outw(int, int)
+func Popcli(int)
+func Pushcli() int
+func Rcr0() uintptr
 func Rcr2() uintptr
 func Rcr3() uintptr
 func Rcr4() uintptr
+func Rdmsr(int) int
+func Rdtsc() uint64
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
-func Sti()
-func Vtop(*[512]int) int
-
-func Crash()
-func Fnaddr(func()) uintptr
-func Fnaddri(func(uint)) uintptr
-func Trapwake()
-
-// runtime/asm_amd64.s
-func htpause()
-func finit()
-func fs_null()
-func fxsave(*[FXREGS]uintptr)
-func gs_null()
-func lgdt(pdesc_t)
-func lidt(pdesc_t)
-func cli()
-func sti()
-func ltr(uint)
-func rcr0() uintptr
-func rcr4() uintptr
-func lcr0(uintptr)
-func lcr4(uintptr)
-func clone_call(uintptr)
-func cpu_halt(uintptr)
-func fxrstor(*[FXREGS]uintptr)
-func trapret(*[TFSIZE]uintptr, uintptr)
-func mktrap(int)
 func stackcheck()
+func Sti()
 func _sysentry()
 func _trapret(*[TFSIZE]uintptr)
+func trapret(*[TFSIZE]uintptr, uintptr)
 func _userint()
+func _Userrun(*[24]int, bool) (int, int)
+func Wrmsr(int, int)
+
+// proc.c
+func Trapwake()
+func trapsched_m(*g)
+func trapinit_m(*g)
 
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
@@ -154,15 +147,6 @@ const(
     TF_FL_IF     uintptr = 1 << 9
 )
 
-func Pushcli() int
-func Popcli(int)
-func _Gscpu() *cpu_t
-func Rdmsr(int) int
-func Wrmsr(int, int)
-func _Userrun(*[24]int, bool) (int, int)
-
-func Cprint(byte, int)
-
 //go:nosplit
 func Gscpu() *cpu_t {
 	if rflags() & TF_FL_IF != 0 {
@@ -291,7 +275,7 @@ func _consumelbr() {
 		Wrmsr(lastbranch_0_from_ip + cur, 0)
 		Wrmsr(lastbranch_0_to_ip + cur, 0)
 		if idx + 2*i + 1 >= len(nmiprof.buf) {
-			Cprint('!', 1)
+			Cpuprint('!', 1)
 			break
 		}
 		nmiprof.buf[idx+2*i] = from
@@ -539,7 +523,6 @@ var Halt uint32
 func _pmsg(*int8)
 func invlpg(uintptr)
 func rflags() uintptr
-//func timetick(*thread_t)
 func fakesig(int32, unsafe.Pointer, *ucontext_t)
 
 // wait until remove definition from proc.c
@@ -611,6 +594,18 @@ func pnum(n uintptr) {
 	Popcli(fl)
 }
 
+func Pmsga(_p *uint8, c int, attr int8) {
+	pn := uintptr(unsafe.Pointer(_p))
+	fl := Pushcli()
+	splock(pmsglock)
+	for i := uintptr(0); i < uintptr(c); i++ {
+		p := (*int8)(unsafe.Pointer(pn+i))
+		putcha(*p, attr)
+	}
+	spunlock(pmsglock)
+	Popcli(fl)
+}
+
 var _cpuattrs [MAXCPUS]uint16
 
 //go:nosplit
@@ -640,7 +635,7 @@ func pmsg(msg *int8)
 
 //go:nosplit
 func pancake(msg *int8, addr uintptr) {
-	cli()
+	Pushcli()
 	atomicstore(&Halt, 1)
 	_pmsg(msg)
 	_pnum(addr)
@@ -653,7 +648,7 @@ func pancake(msg *int8, addr uintptr) {
 
 //go:nosplit
 func G_pancake(msg string, addr uintptr) {
-	cli()
+	Pushcli()
 	atomicstore(&Halt, 1)
 	_G_pmsg(msg)
 	_pnum(addr)
@@ -664,6 +659,7 @@ func G_pancake(msg string, addr uintptr) {
 	}
 }
 
+var gostr = []int8{'g', 'o', 0}
 
 //go:nosplit
 func chkalign(_p unsafe.Pointer, n uintptr) {
@@ -1038,6 +1034,11 @@ func pml4x(va uintptr) uintptr {
 	return (va >> 39) & 0x1ff
 }
 
+//go:nosplit
+func pte_addr(x uintptr) uintptr {
+	return x &^ ((1 << 12) - 1)
+}
+
 //go:nosplit
 func slotnext(va uintptr) uintptr {
 	return ((va << 9) & ((1 << 48) - 1))
@@ -1223,17 +1224,17 @@ var fxinit [FXREGS]uintptr
 //go:nosplit
 func fpuinit(amfirst bool) {
 	finit()
-	cr0 := rcr0()
+	cr0 := Rcr0()
 	// clear EM
 	cr0 &^= (1 << 2)
 	// set MP
 	cr0 |= 1 << 1
-	lcr0(cr0);
+	Lcr0(cr0);
 
-	cr4 := rcr4()
+	cr4 := Rcr4()
 	// set OSFXSR
 	cr4 |= 1 << 9
-	lcr4(cr4);
+	Lcr4(cr4);
 
 	if amfirst {
 		chkalign(unsafe.Pointer(&fxinit[0]), 16)
@@ -2380,7 +2381,7 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 	ret := 0
 	switch op {
 	case FUTEX_WAIT:
-		cli()
+		Cli()
 		splock(futexlock)
 		dosleep := *uaddr == val
 		if dosleep {
@@ -2396,18 +2397,18 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 			mktrap(TRAP_YIELD)
 			// scheduler unlocks futexlock and returns with
 			// interrupts enabled...
-			cli()
+			Cli()
 			ret = Gscpu().mythread.sleepret
-			sti()
+			Sti()
 		} else {
 			spunlock(futexlock)
-			sti()
+			Sti()
 			eagain := -11
 			ret = eagain
 		}
 	case FUTEX_WAKE:
 		woke := 0
-		cli()
+		Cli()
 		splock(futexlock)
 		splock(threadlock)
 		for i := 0; i < maxthreads && val > 0; i++ {
@@ -2424,7 +2425,7 @@ func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
 		}
 		spunlock(threadlock)
 		spunlock(futexlock)
-		sti()
+		Sti()
 		ret = woke
 	default:
 		G_pancake("unexpected futex op", uintptr(op))
@@ -2442,7 +2443,7 @@ func hack_usleep(delay int64) {
 }
 
 func hack_exit(code int32) {
-	cli()
+	Cli()
 	Gscpu().mythread.status = ST_INVALID
 	G_pmsg("exit with code")
 	pnum(uintptr(code))
@@ -2459,9 +2460,38 @@ func hack_nanotime() int {
 	return int(cyc*Pspercycle/1000)
 }
 
+func Vtop(va unsafe.Pointer) (uintptr, bool) {
+	van := uintptr(va)
+	pte := pgdir_walk(van, false)
+	if pte == nil || *pte & PTE_P == 0 {
+		return 0, false
+	}
+	base := pte_addr(*pte)
+	return base + (van & PGOFFMASK), true
+}
+
 // XXX also called in interupt context; remove when trapstub is moved into
 // runtime
 //go:nosplit
 func Nanotime() int {
 	return hack_nanotime()
 }
+
+// useful for basic tests of filesystem durability
+func Crash() {
+	G_pmsg("CRASH!\n")
+	atomicstore(&Halt, 1)
+	for {
+	}
+}
+
+// XXX also called in interupt context; remove when trapstub is moved into
+// runtime
+//go:nosplit
+func Pnum(n int) {
+	pnum(uintptr(n))
+}
+
+func Kpmap_p() uintptr {
+	return p_kpmap
+}

commit 3b2847880ad9ea408a9c1c3405a6dfb8dc3974b0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 10 23:18:13 2016 -0500

    complete bulk of the conversion
    
    SIGPROF simulation for profiling and various others. now only a dozen or so
    lines of C code remain. the C code has no knowledge of threads or cpus.  great!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ba843e2850..fa23dd05f2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -119,8 +119,6 @@ type thread_t struct {
 	user		tuser_t
 	sigtf		[TFSIZE]uintptr
 	sigfx		[FXREGS]uintptr
-	sigstatus	int
-	siglseepfor	int
 	status		int
 	doingsig	int
 	sigstack	uintptr
@@ -541,10 +539,8 @@ var Halt uint32
 func _pmsg(*int8)
 func invlpg(uintptr)
 func rflags() uintptr
-func kernel_fault(*[TFSIZE]uintptr)
-func timetick(*thread_t)
-func proftick()
-func sigret(*thread_t)
+//func timetick(*thread_t)
+func fakesig(int32, unsafe.Pointer, *ucontext_t)
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -615,6 +611,31 @@ func pnum(n uintptr) {
 	Popcli(fl)
 }
 
+var _cpuattrs [MAXCPUS]uint16
+
+//go:nosplit
+func Cpuprint(n uint16, row uintptr) {
+	p := uintptr(0xb8000)
+	num := Gscpu().num
+	p += uintptr(num) + row*80
+	attr := _cpuattrs[num]
+	_cpuattrs[num] += 0x100
+	*(*uint16)(unsafe.Pointer(p)) = attr | n
+}
+
+//go:nosplit
+func cpupnum(rip uintptr) {
+	for i := uintptr(0); i < 16; i++ {
+		c := uint16((rip >> i*4) & 0xf)
+		if c < 0xa {
+			c += '0'
+		} else {
+			c = 'a' + (c - 0xa)
+		}
+		Cpuprint(c, i)
+	}
+}
+
 func pmsg(msg *int8)
 
 //go:nosplit
@@ -1040,7 +1061,7 @@ func caddr(l4 uintptr, ppd uintptr, pd uintptr, pt uintptr,
 	return uintptr(ret)
 }
 
-// XXX XXX XXX get rid of create
+// XXX get rid of create
 //go:nosplit
 func pgdir_walk(_va uintptr, create bool) *uintptr {
 	v := pgrounddown(_va)
@@ -1440,6 +1461,13 @@ func lapic_setup(calibrate bool) {
 }
 
 func proc_setup() {
+	var dur func()
+	dur = _userint
+	_userintaddr = **(**uintptr)(unsafe.Pointer(&dur))
+	var dur2 func(int32, unsafe.Pointer, *ucontext_t)
+	dur2 = sigsim
+	_sigsimaddr = **(**uintptr)(unsafe.Pointer(&dur2))
+
 	chksize(TFSIZE*8, unsafe.Sizeof(threads[0].tf))
 	fpuinit(true)
 	// initialize the first thread: us
@@ -1525,9 +1553,6 @@ func sysc_setup(myrsp uintptr) {
 
 	sysenter_esp := 0x175
 	Wrmsr(sysenter_esp, int(myrsp))
-
-	dur = _userint
-	_userintaddr = **(**uintptr)(unsafe.Pointer(&dur))
 }
 
 var tlbshoot_wait uintptr
@@ -1642,6 +1667,7 @@ const (
 )
 
 var _userintaddr uintptr
+var _sigsimaddr uintptr
 var _newtrap func(*[TFSIZE]uintptr)
 
 // XXX remove newtrap?
@@ -1649,6 +1675,51 @@ func Install_traphandler(newtrap func(*[TFSIZE]uintptr)) {
 	_newtrap = newtrap
 }
 
+//go:nosplit
+func stack_dump(rsp uintptr) {
+	pte := pgdir_walk(rsp, false)
+	_G_pmsg("STACK DUMP\n")
+	if pte != nil && *pte & PTE_P != 0 {
+		pc := 0
+		p := rsp
+		for i := 0; i < 70; i++ {
+			pte = pgdir_walk(p, false)
+			if pte != nil && *pte & PTE_P != 0 {
+				n := *(*uintptr)(unsafe.Pointer(p))
+				p += 8
+				_pnum(n)
+				if (pc % 4) == 0 {
+					_G_pmsg("\n")
+				}
+				pc++
+			}
+		}
+	} else {
+		_G_pmsg("bad stack")
+		_pnum(rsp)
+	}
+}
+
+//go:nosplit
+func kernel_fault(tf *[TFSIZE]uintptr) {
+	trapno := tf[TF_TRAPNO]
+	_G_pmsg("trap frame at")
+	_pnum(uintptr(unsafe.Pointer(tf)))
+	_G_pmsg("trapno")
+	_pnum(trapno)
+	rip := tf[TF_RIP]
+	_G_pmsg("rip")
+	_pnum(rip)
+	if trapno == TRAP_PGFAULT {
+		cr2 := Rcr2()
+		_G_pmsg("cr2")
+		_pnum(cr2)
+	}
+	rsp := tf[TF_RSP]
+	stack_dump(rsp)
+	G_pancake("kernel fault", trapno)
+}
+
 // XXX
 // may want to only wakeup() on most timer ints since now there is more
 // overhead for timer ints during user time.
@@ -1716,7 +1787,7 @@ func trap(tf *[TFSIZE]uintptr) {
 			fxsave(&ct.fx)
 			ct.tf = *tf
 		}
-		timetick(ct)
+		//timetick(ct)
 	}
 
 	yielding := false
@@ -1799,17 +1870,6 @@ func _tchk() {
 	}
 }
 
-func thread_avail() int {
-	_tchk()
-	for i := range threads {
-		if threads[i].status == ST_INVALID {
-			return i
-		}
-	}
-	G_pancake("no available threads", maxthreads)
-	return -1
-}
-
 //go:nosplit
 func sched_halt() {
 	cpu_halt(Gscpu().rsp)
@@ -1877,6 +1937,156 @@ func yieldy() {
 	sched_halt()
 }
 
+// goprofiling is implemented by simulating the SIGPROF signal. when proftick
+// observes that enough time has elapsed, mksig() is used to deliver SIGPROF to
+// the runtime and things are setup so the runtime returns to sigsim().
+// sigsim() makes sure that, once the "signal" has been delivered, the runtime
+// thread restores its pre-signal context (signals must be taken on the
+// alternate stack since the signal handler pushes a lot of state to the stack
+// and may clobber a goroutine stack since goroutine stacks are small). for
+// simpliclity, sigsim() uses a special trap # (TRAP_SIGRET) to restore the
+// pre-signal context. it is easier using a (software) trap because a trap
+// switches to the interrupt stack where we can easily change the runnability
+// of the current thread.
+
+var _lastprof int
+
+//go:nosplit
+func proftick() {
+	// goprofile period = 10ms
+	profns := 10000000
+	n := hack_nanotime()
+
+	if n - _lastprof < profns {
+		return
+	}
+	_lastprof = n
+
+	for i := range threads {
+		// only do fake SIGPROF if we are already
+		t := &threads[i]
+		if t.prof.enabled == 0 || t.doingsig != 0 {
+			continue
+		}
+		// don't touch running threads
+		if t.status != ST_RUNNABLE {
+			continue
+		}
+		SIGPROF := int32(27)
+		mksig(t, SIGPROF)
+	}
+}
+
+// these are defined by linux since we lie to the go build system that we are
+// running on linux...
+type ucontext_t struct {
+	uc_flags	uintptr
+	uc_link		uintptr
+	uc_stack struct {
+		sp	uintptr
+		flags	int32
+		size	uint64
+	}
+	uc_mcontext struct {
+		r8	uintptr
+		r9	uintptr
+		r10	uintptr
+		r11	uintptr
+		r12	uintptr
+		r13	uintptr
+		r14	uintptr
+		r15	uintptr
+		rdi	uintptr
+		rsi	uintptr
+		rbp	uintptr
+		rbx	uintptr
+		rdx	uintptr
+		rax	uintptr
+		rcx	uintptr
+		rsp	uintptr
+		rip	uintptr
+		eflags	uintptr
+		cs	uint16
+		gs	uint16
+		fs	uint16
+		__pad0	uint16
+		err	uintptr
+		trapno	uintptr
+		oldmask	uintptr
+		cr2	uintptr
+		fpptr	uintptr
+		res	[8]uintptr
+	}
+	uc_sigmask	uintptr
+}
+
+//go:nosplit
+func mksig(t *thread_t, signo int32) {
+	if t.sigstack == 0 {
+		G_pancake("no sig stack", t.sigstack)
+	}
+	// save old context for sigret
+	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
+		G_pancake("thread uninterruptible?", 0)
+	}
+	t.sigtf = t.tf
+	t.sigfx = t.fx
+	t.status = ST_RUNNABLE
+	t.doingsig = 1
+
+	rsp := t.sigstack
+	ucsz := unsafe.Sizeof(ucontext_t{})
+	rsp -= ucsz
+	_ctxt := rsp
+	ctxt := (*ucontext_t)(unsafe.Pointer(_ctxt))
+
+	// the profiler only uses rip and rsp of the context...
+	memclr(unsafe.Pointer(_ctxt), ucsz)
+	ctxt.uc_mcontext.rip = t.tf[TF_RIP]
+	ctxt.uc_mcontext.rsp = t.tf[TF_RSP]
+
+	// simulate call to sigsim with args
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = _ctxt
+	// nil siginfo_t
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = 0
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = uintptr(signo)
+	// bad return addr shouldn't be reached
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = 0
+
+	t.tf[TF_RSP] = rsp
+	t.tf[TF_RIP] = _sigsimaddr
+}
+
+//go:nosplit
+func sigsim(signo int32, si unsafe.Pointer, ctx *ucontext_t) {
+	// the purpose of fakesig is to enter the runtime's usual signal
+	// handler from go code. the signal handler uses sys5abi, so fakesig
+	// converts between calling conventions.
+	fakesig(signo, nil, ctx)
+	mktrap(TRAP_SIGRET)
+}
+
+//go:nosplit
+func sigret(t *thread_t) {
+	if t.status != ST_RUNNING {
+		G_pancake("uh oh!", uintptr(t.status))
+	}
+	t.tf = t.sigtf
+	t.fx = t.sigfx
+	t.doingsig = 0
+	if t.status != ST_RUNNING {
+		G_pancake("wut", uintptr(t.status))
+	}
+	sched_run(t)
+}
+
+// if sigsim() is used to deliver signals other than SIGPROF, you will need to
+// construct siginfo_t and more of context.
+
 func find_empty(sz uintptr) uintptr {
 	v := caddr(0, 0, 0, 1, 0)
 	cantuse := uintptr(0xf0)
@@ -2015,6 +2225,17 @@ func hack_munmap(v, _sz uintptr) {
 	Popcli(fl)
 }
 
+func thread_avail() int {
+	_tchk()
+	for i := range threads {
+		if threads[i].status == ST_INVALID {
+			return i
+		}
+	}
+	G_pancake("no available threads", maxthreads)
+	return -1
+}
+
 func clone_wrap(rip uintptr) {
 	clone_call(rip)
 	G_pancake("clone_wrap returned", 0)

commit 19c7622688382187565f3028ca8b70f697663b07
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 10 11:32:26 2016 -0500

    convert trap()

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0fa3a7a596..ba843e2850 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -20,7 +20,6 @@ func trapinit_m(*g)
 
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
-func Install_traphandler(func(tf *[24]int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
@@ -50,10 +49,11 @@ func Fnaddr(func()) uintptr
 func Fnaddri(func(uint)) uintptr
 func Trapwake()
 
+// runtime/asm_amd64.s
 func htpause()
 func finit()
 func fs_null()
-func fxsave(*[fxwords]uintptr)
+func fxsave(*[FXREGS]uintptr)
 func gs_null()
 func lgdt(pdesc_t)
 func lidt(pdesc_t)
@@ -71,6 +71,8 @@ func trapret(*[TFSIZE]uintptr, uintptr)
 func mktrap(int)
 func stackcheck()
 func _sysentry()
+func _trapret(*[TFSIZE]uintptr)
+func _userint()
 
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
@@ -99,8 +101,8 @@ const MAXCPUS int = 32
 var cpus [MAXCPUS]cpu_t
 
 type tuser_t struct {
-	tf	uintptr
-	fxbuf	uintptr
+	tf	*[TFSIZE]uintptr
+	fxbuf	*[FXREGS]uintptr
 }
 
 type prof_t struct {
@@ -145,7 +147,7 @@ const(
   TF_RCX       = 14
   TF_RBX       = 15
   TF_RAX       = 16
-  TF_TRAP      = TFREGS
+  TF_TRAPNO    = TFREGS
   TF_RIP       = TFREGS + 2
   TF_CS        = TFREGS + 3
   TF_RSP       = TFREGS + 5
@@ -171,8 +173,8 @@ func Gscpu() *cpu_t {
 	return _Gscpu()
 }
 
-func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
-    pms []*[512]int, fastret bool) (int, int) {
+func Userrun(tf *[TFSIZE]int, fxbuf *[FXREGS]int, pmap *[512]int,
+    p_pmap uintptr, pms []*[512]int, fastret bool) (int, int) {
 
 	// {enter,exit}syscall() may not be worth the overhead. i believe the
 	// only benefit for biscuit is that cpus running in the kernel could GC
@@ -200,13 +202,14 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
 	// we only save/restore SSE registers on cpu exception/interrupt, not
 	// during syscall exit/return. this is OK since sys5ABI defines the SSE
 	// registers to be caller-saved.
-	ct.user.tf = uintptr(unsafe.Pointer(tf))
-	ct.user.fxbuf = uintptr(unsafe.Pointer(fxbuf))
+	// XXX types
+	ct.user.tf = (*[TFSIZE]uintptr)(unsafe.Pointer(tf))
+	ct.user.fxbuf = (*[FXREGS]uintptr)(unsafe.Pointer(fxbuf))
 	intno, aux := _Userrun(tf, fastret)
 
 	Wrmsr(ia32_fs_base, kfsbase)
-	ct.user.tf = 0
-	ct.user.fxbuf = 0
+	ct.user.tf = nil
+	ct.user.fxbuf = nil
 	Popcli(fl)
 	exitsyscall()
 	return intno, aux
@@ -538,6 +541,10 @@ var Halt uint32
 func _pmsg(*int8)
 func invlpg(uintptr)
 func rflags() uintptr
+func kernel_fault(*[TFSIZE]uintptr)
+func timetick(*thread_t)
+func proftick()
+func sigret(*thread_t)
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -1189,8 +1196,7 @@ func alloc_map(va uintptr, perms uintptr, fempty bool) {
 	}
 }
 
-const fxwords = 512/8
-var fxinit [fxwords]uintptr
+var fxinit [FXREGS]uintptr
 
 // nosplit because APs call this function before FS is setup
 //go:nosplit
@@ -1374,7 +1380,6 @@ func lapic_setup(calibrate bool) {
 		cycstart := Rdtsc()
 
 		frac := 10
-		// XXX only wait for 100ms instead of 1s
 		for i := 0; i < _pithz/frac; i++ {
 			pit_phasewait()
 		}
@@ -1520,6 +1525,9 @@ func sysc_setup(myrsp uintptr) {
 
 	sysenter_esp := 0x175
 	Wrmsr(sysenter_esp, int(myrsp))
+
+	dur = _userint
+	_userintaddr = **(**uintptr)(unsafe.Pointer(&dur))
 }
 
 var tlbshoot_wait uintptr
@@ -1550,7 +1558,6 @@ func Tlbwait(gen uint64) {
 // must be nosplit since called at interrupt time
 //go:nosplit
 func tlb_shootdown() {
-	lap_eoi()
 	ct := Gscpu().mythread
 	if ct != nil && ct.p_pmap == tlbshoot_pmap {
 		// the TLB was already invalidated since trap() currently
@@ -1567,11 +1574,7 @@ func tlb_shootdown() {
 	if v < 0 {
 		G_pancake("shootwait < 0", uintptr(v))
 	}
-	if ct != nil {
-		sched_run(ct)
-	} else {
-		sched_halt()
-	}
+	sched_resume(ct)
 }
 
 // this function checks to see if another thread is trying to preempt this
@@ -1614,6 +1617,7 @@ const (
 	TRAP_TLBSHOOT	= 70
 	TRAP_SIGRET	= 71
 	TRAP_PERFMASK	= 72
+	IRQ_BASE	= 32
 )
 
 var threadlock = &spinlock_t{}
@@ -1637,6 +1641,154 @@ const (
 	HZ	= 100
 )
 
+var _userintaddr uintptr
+var _newtrap func(*[TFSIZE]uintptr)
+
+// XXX remove newtrap?
+func Install_traphandler(newtrap func(*[TFSIZE]uintptr)) {
+	_newtrap = newtrap
+}
+
+// XXX
+// may want to only wakeup() on most timer ints since now there is more
+// overhead for timer ints during user time.
+//go:nosplit
+func trap(tf *[TFSIZE]uintptr) {
+	trapno := tf[TF_TRAPNO]
+
+	if trapno == TRAP_NMI {
+		perfgather(tf)
+		perfmask()
+		_trapret(tf)
+	}
+
+	Lcr3(p_kpmap)
+
+	// CPU exceptions in kernel mode are fatal errors
+	if trapno < TRAP_TIMER && (tf[TF_CS] & 3) == 0 {
+		kernel_fault(tf)
+	}
+
+	ct := Gscpu().mythread
+
+	if rflags() & TF_FL_IF != 0 {
+		G_pancake("ints enabled in trap", 0)
+	}
+
+	if Halt != 0 {
+		for {
+		}
+	}
+
+	// clear shadow pointers to user pmap
+	shadow_clear()
+
+	// don't add code before FPU context saving unless you've thought very
+	// carefully! it is easy to accidentally and silently corrupt FPU state
+	// (ie calling memmove indirectly by assignment of large datatypes)
+	// before it is saved below.
+
+	// save FPU state immediately before we clobber it
+	if ct != nil {
+		// if in user mode, save to user buffers and make it look like
+		// Userrun returned.
+		if ct.user.tf != nil {
+			ufx := ct.user.fxbuf
+			fxsave(ufx)
+			utf := ct.user.tf
+			*utf = *tf
+			ct.tf[TF_RIP] = _userintaddr
+			ct.tf[TF_RSP] = utf[TF_SYSRSP]
+			ct.tf[TF_RAX] = trapno
+			ct.tf[TF_RBX] = Rcr2()
+			// XXXPANIC
+			if trapno == TRAP_YIELD || trapno == TRAP_SIGRET {
+				G_pancake("nyet", trapno)
+			}
+			// XXX fix this using RIP method
+			// if we are unlucky enough for a timer int to come in
+			// before we execute the first instruction of the new
+			// rip, make sure the state we just saved isn't
+			// clobbered
+			ct.user.tf = nil
+			ct.user.fxbuf = nil
+		} else {
+			fxsave(&ct.fx)
+			ct.tf = *tf
+		}
+		timetick(ct)
+	}
+
+	yielding := false
+	// these interrupts are handled specially by the runtime
+	if trapno == TRAP_YIELD {
+		trapno = TRAP_TIMER
+		tf[TF_TRAPNO] = TRAP_TIMER
+		yielding = true
+	}
+
+	if trapno == TRAP_TLBSHOOT {
+		lap_eoi()
+		// does not return
+		tlb_shootdown()
+	} else if trapno == TRAP_TIMER {
+		splock(threadlock)
+		if ct != nil {
+			if ct.status == ST_WILLSLEEP {
+				ct.status = ST_SLEEPING
+				// XXX set IF, unlock
+				ct.tf[TF_RFLAGS] |= TF_FL_IF
+				spunlock(futexlock)
+			} else {
+				ct.status = ST_RUNNABLE
+			}
+		}
+		if !yielding {
+			lap_eoi()
+			if Gscpu().num == 0 {
+				wakeup()
+				proftick()
+			}
+		}
+		// yieldy doesn't return
+		yieldy()
+	} else if is_irq(trapno) {
+		if _newtrap != nil {
+			// catch kernel faults that occur while trying to
+			// handle user traps
+			_newtrap(tf)
+		} else {
+			G_pancake("IRQ without ntrap", trapno)
+		}
+		sched_resume(ct)
+	} else if is_cpuex(trapno) {
+		// we vet out kernel mode CPU exceptions above must be from
+		// user program. thus return from Userrun() to kernel.
+		sched_run(ct)
+	} else if trapno == TRAP_SIGRET {
+		// does not return
+		sigret(ct)
+	} else if trapno == TRAP_PERFMASK {
+		lap_eoi()
+		perfmask()
+		sched_resume(ct)
+	} else {
+		G_pancake("unexpected int", trapno)
+	}
+	// not reached
+	G_pancake("no returning", 0)
+}
+
+//go:nosplit
+func is_irq(trapno uintptr) bool {
+	return trapno > IRQ_BASE && trapno <= IRQ_BASE + 15
+}
+
+//go:nosplit
+func is_cpuex(trapno uintptr) bool {
+	return trapno < IRQ_BASE
+}
+
 //go:nosplit
 func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
@@ -1673,6 +1825,15 @@ func sched_run(t *thread_t) {
 	trapret(&t.tf, t.p_pmap)
 }
 
+//go:nosplit
+func sched_resume(ct *thread_t) {
+	if ct != nil {
+		sched_run(ct)
+	} else {
+		sched_halt()
+	}
+}
+
 //go:nosplit
 func wakeup() {
 	_tchk()

commit 7ce9f81275ddcb9582b1289a8d57134daa7866f7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 17:09:15 2016 -0500

    finish init code: BSP/AP, syscall, and GS init

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index efea910cb3..0fa3a7a596 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,7 +18,6 @@ func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 func trapsched_m(*g)
 func trapinit_m(*g)
 
-func Ap_setup(int)
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
 func Install_traphandler(func(tf *[24]int))
@@ -48,7 +47,7 @@ func Vtop(*[512]int) int
 
 func Crash()
 func Fnaddr(func()) uintptr
-func Fnaddri(func(int)) uintptr
+func Fnaddri(func(uint)) uintptr
 func Trapwake()
 
 func htpause()
@@ -71,6 +70,7 @@ func fxrstor(*[FXREGS]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
 func mktrap(int)
 func stackcheck()
+func _sysentry()
 
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
@@ -82,7 +82,7 @@ func stackcheck()
 // - using range to iterate over a string (calls stringiter*)
 
 type cpu_t struct {
-	this		uint
+	this		*cpu_t
 	mythread	*thread_t
 	rsp		uintptr
 	num		uint
@@ -622,6 +622,7 @@ func pancake(msg *int8, addr uintptr) {
 		*p = 0x1400 | 'F'
 	}
 }
+
 //go:nosplit
 func G_pancake(msg string, addr uintptr) {
 	cli()
@@ -801,7 +802,7 @@ func tss_init(cpunum uint) uintptr {
 	tss_seginit(cpunum, tss, unsafe.Sizeof(tss_t{}) - 1)
 	segselect := segnum(cpunum) << 3
 	ltr(segselect)
-	cpus[lap_id()].rsp = rsp
+	cpus[cpunum].rsp = rsp
 	return rsp
 }
 
@@ -1211,9 +1212,10 @@ func fpuinit(amfirst bool) {
 		chkalign(unsafe.Pointer(&fxinit[0]), 16)
 		fxsave(&fxinit)
 
-		// XXX XXX XXX XXX XXX XXX XXX dont forget to do this once
-		// thread code is converted to go
-		G_pmsg("VERIFY FX FOR THREADS\n")
+		chksize(FXREGS*8, unsafe.Sizeof(threads[0].fx))
+		for i := range threads {
+			chkalign(unsafe.Pointer(&threads[i].fx), 16)
+		}
 	}
 }
 
@@ -1328,10 +1330,10 @@ func pit_phasewait() {
 var _lapic_quantum uint32
 
 //go:nosplit
-func lapic_setup(calibrate int32) {
+func lapic_setup(calibrate bool) {
 	la := uintptr(0xfee00000)
 
-	if calibrate != 0 {
+	if calibrate {
 		// map lapic IO mem
 		pte := pgdir_walk(la, false)
 		if pte != nil && *pte & PTE_P != 0 {
@@ -1359,7 +1361,7 @@ func lapic_setup(calibrate int32) {
 	divone := uint32(0xb)
 	wlap(LAPDCNT, divone)
 
-	if calibrate != 0 {
+	if calibrate {
 		// figure out how many lapic ticks there are in a second; first
 		// setup 8254 PIT since it has a known clock frequency. openbsd
 		// uses a similar technique.
@@ -1432,6 +1434,94 @@ func lapic_setup(calibrate int32) {
 	}
 }
 
+func proc_setup() {
+	chksize(TFSIZE*8, unsafe.Sizeof(threads[0].tf))
+	fpuinit(true)
+	// initialize the first thread: us
+	threads[0].status = ST_RUNNING
+	threads[0].p_pmap = p_kpmap
+
+	// initialize GS pointers
+	for i := range cpus {
+		cpus[i].this = &cpus[i]
+	}
+
+	lapic_setup(true)
+
+	// 8259a - mask all irqs. see 2.5.3.6 in piix3 documentation.
+	// otherwise an RTC timer interrupt (that turns into a double-fault
+	// since the PIC has not been programmed yet) comes in immediately
+	// after sti.
+	Outb(0x20 + 1, 0xff)
+	Outb(0xa0 + 1, 0xff)
+
+	myrsp := tss_init(0)
+	sysc_setup(myrsp)
+	gs_set(&cpus[0])
+	Gscpu().num = 0
+	Gscpu().mythread = &threads[0]
+}
+
+//go:nosplit
+func Ap_setup(cpunum uint) {
+	// interrupts are probably already cleared
+	fl := Pushcli()
+
+	splock(pmsglock)
+	_G_pmsg("cpu")
+	_pnum(uintptr(cpunum))
+	_G_pmsg("joined\n")
+	spunlock(pmsglock)
+
+	if cpunum >= uint(MAXCPUS) {
+		G_pancake("nice computer!", uintptr(cpunum))
+	}
+	fpuinit(false)
+	lapic_setup(false)
+	myrsp := tss_init(cpunum)
+	sysc_setup(myrsp)
+	mycpu := &cpus[cpunum]
+	if mycpu.num != 0 {
+		G_pancake("cpu id conflict", uintptr(mycpu.num))
+	}
+	fs_null()
+	gs_set(mycpu)
+	Gscpu().num = cpunum
+	Gscpu().mythread = nil
+
+	Popcli(fl)
+}
+
+//go:nosplit
+func gs_set(c *cpu_t) {
+	// we must set fs/gs at least once before we use the MSRs to change
+	// their base address. the MSRs write directly to hidden segment
+	// descriptor cache, and if we don't explicitly fill the segment
+	// descriptor cache, the writes to the MSRs are thrown out (presumably
+	// because the caches are thought to be invalid).
+	gs_null()
+	ia32_gs_base := 0xc0000101
+	Wrmsr(ia32_gs_base, int(uintptr(unsafe.Pointer(c))))
+}
+
+//go:nosplit
+func sysc_setup(myrsp uintptr) {
+	// lowest 2 bits are ignored for sysenter, but used for sysexit
+	kcode64 := 1 << 3 | 3
+	sysenter_cs := 0x174
+	Wrmsr(sysenter_cs, kcode64)
+
+	sysenter_eip := 0x176
+	// asm_amd64.s
+	var dur func()
+	dur = _sysentry
+	sysentryaddr := **(**uintptr)(unsafe.Pointer(&dur))
+	Wrmsr(sysenter_eip, int(sysentryaddr))
+
+	sysenter_esp := 0x175
+	Wrmsr(sysenter_esp, int(myrsp))
+}
+
 var tlbshoot_wait uintptr
 var tlbshoot_pg uintptr
 var tlbshoot_count uintptr

commit 624faa7efb6223d3632e7f3a65efbedab9698a2d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 14:37:55 2016 -0500

    convert LAPIC init code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f85e9fb20d..efea910cb3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -26,10 +26,10 @@ func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(uintptr)
-func Inb(int) int
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
-func Outb(int, int)
+func Outb(uint16, uint8)
+func Inb(uint16) uint
 func Outw(int, int)
 func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
@@ -51,7 +51,6 @@ func Fnaddr(func()) uintptr
 func Fnaddri(func(int)) uintptr
 func Trapwake()
 
-func inb(int) int
 func htpause()
 func finit()
 func fs_null()
@@ -62,7 +61,6 @@ func lidt(pdesc_t)
 func cli()
 func sti()
 func ltr(uint)
-func lap_id() int
 func rcr0() uintptr
 func rcr4() uintptr
 func lcr0(uintptr)
@@ -94,6 +92,7 @@ type cpu_t struct {
 }
 
 var Cpumhz uint
+var Pspercycle uint
 
 const MAXCPUS int = 32
 
@@ -155,8 +154,6 @@ const(
     TF_FL_IF     uintptr = 1 << 9
 )
 
-var Pspercycle uint
-
 func Pushcli() int
 func Popcli(int)
 func _Gscpu() *cpu_t
@@ -418,14 +415,14 @@ func sc_setup() {
 	Outb(com1 + 1, 1)
 }
 
-const com1 = 0x3f8
-const lstatus = 5
+const com1 = uint16(0x3f8)
 
 //go:nosplit
 func sc_put_(c int8) {
-	for inb(com1 + lstatus) & 0x20 == 0 {
+	lstatus := uint16(5)
+	for Inb(com1 + lstatus) & 0x20 == 0 {
 	}
-	Outb(com1, int(c))
+	Outb(com1, uint8(c))
 }
 
 //go:nosplit
@@ -540,7 +537,6 @@ var Halt uint32
 // TEMPORARY CRAP
 func _pmsg(*int8)
 func invlpg(uintptr)
-func lap_eoi()
 func rflags() uintptr
 
 // wait until remove definition from proc.c
@@ -562,7 +558,8 @@ func splock(l *spinlock_t) {
 
 //go:nosplit
 func spunlock(l *spinlock_t) {
-	atomicstore(&l.v, 0)
+	//atomicstore(&l.v, 0)
+	l.v = 0
 }
 
 // since this lock may be taken during an interrupt (only under fatal error
@@ -1220,142 +1217,219 @@ func fpuinit(amfirst bool) {
 	}
 }
 
-func find_empty(sz uintptr) uintptr {
-	v := caddr(0, 0, 0, 1, 0)
-	cantuse := uintptr(0xf0)
-	for {
-		pte := pgdir_walk(v, false)
-		if pte == nil || (*pte != cantuse && *pte & PTE_P == 0) {
-			failed := false
-			for i := uintptr(0); i < sz; i += PGSIZE {
-				pte = pgdir_walk(v + i, false)
-				if pte != nil &&
-				    (*pte & PTE_P != 0 || *pte == cantuse) {
-					failed = true
-					v += i
-					break
-				}
-			}
-			if !failed {
-				return v
-			}
-		}
-		v += PGSIZE
+// LAPIC registers
+const (
+	LAPID		= 0x20/4
+	LAPEOI		= 0xb0/4
+	LAPVER		= 0x30/4
+	LAPDCNT		= 0x3e0/4
+	LAPICNT		= 0x380/4
+	LAPCCNT		= 0x390/4
+	LVSPUR		= 0xf0/4
+	LVTIMER		= 0x320/4
+	LVCMCI		= 0x2f0/4
+	LVINT0		= 0x350/4
+	LVINT1		= 0x360/4
+	LVERROR		= 0x370/4
+	LVPERF		= 0x340/4
+	LVTHERMAL	= 0x330/4
+)
+
+var _lapaddr uintptr
+
+//go:nosplit
+func rlap(reg uint) uint32 {
+	if _lapaddr == 0 {
+		G_pancake("lapic not init", 0)
 	}
+	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
+	return atomicload(&lpg[reg])
 }
 
-func prot_none(v, sz uintptr) {
-	for i := uintptr(0); i < sz; i += PGSIZE {
-		pte := pgdir_walk(v + i, true)
-		if pte != nil {
-			*pte = *pte & ^PTE_P
-			invlpg(v + i)
-		}
+//go:nosplit
+func wlap(reg uint, val uint32) {
+	if _lapaddr == 0 {
+		G_pancake("lapic not init", 0)
 	}
+	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
+	lpg[reg] = val
 }
 
-var maplock = &spinlock_t{}
+//go:nosplit
+func lap_id() uint32 {
+	if rflags() & TF_FL_IF != 0 {
+		G_pancake("interrupts must be cleared", 0)
+	}
+	if _lapaddr == 0 {
+		G_pancake("lapic not init", 0)
+	}
+	lpg := (*[PGSIZE/4]uint32)(unsafe.Pointer(_lapaddr))
+	return lpg[LAPID] >> 24
+}
 
-// this flag makes hack_mmap panic if a new pml4 entry is ever added to the
-// kernel's pmap. we want to make sure all kernel mappings added after bootup
-// fall into the same pml4 entry so that all the kernel mappings can be easily
-// shared in user process pmaps.
-var _nopml4 bool
+//go:nosplit
+func lap_eoi() {
+	if _lapaddr == 0 {
+		G_pancake("lapic not init", 0)
+	}
+	wlap(LAPEOI, 0)
+}
 
-func Pml4freeze() {
-	_nopml4 = true
+// PIT registers
+const (
+	CNT0	uint16 = 0x40
+	CNTCTL	uint16 = 0x43
+	_pitfreq	= 1193182
+	_pithz		= 100
+	PITDIV		= _pitfreq/_pithz
+)
+
+//go:nosplit
+func pit_ticks() uint {
+	// counter latch command for counter 0
+	cmd := uint8(0)
+	Outb(CNTCTL, cmd)
+	low := Inb(CNT0)
+	hi := Inb(CNT0)
+	return hi << 8 | low
 }
 
-func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
-    fd int32, offset int32) uintptr {
-	fl := Pushcli()
-	splock(maplock)
+//go:nosplit
+func pit_enable() {
+	// rate generator mode, lsb then msb (if square wave mode is used, the
+	// PIT uses div/2 for the countdown since div is taken to be the period
+	// of the wave)
+	Outb(CNTCTL, 0x34)
+	Outb(CNT0, uint8(PITDIV & 0xff))
+	Outb(CNT0, uint8(PITDIV >> 8))
+}
 
-	MAP_ANON := uintptr(0x20)
-	MAP_PRIVATE := uintptr(0x2)
-	PROT_NONE := uintptr(0x0)
-	PROT_WRITE := uintptr(0x2)
+func pit_disable() {
+	// disable PIT: one-shot, lsb then msb
+	Outb(CNTCTL, 0x32);
+	Outb(CNT0, uint8(PITDIV & 0xff))
+	Outb(CNT0, uint8(PITDIV >> 8))
+}
 
-	prot := uintptr(_prot)
-	flags := uintptr(_flags)
-	var vaend uintptr
-	var perms uintptr
-	var ret uintptr
-	var t uintptr
-	pgleft := pglast - pgfirst
-	sz := pgroundup(_sz)
-	if sz > pgleft {
-		ret = ^uintptr(0)
-		goto out
-	}
-	sz = pgroundup(va + _sz)
-	sz -= pgrounddown(va)
-	if va == 0 {
-		va = find_empty(sz)
-	}
-	vaend = caddr(VUEND, 0, 0, 0, 0)
-	if va >= vaend || va + sz >= vaend {
-		G_pancake("va space exhausted", va)
+// wait until 8254 resets the counter
+//go:nosplit
+func pit_phasewait() {
+	// 8254 timers are 16 bits, thus always smaller than last;
+	last := uint(1 << 16)
+	for {
+		cur := pit_ticks()
+		if cur > last {
+			return
+		}
+		last = cur
 	}
+}
 
-	t = MAP_ANON | MAP_PRIVATE
-	if flags & t != t {
-		G_pancake("unexpected flags", flags)
-	}
-	perms = PTE_P
-	if prot == PROT_NONE {
-		prot_none(va, sz)
-		ret = va
-		goto out
-	}
+var _lapic_quantum uint32
 
-	if prot & PROT_WRITE != 0 {
-		perms |= PTE_W
-	}
+//go:nosplit
+func lapic_setup(calibrate int32) {
+	la := uintptr(0xfee00000)
 
-	if _nopml4 {
-		eidx := pml4x(va + sz - 1)
-		for sidx := pml4x(va); sidx <= eidx; sidx++ {
-			pml4 := caddr(VREC, VREC, VREC, VREC, sidx)
-			pml4e := (*uintptr)(unsafe.Pointer(pml4))
-			if *pml4e & PTE_P == 0 {
-				G_pancake("new pml4 entry to kernel pmap", va)
-			}
+	if calibrate != 0 {
+		// map lapic IO mem
+		pte := pgdir_walk(la, false)
+		if pte != nil && *pte & PTE_P != 0 {
+			G_pancake("lapic mem already mapped", 0)
 		}
 	}
 
-	for i := uintptr(0); i < sz; i += PGSIZE {
-		alloc_map(va + i, perms, true)
+	pte := pgdir_walk(la, true)
+	*pte = la | PTE_W | PTE_P | PTE_PCD
+	_lapaddr = la
+
+	lver := rlap(LAPVER)
+	if lver < 0x10 {
+		G_pancake("82489dx not supported", uintptr(lver))
 	}
-	ret = va
-out:
-	spunlock(maplock)
-	Popcli(fl)
-	return ret
-}
 
-func hack_munmap(v, _sz uintptr) {
-	fl := Pushcli()
-	splock(maplock)
-	sz := pgroundup(_sz)
-	cantuse := uintptr(0xf0)
-	for i := uintptr(0); i < sz; i += PGSIZE {
-		va := v + i
-		pte := pgdir_walk(va, false)
-		if pml4x(va) >= VUEND {
-			G_pancake("high unmap", va)
+	// enable lapic, set spurious int vector
+	apicenable := 1 << 8
+	wlap(LVSPUR, uint32(apicenable | TRAP_SPUR))
+
+	// timer: periodic, int 32
+	periodic := 1 << 17
+	wlap(LVTIMER, uint32(periodic | TRAP_TIMER))
+	// divide by 1
+	divone := uint32(0xb)
+	wlap(LAPDCNT, divone)
+
+	if calibrate != 0 {
+		// figure out how many lapic ticks there are in a second; first
+		// setup 8254 PIT since it has a known clock frequency. openbsd
+		// uses a similar technique.
+		pit_enable()
+
+		// start lapic counting
+		wlap(LAPICNT, 0x80000000)
+		pit_phasewait()
+		lapstart := rlap(LAPCCNT)
+		cycstart := Rdtsc()
+
+		frac := 10
+		// XXX only wait for 100ms instead of 1s
+		for i := 0; i < _pithz/frac; i++ {
+			pit_phasewait()
 		}
-		// XXX goodbye, memory
-		if pte != nil && *pte & PTE_P != 0 {
-			// make sure these pages aren't remapped via
-			// hack_munmap
-			*pte = cantuse
-			invlpg(va)
+
+		lapend := rlap(LAPCCNT)
+		if lapend > lapstart {
+			G_pancake("lapic timer wrapped?", uintptr(lapend))
 		}
+		lapelapsed := (lapstart - lapend)*uint32(frac)
+		cycelapsed := (Rdtsc() - cycstart)*uint64(frac)
+		G_pmsg("LAPIC Mhz:")
+		pnum(uintptr(lapelapsed/(1000 * 1000)))
+		G_pmsg("\n")
+		_lapic_quantum = lapelapsed / HZ
+
+		G_pmsg("CPU Mhz:")
+		Cpumhz = uint(cycelapsed/(1000 * 1000))
+		pnum(uintptr(Cpumhz))
+		G_pmsg("\n")
+		Pspercycle = uint(1000000000000/cycelapsed)
+
+		pit_disable()
+	}
+
+	// initial count; the LAPIC's frequency is not the same as the CPU's
+	// frequency
+	wlap(LAPICNT, _lapic_quantum)
+
+	maskint := uint32(1 << 16)
+	// mask cmci, lint[01], error, perf counters, and thermal sensor
+	wlap(LVCMCI,    maskint)
+	// unmask LINT0 and LINT1. soon, i will use IO APIC instead.
+	wlap(LVINT0,    rlap(LVINT0) &^ maskint)
+	wlap(LVINT1,    rlap(LVINT1) &^ maskint)
+	wlap(LVERROR,   maskint)
+	wlap(LVPERF,    maskint)
+	wlap(LVTHERMAL, maskint)
+
+	ia32_apic_base := 0x1b
+	reg := uintptr(Rdmsr(ia32_apic_base))
+	if reg & (1 << 11) == 0 {
+		G_pancake("lapic disabled?", reg)
+	}
+	if (reg >> 12) != 0xfee00 {
+		G_pancake("weird base addr?", reg >> 12)
+	}
+
+	lreg := rlap(LVSPUR)
+	if lreg & (1 << 12) != 0 {
+		G_pmsg("EOI broadcast surpression\n")
+	}
+	if lreg & (1 << 9) != 0 {
+		G_pmsg("focus processor checking\n")
+	}
+	if lreg & (1 << 8) == 0 {
+		G_pmsg("apic disabled\n")
 	}
-	G_pmsg("POOF\n")
-	spunlock(maplock)
-	Popcli(fl)
 }
 
 var tlbshoot_wait uintptr
@@ -1438,68 +1512,27 @@ func _dummy() {
 	_notdeadcode = 0
 }
 
+// cpu exception/interrupt vectors
 const (
+	TRAP_NMI	= 2
+	TRAP_PGFAULT	= 14
+	TRAP_SYSCALL	= 64
 	TRAP_TIMER	= 32
+	TRAP_DISK	= (32 + 14)
+	TRAP_SPUR	= 48
 	TRAP_YIELD	= 49
+	TRAP_TLBSHOOT	= 70
+	TRAP_SIGRET	= 71
+	TRAP_PERFMASK	= 72
 )
 
-func clone_wrap(rip uintptr) {
-	clone_call(rip)
-	G_pancake("clone_wrap returned", 0)
-}
-
-func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
-	CLONE_VM := 0x100
-	CLONE_FS := 0x200
-	CLONE_FILES := 0x400
-	CLONE_SIGHAND := 0x800
-	CLONE_THREAD := 0x10000
-	chk := uint32(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
-	    CLONE_THREAD)
-	if flags != chk {
-		G_pancake("unexpected clone args", uintptr(flags))
-	}
-	var dur func(uintptr)
-	dur = clone_wrap
-	cloneaddr := **(**uintptr)(unsafe.Pointer(&dur))
-
-	fl := Pushcli()
-	splock(threadlock)
-
-	ti := thread_avail()
-	// provide fn as arg to clone_wrap
-	rsp -= 8
-	*(*uintptr)(unsafe.Pointer(rsp)) = fn
-	rsp -= 8
-	// bad return address
-	*(*uintptr)(unsafe.Pointer(rsp)) = 0
-
-	mt := &threads[ti]
-	memclr(unsafe.Pointer(mt), unsafe.Sizeof(thread_t{}))
-	mt.tf[TF_CS] = KCODE64 << 3
-	mt.tf[TF_RSP] = rsp
-	mt.tf[TF_RIP] = cloneaddr
-	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
-	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 16
-
-	gp.m = mp
-	mp.tls[0] = uintptr(unsafe.Pointer(gp))
-	mp.procid = uint64(ti)
-	mt.status = ST_RUNNABLE
-	mt.p_pmap = p_kpmap
-
-	mt.fx = fxinit
-
-	spunlock(threadlock)
-	Popcli(fl)
-}
-
 var threadlock = &spinlock_t{}
 
 // maximum # of runtime "OS" threads
 const maxthreads = 64
 var threads [maxthreads]thread_t
 
+// thread states
 const (
 	ST_INVALID	= 0
 	ST_RUNNABLE	= 1
@@ -1509,6 +1542,11 @@ const (
 	ST_WILLSLEEP	= 5
 )
 
+// scheduler constants
+const (
+	HZ	= 100
+)
+
 //go:nosplit
 func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
@@ -1588,6 +1626,195 @@ func yieldy() {
 	sched_halt()
 }
 
+func find_empty(sz uintptr) uintptr {
+	v := caddr(0, 0, 0, 1, 0)
+	cantuse := uintptr(0xf0)
+	for {
+		pte := pgdir_walk(v, false)
+		if pte == nil || (*pte != cantuse && *pte & PTE_P == 0) {
+			failed := false
+			for i := uintptr(0); i < sz; i += PGSIZE {
+				pte = pgdir_walk(v + i, false)
+				if pte != nil &&
+				    (*pte & PTE_P != 0 || *pte == cantuse) {
+					failed = true
+					v += i
+					break
+				}
+			}
+			if !failed {
+				return v
+			}
+		}
+		v += PGSIZE
+	}
+}
+
+func prot_none(v, sz uintptr) {
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		pte := pgdir_walk(v + i, true)
+		if pte != nil {
+			*pte = *pte & ^PTE_P
+			invlpg(v + i)
+		}
+	}
+}
+
+var maplock = &spinlock_t{}
+
+// this flag makes hack_mmap panic if a new pml4 entry is ever added to the
+// kernel's pmap. we want to make sure all kernel mappings added after bootup
+// fall into the same pml4 entry so that all the kernel mappings can be easily
+// shared in user process pmaps.
+var _nopml4 bool
+
+func Pml4freeze() {
+	_nopml4 = true
+}
+
+func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
+    fd int32, offset int32) uintptr {
+	fl := Pushcli()
+	splock(maplock)
+
+	MAP_ANON := uintptr(0x20)
+	MAP_PRIVATE := uintptr(0x2)
+	PROT_NONE := uintptr(0x0)
+	PROT_WRITE := uintptr(0x2)
+
+	prot := uintptr(_prot)
+	flags := uintptr(_flags)
+	var vaend uintptr
+	var perms uintptr
+	var ret uintptr
+	var t uintptr
+	pgleft := pglast - pgfirst
+	sz := pgroundup(_sz)
+	if sz > pgleft {
+		ret = ^uintptr(0)
+		goto out
+	}
+	sz = pgroundup(va + _sz)
+	sz -= pgrounddown(va)
+	if va == 0 {
+		va = find_empty(sz)
+	}
+	vaend = caddr(VUEND, 0, 0, 0, 0)
+	if va >= vaend || va + sz >= vaend {
+		G_pancake("va space exhausted", va)
+	}
+
+	t = MAP_ANON | MAP_PRIVATE
+	if flags & t != t {
+		G_pancake("unexpected flags", flags)
+	}
+	perms = PTE_P
+	if prot == PROT_NONE {
+		prot_none(va, sz)
+		ret = va
+		goto out
+	}
+
+	if prot & PROT_WRITE != 0 {
+		perms |= PTE_W
+	}
+
+	if _nopml4 {
+		eidx := pml4x(va + sz - 1)
+		for sidx := pml4x(va); sidx <= eidx; sidx++ {
+			pml4 := caddr(VREC, VREC, VREC, VREC, sidx)
+			pml4e := (*uintptr)(unsafe.Pointer(pml4))
+			if *pml4e & PTE_P == 0 {
+				G_pancake("new pml4 entry to kernel pmap", va)
+			}
+		}
+	}
+
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		alloc_map(va + i, perms, true)
+	}
+	ret = va
+out:
+	spunlock(maplock)
+	Popcli(fl)
+	return ret
+}
+
+func hack_munmap(v, _sz uintptr) {
+	fl := Pushcli()
+	splock(maplock)
+	sz := pgroundup(_sz)
+	cantuse := uintptr(0xf0)
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		va := v + i
+		pte := pgdir_walk(va, false)
+		if pml4x(va) >= VUEND {
+			G_pancake("high unmap", va)
+		}
+		// XXX goodbye, memory
+		if pte != nil && *pte & PTE_P != 0 {
+			// make sure these pages aren't remapped via
+			// hack_munmap
+			*pte = cantuse
+			invlpg(va)
+		}
+	}
+	G_pmsg("POOF\n")
+	spunlock(maplock)
+	Popcli(fl)
+}
+
+func clone_wrap(rip uintptr) {
+	clone_call(rip)
+	G_pancake("clone_wrap returned", 0)
+}
+
+func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
+	CLONE_VM := 0x100
+	CLONE_FS := 0x200
+	CLONE_FILES := 0x400
+	CLONE_SIGHAND := 0x800
+	CLONE_THREAD := 0x10000
+	chk := uint32(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
+	    CLONE_THREAD)
+	if flags != chk {
+		G_pancake("unexpected clone args", uintptr(flags))
+	}
+	var dur func(uintptr)
+	dur = clone_wrap
+	cloneaddr := **(**uintptr)(unsafe.Pointer(&dur))
+
+	fl := Pushcli()
+	splock(threadlock)
+
+	ti := thread_avail()
+	// provide fn as arg to clone_wrap
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = fn
+	rsp -= 8
+	// bad return address
+	*(*uintptr)(unsafe.Pointer(rsp)) = 0
+
+	mt := &threads[ti]
+	memclr(unsafe.Pointer(mt), unsafe.Sizeof(thread_t{}))
+	mt.tf[TF_CS] = KCODE64 << 3
+	mt.tf[TF_RSP] = rsp
+	mt.tf[TF_RIP] = cloneaddr
+	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
+	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 16
+
+	gp.m = mp
+	mp.tls[0] = uintptr(unsafe.Pointer(gp))
+	mp.procid = uint64(ti)
+	mt.status = ST_RUNNABLE
+	mt.p_pmap = p_kpmap
+
+	mt.fx = fxinit
+
+	spunlock(threadlock)
+	Popcli(fl)
+}
+
 func hack_setitimer(timer uint32, new, old *itimerval) {
 	TIMER_PROF := uint32(2)
 	if timer != TIMER_PROF {

commit fe3471e33f3fe3e430246c9855c46036355c5e4f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 9 11:14:30 2016 -0500

    finish fake syscalls: exit, sigaltstack, nanotime, futex, setitimer, write

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 62908c3bdf..f85e9fb20d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -26,7 +26,6 @@ func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(uintptr)
-func Nanotime() int
 func Inb(int) int
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
@@ -42,7 +41,6 @@ func Rdtsc() uint64
 func Rcr2() uintptr
 func Rcr3() uintptr
 func Rcr4() uintptr
-func Rrsp() int
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
 func Sti()
@@ -51,10 +49,6 @@ func Vtop(*[512]int) int
 func Crash()
 func Fnaddr(func()) uintptr
 func Fnaddri(func(int)) uintptr
-func Tfdump(*[24]int)
-func Rflags() int
-func Resetgcticks() uint64
-func Gcticks() uint64
 func Trapwake()
 
 func inb(int) int
@@ -77,9 +71,8 @@ func clone_call(uintptr)
 func cpu_halt(uintptr)
 func fxrstor(*[FXREGS]uintptr)
 func trapret(*[TFSIZE]uintptr, uintptr)
-
-// os_linux.c
-var gcticks uint64
+func mktrap(int)
+func stackcheck()
 
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
@@ -129,11 +122,11 @@ type thread_t struct {
 	siglseepfor	int
 	status		int
 	doingsig	int
-	sigstack	int
+	sigstack	uintptr
 	prof		prof_t
 	sleepfor	int
 	sleepret	int
-	futaddr		int
+	futaddr		uintptr
 	p_pmap		uintptr
 	//_pad		int
 }
@@ -166,13 +159,21 @@ var Pspercycle uint
 
 func Pushcli() int
 func Popcli(int)
-func Gscpu() *cpu_t
+func _Gscpu() *cpu_t
 func Rdmsr(int) int
 func Wrmsr(int, int)
 func _Userrun(*[24]int, bool) (int, int)
 
 func Cprint(byte, int)
 
+//go:nosplit
+func Gscpu() *cpu_t {
+	if rflags() & TF_FL_IF != 0 {
+		G_pancake("must not be interruptible", 0)
+	}
+	return _Gscpu()
+}
+
 func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
     pms []*[512]int, fastret bool) (int, int) {
 
@@ -534,8 +535,6 @@ func Trapinit() {
 // conversion is underway. remove prefix afterwards. we need two versions of
 // functions that take a string as an argument since string literals are
 // different data types in C and Go.
-//
-// XXX XXX many of these do not need nosplits!
 var Halt uint32
 
 // TEMPORARY CRAP
@@ -543,7 +542,6 @@ func _pmsg(*int8)
 func invlpg(uintptr)
 func lap_eoi()
 func rflags() uintptr
-func hack_nanotime() int
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -1417,8 +1415,8 @@ func tlb_shootdown() {
 // spinlock in order to avoid a deadlock where the thread that acquired the
 // spinlock starts a GC and waits forever for the spinning thread. (go code
 // should probably not use spinlocks. tlb shootdown code is the only code
-// protected by a spinlock since the lock must be acquired in interrupt
-// context.)
+// protected by a spinlock since the lock must both be acquired in go code and
+// in interrupt context.)
 //
 // alternatively, we could make sure that no allocations are made while the
 // spinlock is acquired.
@@ -1589,3 +1587,182 @@ func yieldy() {
 	spunlock(threadlock)
 	sched_halt()
 }
+
+func hack_setitimer(timer uint32, new, old *itimerval) {
+	TIMER_PROF := uint32(2)
+	if timer != TIMER_PROF {
+		G_pancake("weird timer", uintptr(timer))
+	}
+
+	fl := Pushcli()
+	ct := Gscpu().mythread
+	nsecs := new.it_interval.tv_sec * 1000000000 +
+	    new.it_interval.tv_usec * 1000
+	if nsecs != 0 {
+		ct.prof.enabled = 1
+	} else {
+		ct.prof.enabled = 0
+	}
+	Popcli(fl)
+}
+
+func hack_sigaltstack(new, old *sigaltstackt) {
+	fl := Pushcli()
+	ct := Gscpu().mythread
+	SS_DISABLE := int32(2)
+	if new.ss_flags & SS_DISABLE != 0 {
+		ct.sigstack = 0
+	} else {
+		ct.sigstack = uintptr(unsafe.Pointer(new.ss_sp)) +
+		    uintptr(new.ss_size)
+	}
+	Popcli(fl)
+}
+
+func hack_write(fd int, bufn uintptr, sz uint32) int64 {
+	if fd != 1 && fd != 2 {
+		G_pancake("unexpected fd", uintptr(fd))
+	}
+	fl := Pushcli()
+	splock(pmsglock)
+	c := uintptr(sz)
+	for i := uintptr(0); i < c; i++ {
+		p := (*int8)(unsafe.Pointer(bufn + i))
+		putch(*p)
+	}
+	spunlock(pmsglock)
+	Popcli(fl)
+	return int64(sz)
+}
+
+// "/etc/localtime"
+var fnwhite = []int8{0x2f, 0x65, 0x74, 0x63, 0x2f, 0x6c, 0x6f, 0x63, 0x61,
+    0x6c, 0x74, 0x69, 0x6d, 0x65}
+
+// a is the C string.
+func cstrmatch(a uintptr, b []int8) bool {
+	for i, c := range b {
+		p := (*int8)(unsafe.Pointer(a + uintptr(i)))
+		if *p != c {
+			return false
+		}
+	}
+	return true
+}
+
+func hack_syscall(trap, a1, a2, a3 int64) (int64, int64, int64) {
+	switch trap {
+	case 1:
+		r1 := hack_write(int(a1), uintptr(a2), uint32(a3))
+		return r1, 0, 0
+	case 2:
+		enoent := int64(-2)
+		if !cstrmatch(uintptr(a1), fnwhite) {
+			G_pancake("unexpected open", 0)
+		}
+		return 0, 0, enoent
+	default:
+		G_pancake("unexpected syscall", uintptr(trap))
+	}
+	// not reached
+	return 0, 0, -1
+}
+
+var futexlock = &spinlock_t{}
+
+// XXX not sure why stack splitting prologue is not ok here
+//go:nosplit
+func hack_futex(uaddr *int32, op, val int32, to *timespec, uaddr2 *int32,
+    val2 int32) int64 {
+	stackcheck()
+	FUTEX_WAIT := int32(0)
+	FUTEX_WAKE := int32(1)
+	uaddrn := uintptr(unsafe.Pointer(uaddr))
+	ret := 0
+	switch op {
+	case FUTEX_WAIT:
+		cli()
+		splock(futexlock)
+		dosleep := *uaddr == val
+		if dosleep {
+			ct := Gscpu().mythread
+			ct.futaddr = uaddrn
+			ct.status = ST_WILLSLEEP
+			ct.sleepfor = -1
+			if to != nil {
+				t := to.tv_sec * 1000000000
+				t += to.tv_nsec
+				ct.sleepfor = hack_nanotime() + int(t)
+			}
+			mktrap(TRAP_YIELD)
+			// scheduler unlocks futexlock and returns with
+			// interrupts enabled...
+			cli()
+			ret = Gscpu().mythread.sleepret
+			sti()
+		} else {
+			spunlock(futexlock)
+			sti()
+			eagain := -11
+			ret = eagain
+		}
+	case FUTEX_WAKE:
+		woke := 0
+		cli()
+		splock(futexlock)
+		splock(threadlock)
+		for i := 0; i < maxthreads && val > 0; i++ {
+			t := &threads[i]
+			st := t.status
+			if t.futaddr == uaddrn && st == ST_SLEEPING {
+				t.status = ST_RUNNABLE
+				t.sleepfor = 0
+				t.futaddr = 0
+				t.sleepret = 0
+				val--
+				woke++
+			}
+		}
+		spunlock(threadlock)
+		spunlock(futexlock)
+		sti()
+		ret = woke
+	default:
+		G_pancake("unexpected futex op", uintptr(op))
+	}
+	return int64(ret)
+}
+
+func hack_usleep(delay int64) {
+	ts := timespec{}
+	ts.tv_sec = delay/1000000
+	ts.tv_nsec = (delay%1000000)*1000
+	dummy := int32(0)
+	FUTEX_WAIT := int32(0)
+	hack_futex(&dummy, FUTEX_WAIT, 0, &ts, nil, 0)
+}
+
+func hack_exit(code int32) {
+	cli()
+	Gscpu().mythread.status = ST_INVALID
+	G_pmsg("exit with code")
+	pnum(uintptr(code))
+	G_pmsg(".\nhalting\n")
+	atomicstore(&Halt, 1)
+	for {
+	}
+}
+
+// called in interupt context
+//go:nosplit
+func hack_nanotime() int {
+	cyc := uint(Rdtsc())
+	return int(cyc*Pspercycle/1000)
+}
+
+// XXX also called in interupt context; remove when trapstub is moved into
+// runtime
+//go:nosplit
+func Nanotime() int {
+	return hack_nanotime()
+}

commit c8bf8e0cb06d3ba231ba90071303ab38d27087fd
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 8 16:37:44 2016 -0500

    start converting scheduler code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f9feaa42e3..62908c3bdf 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -74,6 +74,9 @@ func rcr4() uintptr
 func lcr0(uintptr)
 func lcr4(uintptr)
 func clone_call(uintptr)
+func cpu_halt(uintptr)
+func fxrstor(*[FXREGS]uintptr)
+func trapret(*[TFSIZE]uintptr, uintptr)
 
 // os_linux.c
 var gcticks uint64
@@ -539,9 +542,8 @@ var Halt uint32
 func _pmsg(*int8)
 func invlpg(uintptr)
 func lap_eoi()
-func sched_run(*thread_t)
-func sched_halt()
 func rflags() uintptr
+func hack_nanotime() int
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -1509,13 +1511,18 @@ const (
 	ST_WILLSLEEP	= 5
 )
 
-func thread_avail() int {
+//go:nosplit
+func _tchk() {
 	if rflags() & TF_FL_IF != 0 {
 		G_pancake("must not be interruptible", 0)
 	}
 	if threadlock.v == 0 {
 		G_pancake("must hold threadlock", 0)
 	}
+}
+
+func thread_avail() int {
+	_tchk()
 	for i := range threads {
 		if threads[i].status == ST_INVALID {
 			return i
@@ -1524,3 +1531,61 @@ func thread_avail() int {
 	G_pancake("no available threads", maxthreads)
 	return -1
 }
+
+//go:nosplit
+func sched_halt() {
+	cpu_halt(Gscpu().rsp)
+}
+
+//go:nosplit
+func sched_run(t *thread_t) {
+	if t.tf[TF_RFLAGS] & TF_FL_IF == 0 {
+		G_pancake("thread not interurptible", 0)
+	}
+	Gscpu().mythread = t
+	fxrstor(&t.fx)
+	trapret(&t.tf, t.p_pmap)
+}
+
+//go:nosplit
+func wakeup() {
+	_tchk()
+	now := hack_nanotime()
+	timedout := -110
+	for i := range threads {
+		t := &threads[i]
+		sf := t.sleepfor
+		if t.status == ST_SLEEPING && sf != -1 && sf < now {
+			t.status = ST_RUNNABLE
+			t.sleepfor = 0
+			t.futaddr = 0
+			t.sleepret = timedout
+		}
+	}
+}
+
+//go:nosplit
+func yieldy() {
+	_tchk()
+	cpu := Gscpu()
+	ct := cpu.mythread
+	_ti := (uintptr(unsafe.Pointer(ct)) -
+	    uintptr(unsafe.Pointer(&threads[0])))/unsafe.Sizeof(thread_t{})
+	ti := int(_ti)
+	start := (ti + 1) % maxthreads
+	if ct == nil {
+		start = 0
+	}
+	for i := 0; i < maxthreads; i++ {
+		idx := (start + i) % maxthreads
+		t := &threads[idx]
+		if t.status == ST_RUNNABLE {
+			t.status = ST_RUNNING
+			spunlock(threadlock)
+			sched_run(t)
+		}
+	}
+	cpu.mythread = nil
+	spunlock(threadlock)
+	sched_halt()
+}

commit 01398330b0a342dca59abcd1fcbd4f9172c52c5d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 8 15:19:32 2016 -0500

    convert fake clone

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index eb5f3fc847..f9feaa42e3 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -73,6 +73,7 @@ func rcr0() uintptr
 func rcr4() uintptr
 func lcr0(uintptr)
 func lcr4(uintptr)
+func clone_call(uintptr)
 
 // os_linux.c
 var gcticks uint64
@@ -113,12 +114,14 @@ type prof_t struct {
 	stampstart	int
 }
 
+// XXX rearrange these for better spatial locality; p_pmap should probably be
+// near front
 type thread_t struct {
-	tf		[24]int
-	fx		[64]int
+	tf		[TFSIZE]uintptr
+	fx		[FXREGS]uintptr
 	user		tuser_t
-	sigtf		[24]int
-	sigfx		[64]int
+	sigtf		[TFSIZE]uintptr
+	sigfx		[FXREGS]uintptr
 	sigstatus	int
 	siglseepfor	int
 	status		int
@@ -132,8 +135,10 @@ type thread_t struct {
 	//_pad		int
 }
 
+// XXX fix these misleading names
 const(
   TFSIZE       = 24
+  FXREGS       = 64
   TFREGS       = 17
   TF_SYSRSP    = 0
   TF_FSBASE    = 1
@@ -151,7 +156,7 @@ const(
   TF_RSP       = TFREGS + 5
   TF_SS        = TFREGS + 6
   TF_RFLAGS    = TFREGS + 4
-    TF_FL_IF     = 1 << 9
+    TF_FL_IF     uintptr = 1 << 9
 )
 
 var Pspercycle uint
@@ -536,6 +541,7 @@ func invlpg(uintptr)
 func lap_eoi()
 func sched_run(*thread_t)
 func sched_halt()
+func rflags() uintptr
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -708,6 +714,8 @@ const (
 	TSS	uint32 = (0x09 << 8)
 	USER	uint32 = (0x60 << 8)
 	INT	uint16 = (0x0e << 8)
+
+	KCODE64		= 1
 )
 
 var _segs = [7 + 2*MAXCPUS]seg64_t{
@@ -818,6 +826,8 @@ func hexdump(_p unsafe.Pointer, sz uintptr) {
 	}
 }
 
+// must be nosplit since stack splitting prologue uses FS which this function
+// initializes.
 //go:nosplit
 func seg_setup() {
 	p := pdesc_t{}
@@ -901,14 +911,13 @@ func int_set(idx int, intentry func(), istn int) {
 	var f func()
 	f = intentry
 	entry := **(**uint)(unsafe.Pointer(&f))
-	kcode64 := 1
 
 	p := &_idt[idx]
 	p.baselow = uint16(entry)
 	p.basemid = uint16(entry >> 16)
 	p.basehi = uint32(entry >> 32)
 
-	p.segsel = uint16(kcode64 << 3)
+	p.segsel = uint16(KCODE64 << 3)
 
 	p.details = uint16(P) | INT | uint16(istn & 0x7)
 }
@@ -995,6 +1004,9 @@ const (
 	VTEMP		uintptr = 0x43
 )
 
+// physical address of kernel's pmap, given to us by bootloader
+var p_kpmap uintptr
+
 //go:nosplit
 func pml4x(va uintptr) uintptr {
 	return (va >> 39) & 0x1ff
@@ -1182,6 +1194,7 @@ func alloc_map(va uintptr, perms uintptr, fempty bool) {
 const fxwords = 512/8
 var fxinit [fxwords]uintptr
 
+// nosplit because APs call this function before FS is setup
 //go:nosplit
 func fpuinit(amfirst bool) {
 	finit()
@@ -1207,7 +1220,6 @@ func fpuinit(amfirst bool) {
 	}
 }
 
-//go:nosplit
 func find_empty(sz uintptr) uintptr {
 	v := caddr(0, 0, 0, 1, 0)
 	cantuse := uintptr(0xf0)
@@ -1232,7 +1244,6 @@ func find_empty(sz uintptr) uintptr {
 	}
 }
 
-//go:nosplit
 func prot_none(v, sz uintptr) {
 	for i := uintptr(0); i < sz; i += PGSIZE {
 		pte := pgdir_walk(v + i, true)
@@ -1255,7 +1266,6 @@ func Pml4freeze() {
 	_nopml4 = true
 }
 
-//go:nosplit
 func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
     fd int32, offset int32) uintptr {
 	fl := Pushcli()
@@ -1324,7 +1334,6 @@ out:
 	return ret
 }
 
-//go:nosplit
 func hack_munmap(v, _sz uintptr) {
 	fl := Pushcli()
 	splock(maplock)
@@ -1433,3 +1442,85 @@ const (
 	TRAP_TIMER	= 32
 	TRAP_YIELD	= 49
 )
+
+func clone_wrap(rip uintptr) {
+	clone_call(rip)
+	G_pancake("clone_wrap returned", 0)
+}
+
+func hack_clone(flags uint32, rsp uintptr, mp *m, gp *g, fn uintptr) {
+	CLONE_VM := 0x100
+	CLONE_FS := 0x200
+	CLONE_FILES := 0x400
+	CLONE_SIGHAND := 0x800
+	CLONE_THREAD := 0x10000
+	chk := uint32(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND |
+	    CLONE_THREAD)
+	if flags != chk {
+		G_pancake("unexpected clone args", uintptr(flags))
+	}
+	var dur func(uintptr)
+	dur = clone_wrap
+	cloneaddr := **(**uintptr)(unsafe.Pointer(&dur))
+
+	fl := Pushcli()
+	splock(threadlock)
+
+	ti := thread_avail()
+	// provide fn as arg to clone_wrap
+	rsp -= 8
+	*(*uintptr)(unsafe.Pointer(rsp)) = fn
+	rsp -= 8
+	// bad return address
+	*(*uintptr)(unsafe.Pointer(rsp)) = 0
+
+	mt := &threads[ti]
+	memclr(unsafe.Pointer(mt), unsafe.Sizeof(thread_t{}))
+	mt.tf[TF_CS] = KCODE64 << 3
+	mt.tf[TF_RSP] = rsp
+	mt.tf[TF_RIP] = cloneaddr
+	mt.tf[TF_RFLAGS] = rflags() | TF_FL_IF
+	mt.tf[TF_FSBASE] = uintptr(unsafe.Pointer(&mp.tls[0])) + 16
+
+	gp.m = mp
+	mp.tls[0] = uintptr(unsafe.Pointer(gp))
+	mp.procid = uint64(ti)
+	mt.status = ST_RUNNABLE
+	mt.p_pmap = p_kpmap
+
+	mt.fx = fxinit
+
+	spunlock(threadlock)
+	Popcli(fl)
+}
+
+var threadlock = &spinlock_t{}
+
+// maximum # of runtime "OS" threads
+const maxthreads = 64
+var threads [maxthreads]thread_t
+
+const (
+	ST_INVALID	= 0
+	ST_RUNNABLE	= 1
+	ST_RUNNING	= 2
+	ST_WAITING	= 3
+	ST_SLEEPING	= 4
+	ST_WILLSLEEP	= 5
+)
+
+func thread_avail() int {
+	if rflags() & TF_FL_IF != 0 {
+		G_pancake("must not be interruptible", 0)
+	}
+	if threadlock.v == 0 {
+		G_pancake("must hold threadlock", 0)
+	}
+	for i := range threads {
+		if threads[i].status == ST_INVALID {
+			return i
+		}
+	}
+	G_pancake("no available threads", maxthreads)
+	return -1
+}

commit fabac2066375972d19f23a2997a9718b9f813e4d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 8 13:05:37 2016 -0500

    convert and fix fake munmap and tlb shootdown code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f344857261..eb5f3fc847 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -46,8 +46,6 @@ func Rrsp() int
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
 func Sti()
-func Tlbadmit(int, int, int, int) uint
-func Tlbwait(uint)
 func Vtop(*[512]int) int
 
 func Crash()
@@ -90,7 +88,7 @@ var gcticks uint64
 
 type cpu_t struct {
 	this		uint
-	mythread	uint
+	mythread	*thread_t
 	rsp		uintptr
 	num		uint
 	pmap		*[512]int
@@ -130,7 +128,7 @@ type thread_t struct {
 	sleepfor	int
 	sleepret	int
 	futaddr		int
-	pmap		int
+	p_pmap		uintptr
 	//_pad		int
 }
 
@@ -175,11 +173,11 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
 	// while other cpus execute user programs.
 	entersyscall()
 	fl := Pushcli()
-	ct := (*thread_t)(unsafe.Pointer(uintptr(Gscpu().mythread)))
+	cpu := Gscpu()
+	ct := cpu.mythread
 
 	// set shadow pointers for user pmap so it isn't free'd out from under
 	// us if the process terminates soon
-	cpu := Gscpu()
 	cpu.pmap = pmap
 	cpu.pms = pms
 	//cpu.pid = uintptr(pid)
@@ -535,6 +533,9 @@ var Halt uint32
 // TEMPORARY CRAP
 func _pmsg(*int8)
 func invlpg(uintptr)
+func lap_eoi()
+func sched_run(*thread_t)
+func sched_halt()
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -562,9 +563,8 @@ func spunlock(l *spinlock_t) {
 // conditions), interrupts must be cleared before attempting to take this lock.
 var pmsglock = &spinlock_t{}
 
-// msg must be utf-8 string
 //go:nosplit
-func G_pmsg(msg string) {
+func _G_pmsg(msg string) {
 	putch(' ');
 	// can't use range since it results in calls stringiter2 which has the
 	// stack splitting proglogue
@@ -573,6 +573,16 @@ func G_pmsg(msg string) {
 	}
 }
 
+// msg must be utf-8 string
+//go:nosplit
+func G_pmsg(msg string) {
+	fl := Pushcli()
+	splock(pmsglock)
+	_G_pmsg(msg)
+	spunlock(pmsglock)
+	Popcli(fl)
+}
+
 //go:nosplit
 func _pnum(n uintptr) {
 	putch(' ')
@@ -595,13 +605,15 @@ func pnum(n uintptr) {
 	Popcli(fl)
 }
 
+func pmsg(msg *int8)
+
 //go:nosplit
 func pancake(msg *int8, addr uintptr) {
 	cli()
 	atomicstore(&Halt, 1)
 	_pmsg(msg)
 	_pnum(addr)
-	G_pmsg("PANCAKE")
+	_G_pmsg("PANCAKE")
 	for {
 		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
 		*p = 0x1400 | 'F'
@@ -611,9 +623,9 @@ func pancake(msg *int8, addr uintptr) {
 func G_pancake(msg string, addr uintptr) {
 	cli()
 	atomicstore(&Halt, 1)
-	G_pmsg(msg)
+	_G_pmsg(msg)
 	_pnum(addr)
-	G_pmsg("PANCAKE")
+	_G_pmsg("PANCAKE")
 	for {
 		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
 		*p = 0x1400 | 'F'
@@ -1198,13 +1210,15 @@ func fpuinit(amfirst bool) {
 //go:nosplit
 func find_empty(sz uintptr) uintptr {
 	v := caddr(0, 0, 0, 1, 0)
+	cantuse := uintptr(0xf0)
 	for {
 		pte := pgdir_walk(v, false)
-		if pte == nil || *pte & PTE_P == 0 {
+		if pte == nil || (*pte != cantuse && *pte & PTE_P == 0) {
 			failed := false
 			for i := uintptr(0); i < sz; i += PGSIZE {
 				pte = pgdir_walk(v + i, false)
-				if pte != nil && *pte & PTE_P != 0 {
+				if pte != nil &&
+				    (*pte & PTE_P != 0 || *pte == cantuse) {
 					failed = true
 					v += i
 					break
@@ -1235,7 +1249,11 @@ var maplock = &spinlock_t{}
 // kernel's pmap. we want to make sure all kernel mappings added after bootup
 // fall into the same pml4 entry so that all the kernel mappings can be easily
 // shared in user process pmaps.
-var No_pml4 int
+var _nopml4 bool
+
+func Pml4freeze() {
+	_nopml4 = true
+}
 
 //go:nosplit
 func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
@@ -1285,7 +1303,7 @@ func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
 		perms |= PTE_W
 	}
 
-	if No_pml4 != 0 {
+	if _nopml4 {
 		eidx := pml4x(va + sz - 1)
 		for sidx := pml4x(va); sidx <= eidx; sidx++ {
 			pml4 := caddr(VREC, VREC, VREC, VREC, sidx)
@@ -1305,3 +1323,113 @@ out:
 	Popcli(fl)
 	return ret
 }
+
+//go:nosplit
+func hack_munmap(v, _sz uintptr) {
+	fl := Pushcli()
+	splock(maplock)
+	sz := pgroundup(_sz)
+	cantuse := uintptr(0xf0)
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		va := v + i
+		pte := pgdir_walk(va, false)
+		if pml4x(va) >= VUEND {
+			G_pancake("high unmap", va)
+		}
+		// XXX goodbye, memory
+		if pte != nil && *pte & PTE_P != 0 {
+			// make sure these pages aren't remapped via
+			// hack_munmap
+			*pte = cantuse
+			invlpg(va)
+		}
+	}
+	G_pmsg("POOF\n")
+	spunlock(maplock)
+	Popcli(fl)
+}
+
+var tlbshoot_wait uintptr
+var tlbshoot_pg uintptr
+var tlbshoot_count uintptr
+var tlbshoot_pmap uintptr
+var tlbshoot_gen uint64
+
+func Tlbadmit(p_pmap, cpuwait, pg, pgcount uintptr) uint64 {
+	for !casuintptr(&tlbshoot_wait, 0, cpuwait) {
+		preemptok()
+	}
+	xchguintptr(&tlbshoot_pg, pg)
+	xchguintptr(&tlbshoot_count, pgcount)
+	xchguintptr(&tlbshoot_pmap, p_pmap)
+	xadd64(&tlbshoot_gen, 1)
+	return tlbshoot_gen
+}
+
+func Tlbwait(gen uint64) {
+	for atomicloaduintptr(&tlbshoot_wait) != 0 {
+		if atomicload64(&tlbshoot_gen) != gen {
+			break
+		}
+	}
+}
+
+// must be nosplit since called at interrupt time
+//go:nosplit
+func tlb_shootdown() {
+	lap_eoi()
+	ct := Gscpu().mythread
+	if ct != nil && ct.p_pmap == tlbshoot_pmap {
+		// the TLB was already invalidated since trap() currently
+		// switches to kernel pmap on any exception/interrupt other
+		// than NMI.
+		//start := tlbshoot_pg
+		//end := tlbshoot_pg + tlbshoot_count * PGSIZE
+		//for ; start < end; start += PGSIZE {
+		//	invlpg(start)
+		//}
+	}
+	dur := (*uint64)(unsafe.Pointer(&tlbshoot_wait))
+	v := xadd64(dur, -1)
+	if v < 0 {
+		G_pancake("shootwait < 0", uintptr(v))
+	}
+	if ct != nil {
+		sched_run(ct)
+	} else {
+		sched_halt()
+	}
+}
+
+// this function checks to see if another thread is trying to preempt this
+// thread (perhaps to start a GC). this is called when go code is spinning on a
+// spinlock in order to avoid a deadlock where the thread that acquired the
+// spinlock starts a GC and waits forever for the spinning thread. (go code
+// should probably not use spinlocks. tlb shootdown code is the only code
+// protected by a spinlock since the lock must be acquired in interrupt
+// context.)
+//
+// alternatively, we could make sure that no allocations are made while the
+// spinlock is acquired.
+func preemptok() {
+	gp := getg()
+	StackPreempt := uintptr(0xfffffffffffffade)
+	if gp.stackguard0 == StackPreempt {
+		G_pmsg("!")
+		// call function with stack splitting prologue
+		_dummy()
+	}
+}
+
+var _notdeadcode uint32
+func _dummy() {
+	if _notdeadcode != 0 {
+		_dummy()
+	}
+	_notdeadcode = 0
+}
+
+const (
+	TRAP_TIMER	= 32
+	TRAP_YIELD	= 49
+)

commit 52206a8a4e651d7bf860ffe777e620e49e857753
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 22:29:40 2016 -0500

    convert runtime's fake mmap
    
    all this physical memory management code will probably go away soon

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 19d21ae925..f344857261 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -78,7 +78,6 @@ func lcr4(uintptr)
 
 // os_linux.c
 var gcticks uint64
-var No_pml4 int
 
 // we have to carefully write go code that may be executed early (during boot)
 // or in interrupt context. such code cannot allocate or call functions that
@@ -976,6 +975,8 @@ const (
 
 	// special pml4 slots, agreed upon with the bootloader (which creates
 	// our pmap).
+	// highest runtime heap mapping
+	VUEND		uintptr = 0x42
 	// recursive mapping
 	VREC		uintptr = 0x42
 	// available mapping
@@ -1193,3 +1194,114 @@ func fpuinit(amfirst bool) {
 		G_pmsg("VERIFY FX FOR THREADS\n")
 	}
 }
+
+//go:nosplit
+func find_empty(sz uintptr) uintptr {
+	v := caddr(0, 0, 0, 1, 0)
+	for {
+		pte := pgdir_walk(v, false)
+		if pte == nil || *pte & PTE_P == 0 {
+			failed := false
+			for i := uintptr(0); i < sz; i += PGSIZE {
+				pte = pgdir_walk(v + i, false)
+				if pte != nil && *pte & PTE_P != 0 {
+					failed = true
+					v += i
+					break
+				}
+			}
+			if !failed {
+				return v
+			}
+		}
+		v += PGSIZE
+	}
+}
+
+//go:nosplit
+func prot_none(v, sz uintptr) {
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		pte := pgdir_walk(v + i, true)
+		if pte != nil {
+			*pte = *pte & ^PTE_P
+			invlpg(v + i)
+		}
+	}
+}
+
+var maplock = &spinlock_t{}
+
+// this flag makes hack_mmap panic if a new pml4 entry is ever added to the
+// kernel's pmap. we want to make sure all kernel mappings added after bootup
+// fall into the same pml4 entry so that all the kernel mappings can be easily
+// shared in user process pmaps.
+var No_pml4 int
+
+//go:nosplit
+func hack_mmap(va, _sz uintptr, _prot uint32, _flags uint32,
+    fd int32, offset int32) uintptr {
+	fl := Pushcli()
+	splock(maplock)
+
+	MAP_ANON := uintptr(0x20)
+	MAP_PRIVATE := uintptr(0x2)
+	PROT_NONE := uintptr(0x0)
+	PROT_WRITE := uintptr(0x2)
+
+	prot := uintptr(_prot)
+	flags := uintptr(_flags)
+	var vaend uintptr
+	var perms uintptr
+	var ret uintptr
+	var t uintptr
+	pgleft := pglast - pgfirst
+	sz := pgroundup(_sz)
+	if sz > pgleft {
+		ret = ^uintptr(0)
+		goto out
+	}
+	sz = pgroundup(va + _sz)
+	sz -= pgrounddown(va)
+	if va == 0 {
+		va = find_empty(sz)
+	}
+	vaend = caddr(VUEND, 0, 0, 0, 0)
+	if va >= vaend || va + sz >= vaend {
+		G_pancake("va space exhausted", va)
+	}
+
+	t = MAP_ANON | MAP_PRIVATE
+	if flags & t != t {
+		G_pancake("unexpected flags", flags)
+	}
+	perms = PTE_P
+	if prot == PROT_NONE {
+		prot_none(va, sz)
+		ret = va
+		goto out
+	}
+
+	if prot & PROT_WRITE != 0 {
+		perms |= PTE_W
+	}
+
+	if No_pml4 != 0 {
+		eidx := pml4x(va + sz - 1)
+		for sidx := pml4x(va); sidx <= eidx; sidx++ {
+			pml4 := caddr(VREC, VREC, VREC, VREC, sidx)
+			pml4e := (*uintptr)(unsafe.Pointer(pml4))
+			if *pml4e & PTE_P == 0 {
+				G_pancake("new pml4 entry to kernel pmap", va)
+			}
+		}
+	}
+
+	for i := uintptr(0); i < sz; i += PGSIZE {
+		alloc_map(va + i, perms, true)
+	}
+	ret = va
+out:
+	spunlock(maplock)
+	Popcli(fl)
+	return ret
+}

commit c25f8fd81340dac2232dda30103b50c15ee9a1b2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 16:58:16 2016 -0500

    convert/cleanup pmap, FPU, and physical memory code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3855f7d5fb..19d21ae925 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -61,14 +61,20 @@ func Trapwake()
 
 func inb(int) int
 func htpause()
+func finit()
 func fs_null()
+func fxsave(*[fxwords]uintptr)
 func gs_null()
 func lgdt(pdesc_t)
 func lidt(pdesc_t)
 func cli()
 func sti()
-func ltr(int)
+func ltr(uint)
 func lap_id() int
+func rcr0() uintptr
+func rcr4() uintptr
+func lcr0(uintptr)
+func lcr4(uintptr)
 
 // os_linux.c
 var gcticks uint64
@@ -529,7 +535,7 @@ var Halt uint32
 
 // TEMPORARY CRAP
 func _pmsg(*int8)
-func alloc_map(uintptr, uint, int)
+func invlpg(uintptr)
 
 // wait until remove definition from proc.c
 //type spinlock_t struct {
@@ -569,7 +575,7 @@ func G_pmsg(msg string) {
 }
 
 //go:nosplit
-func _pnum(n uint) {
+func _pnum(n uintptr) {
 	putch(' ')
 	for i := 60; i >= 0; i -= 4 {
 		cn := (n >> uint(i)) & 0xf
@@ -582,7 +588,7 @@ func _pnum(n uint) {
 }
 
 //go:nosplit
-func pnum(n uint) {
+func pnum(n uintptr) {
 	fl := Pushcli()
 	splock(pmsglock)
 	_pnum(n)
@@ -591,7 +597,7 @@ func pnum(n uint) {
 }
 
 //go:nosplit
-func pancake(msg *int8, addr uint) {
+func pancake(msg *int8, addr uintptr) {
 	cli()
 	atomicstore(&Halt, 1)
 	_pmsg(msg)
@@ -603,7 +609,7 @@ func pancake(msg *int8, addr uint) {
 	}
 }
 //go:nosplit
-func G_pancake(msg string, addr uint) {
+func G_pancake(msg string, addr uintptr) {
 	cli()
 	atomicstore(&Halt, 1)
 	G_pmsg(msg)
@@ -617,8 +623,8 @@ func G_pancake(msg string, addr uint) {
 
 
 //go:nosplit
-func chkalign(_p unsafe.Pointer, n uint) {
-	p := uint(uintptr(_p))
+func chkalign(_p unsafe.Pointer, n uintptr) {
+	p := uintptr(_p)
 	if p & (n - 1) != 0 {
 		G_pancake("not aligned", p)
 	}
@@ -627,7 +633,7 @@ func chkalign(_p unsafe.Pointer, n uint) {
 //go:nosplit
 func chksize(n uintptr, exp uintptr) {
 	if n != exp {
-		G_pancake("size mismatch", uint(n))
+		G_pancake("size mismatch", n)
 	}
 }
 
@@ -718,7 +724,7 @@ var _segs = [7 + 2*MAXCPUS]seg64_t{
 var _tss [MAXCPUS]tss_t
 
 //go:nosplit
-func tss_set(id int, rsp, nmi uintptr) *tss_t {
+func tss_set(id uint, rsp, nmi uintptr) *tss_t {
 	sz := unsafe.Sizeof(_tss[id])
 	if sz != 104 + 8 {
 		panic("bad tss_t")
@@ -742,12 +748,12 @@ func tss_set(id int, rsp, nmi uintptr) *tss_t {
 
 // maps cpu number to the per-cpu TSS segment descriptor in the GDT
 //go:nosplit
-func segnum(cpunum int) int {
+func segnum(cpunum uint) uint {
 	return 7 + 2*cpunum
 }
 
 //go:nosplit
-func tss_seginit(cpunum int, _tssaddr *tss_t, lim uintptr) {
+func tss_seginit(cpunum uint, _tssaddr *tss_t, lim uintptr) {
 	seg := &_segs[segnum(cpunum)]
 	seg.rest = P | TSS | G
 
@@ -765,13 +771,13 @@ func tss_seginit(cpunum int, _tssaddr *tss_t, lim uintptr) {
 }
 
 //go:nosplit
-func tss_init(cpunum int) uintptr {
-	intstk := uintptr(0xa100001000 + cpunum*4*PGSIZE)
-	nmistk := uintptr(0xa100003000 + cpunum*4*PGSIZE)
+func tss_init(cpunum uint) uintptr {
+	intstk := 0xa100001000 + uintptr(cpunum)*4*PGSIZE
+	nmistk := 0xa100003000 + uintptr(cpunum)*4*PGSIZE
 	// BSP maps AP's stack for them
 	if cpunum == 0 {
-		alloc_map(intstk - 1, PTE_W, 1)
-		alloc_map(nmistk - 1, PTE_W, 1)
+		alloc_map(intstk - 1, PTE_W, true)
+		alloc_map(nmistk - 1, PTE_W, true)
 	}
 	rsp := intstk
 	rspnmi := nmistk
@@ -794,10 +800,10 @@ func pdsetup(pd *pdesc_t, _addr unsafe.Pointer, lim uintptr) {
 }
 
 //go:nosplit
-func dur(_p unsafe.Pointer, sz uintptr) {
+func hexdump(_p unsafe.Pointer, sz uintptr) {
 	for i := uintptr(0); i < sz; i++ {
 		p := (*uint8)(unsafe.Pointer(uintptr(_p) + i))
-		_pnum(uint(*p))
+		_pnum(uintptr(*p))
 	}
 }
 
@@ -809,7 +815,7 @@ func seg_setup() {
 	lgdt(p)
 
 	// now that we have a GDT, setup tls for the first thread.
-	// elf tls defines user tls at -16(%fs)
+	// elf tls specification defines user tls at -16(%fs)
 	t := uintptr(unsafe.Pointer(&tls0[0]))
 	tlsaddr := int(t + 16)
 	// we must set fs/gs at least once before we use the MSRs to change
@@ -960,9 +966,230 @@ func int_setup() {
 }
 
 const (
-	PGSIZE	= 1 << 12
-	PTE_P	= 1 << 0
-	PTE_W	= 1 << 1
-	PTE_U	= 1 << 2
-	PTE_PCD	= 1 << 4
+	PTE_P		uintptr = 1 << 0
+	PTE_W		uintptr = 1 << 1
+	PTE_U		uintptr = 1 << 2
+	PTE_PCD		uintptr = 1 << 4
+	PGSIZE		uintptr = 1 << 12
+	PGOFFMASK	uintptr = PGSIZE - 1
+	PGMASK		uintptr = ^PGOFFMASK
+
+	// special pml4 slots, agreed upon with the bootloader (which creates
+	// our pmap).
+	// recursive mapping
+	VREC		uintptr = 0x42
+	// available mapping
+	VTEMP		uintptr = 0x43
 )
+
+//go:nosplit
+func pml4x(va uintptr) uintptr {
+	return (va >> 39) & 0x1ff
+}
+
+//go:nosplit
+func slotnext(va uintptr) uintptr {
+	return ((va << 9) & ((1 << 48) - 1))
+}
+
+//go:nosplit
+func pgroundup(va uintptr) uintptr {
+	return (va + PGSIZE - 1) & PGMASK
+}
+
+//go:nosplit
+func pgrounddown(va uintptr) uintptr {
+	return va & PGMASK
+}
+
+//go:nosplit
+func caddr(l4 uintptr, ppd uintptr, pd uintptr, pt uintptr,
+    off uintptr) uintptr {
+	ret := l4 << 39 | ppd << 30 | pd << 21 | pt << 12
+	ret += off*8
+	return uintptr(ret)
+}
+
+// XXX XXX XXX get rid of create
+//go:nosplit
+func pgdir_walk(_va uintptr, create bool) *uintptr {
+	v := pgrounddown(_va)
+	if v == 0 && create {
+		G_pancake("map zero pg", _va)
+	}
+	slot0 := pml4x(v)
+	if slot0 == VREC {
+		G_pancake("map in VREC", _va)
+	}
+	pml4 := caddr(VREC, VREC, VREC, VREC, slot0)
+	return pgdir_walk1(pml4, slotnext(v), create)
+}
+
+//go:nosplit
+func pgdir_walk1(slot, van uintptr, create bool) *uintptr {
+	ns := slotnext(slot)
+	ns += pml4x(van)*8
+	if pml4x(ns) != VREC {
+		return (*uintptr)(unsafe.Pointer(slot))
+	}
+	sp := (*uintptr)(unsafe.Pointer(slot))
+	if *sp & PTE_P == 0 {
+		if !create{
+			return nil
+		}
+		p_pg := get_pg()
+		zero_phys(p_pg)
+		*sp = p_pg | PTE_P | PTE_W
+	}
+	return pgdir_walk1(ns, slotnext(van), create)
+}
+
+//go:nosplit
+func zero_phys(_phys uintptr) {
+	rec := caddr(VREC, VREC, VREC, VREC, VTEMP)
+	pml4 := (*uintptr)(unsafe.Pointer(rec))
+	if *pml4 & PTE_P != 0 {
+		G_pancake("vtemp in use", *pml4)
+	}
+	phys := pgrounddown(_phys)
+	*pml4 = phys | PTE_P | PTE_W
+	_tva := caddr(VREC, VREC, VREC, VTEMP, 0)
+	tva := unsafe.Pointer(_tva)
+	memclr(tva, PGSIZE)
+	*pml4 = 0
+	invlpg(_tva)
+}
+
+// this physical allocation code is temporary. biscuit probably shouldn't
+// bother resizing its heap, ever. instead of providing a fake mmap to the
+// runtime, the runtime should simply mmap its entire heap during
+// initialization according to the amount of available memory.
+//
+// XXX when you write the new code, check and see if we can use ACPI to find
+// available memory instead of e820. since e820 is only usable in realmode, we
+// have to have e820 code in the bootloader. it would be better to have such
+// memory management code in the kernel and not the bootloader.
+
+type e820_t struct {
+	start	uintptr
+	len	uintptr
+}
+
+// "secret structure". created by bootloader for passing info to the kernel.
+type secret_t struct {
+	e820p	uintptr
+	pmap	uintptr
+	freepg	uintptr
+}
+
+// regions of memory not included in the e820 map, into which we cannot
+// allocate
+type badregion_t struct {
+	start	uintptr
+	end	uintptr
+}
+
+var badregs = []badregion_t{
+	// VGA
+	{0xa0000, 0x100000},
+	// secret storage
+	{0x7000, 0x8000},
+}
+
+//go:nosplit
+func skip_bad(cur uintptr) uintptr {
+	for _, br := range badregs {
+		if cur >= br.start && cur < br.end {
+			return br.end
+		}
+	}
+	return cur
+}
+
+var pgfirst uintptr
+var pglast uintptr
+
+//go:nosplit
+func phys_init() {
+	sec := (*secret_t)(unsafe.Pointer(uintptr(0x7c00)))
+	found := false
+	base := sec.e820p
+	// bootloader provides 15 e820 entries at most (it panicks if the PC
+	// provides more).
+	for i := uintptr(0); i < 15; i++ {
+		ep := (*e820_t)(unsafe.Pointer(base + i*28))
+		if ep.len == 0 {
+			continue
+		}
+		endpg := ep.start + ep.len
+		if pgfirst >= ep.start && pgfirst < endpg {
+			pglast = endpg
+			found = true
+			break
+		}
+	}
+	if !found {
+		G_pancake("e820 problems", pgfirst)
+	}
+	if pgfirst & PGOFFMASK != 0 {
+		G_pancake("pgfist not aligned", pgfirst)
+	}
+}
+
+//go:nosplit
+func get_pg() uintptr {
+	if pglast == 0 {
+		phys_init()
+	}
+	pgfirst = skip_bad(pgfirst)
+	if pgfirst >= pglast {
+		G_pancake("oom", pglast)
+	}
+	ret := pgfirst
+	pgfirst += PGSIZE
+	return ret
+}
+
+//go:nosplit
+func alloc_map(va uintptr, perms uintptr, fempty bool) {
+	pte := pgdir_walk(va, true)
+	old := *pte
+	if old & PTE_P != 0 && fempty {
+		G_pancake("expected empty pte", old)
+	}
+	p_pg := get_pg()
+	zero_phys(p_pg)
+	// XXX goodbye, memory
+	*pte = p_pg | perms | PTE_P
+	if old & PTE_P != 0 {
+		invlpg(va)
+	}
+}
+
+const fxwords = 512/8
+var fxinit [fxwords]uintptr
+
+//go:nosplit
+func fpuinit(amfirst bool) {
+	finit()
+	cr0 := rcr0()
+	// clear EM
+	cr0 &^= (1 << 2)
+	// set MP
+	cr0 |= 1 << 1
+	lcr0(cr0);
+
+	cr4 := rcr4()
+	// set OSFXSR
+	cr4 |= 1 << 9
+	lcr4(cr4);
+
+	if amfirst {
+		chkalign(unsafe.Pointer(&fxinit[0]), 16)
+		fxsave(&fxinit)
+
+		// XXX XXX XXX XXX XXX XXX XXX dont forget to do this once
+		// thread code is converted to go
+		G_pmsg("VERIFY FX FOR THREADS\n")
+	}
+}

commit a462abcffb2d074d365b07930e844e62b852af99
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 12:27:01 2016 -0500

    convert and cleanup interrupt init code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a67ca23734..3855f7d5fb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -63,6 +63,8 @@ func inb(int) int
 func htpause()
 func fs_null()
 func gs_null()
+func lgdt(pdesc_t)
+func lidt(pdesc_t)
 func cli()
 func sti()
 func ltr(int)
@@ -529,6 +531,7 @@ var Halt uint32
 func _pmsg(*int8)
 func alloc_map(uintptr, uint, int)
 
+// wait until remove definition from proc.c
 //type spinlock_t struct {
 //	v	uint32
 //}
@@ -550,6 +553,10 @@ func spunlock(l *spinlock_t) {
 	atomicstore(&l.v, 0)
 }
 
+// since this lock may be taken during an interrupt (only under fatal error
+// conditions), interrupts must be cleared before attempting to take this lock.
+var pmsglock = &spinlock_t{}
+
 // msg must be utf-8 string
 //go:nosplit
 func G_pmsg(msg string) {
@@ -574,6 +581,15 @@ func _pnum(n uint) {
 	}
 }
 
+//go:nosplit
+func pnum(n uint) {
+	fl := Pushcli()
+	splock(pmsglock)
+	_pnum(n)
+	spunlock(pmsglock)
+	Popcli(fl)
+}
+
 //go:nosplit
 func pancake(msg *int8, addr uint) {
 	cli()
@@ -604,7 +620,7 @@ func G_pancake(msg string, addr uint) {
 func chkalign(_p unsafe.Pointer, n uint) {
 	p := uint(uintptr(_p))
 	if p & (n - 1) != 0 {
-		G_pancake("not aligned", n)
+		G_pancake("not aligned", p)
 	}
 }
 
@@ -617,7 +633,11 @@ func chksize(n uintptr, exp uintptr) {
 
 type pdesc_t struct {
 	limit	uint16
-	addr	[8]uint8
+	addrlow uint16
+	addrmid	uint32
+	addrhi	uint16
+	_res1	uint16
+	_res2	uint32
 }
 
 type seg64_t struct {
@@ -670,10 +690,9 @@ const (
 	DATA	uint32 = (0x02 << 8)
 	TSS	uint32 = (0x09 << 8)
 	USER	uint32 = (0x60 << 8)
+	INT	uint16 = (0x0e << 8)
 )
 
-var _pgdt pdesc_t
-
 var _segs = [7 + 2*MAXCPUS]seg64_t{
 	// 0: null segment
 	{0, 0, 0},
@@ -721,7 +740,7 @@ func tss_set(id int, rsp, nmi uintptr) *tss_t {
 	return p
 }
 
-// maps cpu number to TSS segment descriptor in the GDT
+// maps cpu number to the per-cpu TSS segment descriptor in the GDT
 //go:nosplit
 func segnum(cpunum int) int {
 	return 7 + 2*cpunum
@@ -730,7 +749,7 @@ func segnum(cpunum int) int {
 //go:nosplit
 func tss_seginit(cpunum int, _tssaddr *tss_t, lim uintptr) {
 	seg := &_segs[segnum(cpunum)]
-	seg.rest |= P | TSS | G
+	seg.rest = P | TSS | G
 
 	seg.lim = uint16(lim)
 	seg.rest |= uint32((lim >> 16) & 0xf) << 16
@@ -766,11 +785,12 @@ func tss_init(cpunum int) uintptr {
 
 //go:nosplit
 func pdsetup(pd *pdesc_t, _addr unsafe.Pointer, lim uintptr) {
+	chkalign(_addr, 8)
 	addr := uintptr(_addr)
 	pd.limit = uint16(lim)
-	for i := uint(0); i < 8; i++ {
-		pd.addr[i] = uint8((addr >> (i*8)) & 0xff)
-	}
+	pd.addrlow = uint16(addr)
+	pd.addrmid = uint32(addr >> 16)
+	pd.addrhi = uint16(addr >> 48)
 }
 
 //go:nosplit
@@ -782,29 +802,163 @@ func dur(_p unsafe.Pointer, sz uintptr) {
 }
 
 //go:nosplit
-func segsetup() *pdesc_t {
-	chksize(unsafe.Sizeof(_pgdt), 10)
+func seg_setup() {
+	p := pdesc_t{}
 	chksize(unsafe.Sizeof(seg64_t{}), 8)
-	chkalign(unsafe.Pointer(&_pgdt), 8)
-	pdsetup(&_pgdt, unsafe.Pointer(&_segs[0]), unsafe.Sizeof(_segs) - 1)
-	return &_pgdt
-}
+	pdsetup(&p, unsafe.Pointer(&_segs[0]), unsafe.Sizeof(_segs) - 1)
+	lgdt(p)
 
-//go:nosplit
-func fs0init(tls0 uintptr) {
+	// now that we have a GDT, setup tls for the first thread.
 	// elf tls defines user tls at -16(%fs)
-	tlsaddr := int(tls0 + 16)
-	// we must set fs/gs, the only segment descriptors in ia32e mode, at
-	// least once before we use the MSRs to change their base address. the
-	// MSRs write directly to hidden segment descriptor cache, and if we
-	// don't explicitly fill the segment descriptor cache, the writes to
-	// the MSRs are thrown out (presumably because the caches are thought
-	// to be invalid).
+	t := uintptr(unsafe.Pointer(&tls0[0]))
+	tlsaddr := int(t + 16)
+	// we must set fs/gs at least once before we use the MSRs to change
+	// their base address. the MSRs write directly to hidden segment
+	// descriptor cache, and if we don't explicitly fill the segment
+	// descriptor cache, the writes to the MSRs are thrown out (presumably
+	// because the caches are thought to be invalid).
 	fs_null()
 	ia32_fs_base := 0xc0000100
 	Wrmsr(ia32_fs_base, tlsaddr)
 }
 
+// interrupt entries, defined in runtime/asm_amd64.s
+func Xdz()
+func Xrz()
+func Xnmi()
+func Xbp()
+func Xov()
+func Xbnd()
+func Xuo()
+func Xnm()
+func Xdf()
+func Xrz2()
+func Xtss()
+func Xsnp()
+func Xssf()
+func Xgp()
+func Xpf()
+func Xrz3()
+func Xmf()
+func Xac()
+func Xmc()
+func Xfp()
+func Xve()
+func Xtimer()
+func Xspur()
+func Xyield()
+func Xsyscall()
+func Xtlbshoot()
+func Xsigret()
+func Xperfmask()
+func Xirq1()
+func Xirq2()
+func Xirq3()
+func Xirq4()
+func Xirq5()
+func Xirq6()
+func Xirq7()
+func Xirq8()
+func Xirq9()
+func Xirq10()
+func Xirq11()
+func Xirq12()
+func Xirq13()
+func Xirq14()
+func Xirq15()
+
+type idte_t struct {
+	baselow	uint16
+	segsel	uint16
+	details	uint16
+	basemid	uint16
+	basehi	uint32
+	_res	uint32
+}
+
+const idtsz uintptr = 128
+var _idt [idtsz]idte_t
+
+//go:nosplit
+func int_set(idx int, intentry func(), istn int) {
+	var f func()
+	f = intentry
+	entry := **(**uint)(unsafe.Pointer(&f))
+	kcode64 := 1
+
+	p := &_idt[idx]
+	p.baselow = uint16(entry)
+	p.basemid = uint16(entry >> 16)
+	p.basehi = uint32(entry >> 32)
+
+	p.segsel = uint16(kcode64 << 3)
+
+	p.details = uint16(P) | INT | uint16(istn & 0x7)
+}
+
+//go:nosplit
+func int_setup() {
+	chksize(unsafe.Sizeof(idte_t{}), 16)
+	chksize(unsafe.Sizeof(_idt), idtsz*16)
+	chkalign(unsafe.Pointer(&_idt[0]), 8)
+
+	// cpu exceptions
+	int_set(0,   Xdz,  0)
+	int_set(1,   Xrz,  0)
+	int_set(2,   Xnmi, 2)
+	int_set(3,   Xbp,  0)
+	int_set(4,   Xov,  0)
+	int_set(5,   Xbnd, 0)
+	int_set(6,   Xuo,  0)
+	int_set(7,   Xnm,  0)
+	int_set(8,   Xdf,  1)
+	int_set(9,   Xrz2, 0)
+	int_set(10,  Xtss, 0)
+	int_set(11,  Xsnp, 0)
+	int_set(12,  Xssf, 0)
+	int_set(13,  Xgp,  1)
+	int_set(14,  Xpf,  1)
+	int_set(15,  Xrz3, 0)
+	int_set(16,  Xmf,  0)
+	int_set(17,  Xac,  0)
+	int_set(18,  Xmc,  0)
+	int_set(19,  Xfp,  0)
+	int_set(20,  Xve,  0)
+
+	// interrupts
+	irqbase := 32
+	int_set(irqbase+ 0,  Xtimer,  1)
+	int_set(irqbase+ 1,  Xirq1,   1)
+	int_set(irqbase+ 2,  Xirq2,   1)
+	int_set(irqbase+ 3,  Xirq3,   1)
+	int_set(irqbase+ 4,  Xirq4,   1)
+	int_set(irqbase+ 5,  Xirq5,   1)
+	int_set(irqbase+ 6,  Xirq6,   1)
+	int_set(irqbase+ 7,  Xirq7,   1)
+	int_set(irqbase+ 8,  Xirq8,   1)
+	int_set(irqbase+ 9,  Xirq9,   1)
+	int_set(irqbase+10,  Xirq10,  1)
+	int_set(irqbase+11,  Xirq11,  1)
+	int_set(irqbase+12,  Xirq12,  1)
+	int_set(irqbase+13,  Xirq13,  1)
+	int_set(irqbase+14,  Xirq14,  1)
+	int_set(irqbase+15,  Xirq15,  1)
+
+	int_set(48,  Xspur,    1)
+	// no longer used
+	//int_set(49,  Xyield,   1)
+	//int_set(64,  Xsyscall, 1)
+
+	int_set(70,  Xtlbshoot, 1)
+	// no longer used
+	//int_set(71,  Xsigret,   1)
+	int_set(72,  Xperfmask, 1)
+
+	p := pdesc_t{}
+	pdsetup(&p, unsafe.Pointer(&_idt[0]), unsafe.Sizeof(_idt) - 1)
+	lidt(p)
+}
+
 const (
 	PGSIZE	= 1 << 12
 	PTE_P	= 1 << 0

commit 9bcf96fb6b3400d4994aab3079f01c0fb0162add
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 7 08:22:19 2016 -0500

    convert and cleanup segmentation code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6168ee18a4..a67ca23734 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -60,14 +60,31 @@ func Gcticks() uint64
 func Trapwake()
 
 func inb(int) int
+func htpause()
+func fs_null()
+func gs_null()
+func cli()
+func sti()
+func ltr(int)
+func lap_id() int
+
 // os_linux.c
 var gcticks uint64
 var No_pml4 int
 
+// we have to carefully write go code that may be executed early (during boot)
+// or in interrupt context. such code cannot allocate or call functions that
+// that have the stack splitting prologue. the following is a list of go code
+// that could result in code that allocates or calls a function with a stack
+// splitting prologue.
+// - function with interface argument (calls convT2E)
+// - taking address of stack variable (may allocate)
+// - using range to iterate over a string (calls stringiter*)
+
 type cpu_t struct {
 	this		uint
 	mythread	uint
-	rsp		uint
+	rsp		uintptr
 	num		uint
 	pmap		*[512]int
 	pms		[]*[512]int
@@ -76,7 +93,9 @@ type cpu_t struct {
 
 var Cpumhz uint
 
-var cpus [32]cpu_t
+const MAXCPUS int = 32
+
+var cpus [MAXCPUS]cpu_t
 
 type tuser_t struct {
 	tf	uintptr
@@ -190,64 +209,6 @@ func shadow_clear() {
 	cpu.pms = nil
 }
 
-type tss_t struct {
-	_res0 uint32
-
-	rsp0l uint32
-	rsp0h uint32
-	rsp1l uint32
-	rsp1h uint32
-	rsp2l uint32
-	rsp2h uint32
-
-	_res1 [2]uint32
-
-	ist1l uint32
-	ist1h uint32
-	ist2l uint32
-	ist2h uint32
-	ist3l uint32
-	ist3h uint32
-	ist4l uint32
-	ist4h uint32
-	ist5l uint32
-	ist5h uint32
-	ist6l uint32
-	ist6h uint32
-	ist7l uint32
-	ist7h uint32
-
-	_res2 [2]uint32
-
-	_res3 uint16
-	iobmap uint16
-	_align uint64
-}
-
-var alltss [32]tss_t
-
-//go:nosplit
-func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
-	sz := unsafe.Sizeof(alltss[id])
-	if sz != 104 + 8 {
-		panic("bad tss_t")
-	}
-	p := &alltss[id]
-	p.rsp0l = uint32(rsp)
-	p.rsp0h = uint32(rsp >> 32)
-
-	p.ist1l = uint32(rsp)
-	p.ist1h = uint32(rsp >> 32)
-
-	p.ist2l = uint32(nmi)
-	p.ist2h = uint32(nmi >> 32)
-
-	p.iobmap = uint16(sz)
-
-	*rsz = sz
-	return uintptr(unsafe.Pointer(p))
-}
-
 type nmiprof_t struct {
 	buf		[]uintptr
 	bufidx		uint64
@@ -370,7 +331,6 @@ func perfgather(tf *[TFSIZE]uintptr) {
 //go:nosplit
 func perfmask() {
 	lapaddr := 0xfee00000
-	const PGSIZE = 1 << 12
 	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
 
 	perfmonc := 208
@@ -557,21 +517,60 @@ func Trapinit() {
 	mcall(trapinit_m)
 }
 
-// TEMPORARY CRAP
 // G_ prefix means a function had to have both C and Go versions while the
-// conversion is underway. remove prefix afterwards.
+// conversion is underway. remove prefix afterwards. we need two versions of
+// functions that take a string as an argument since string literals are
+// different data types in C and Go.
+//
+// XXX XXX many of these do not need nosplits!
 var Halt uint32
 
-func cli()
-func sti()
-func _pnum(uint)
+// TEMPORARY CRAP
 func _pmsg(*int8)
+func alloc_map(uintptr, uint, int)
+
+//type spinlock_t struct {
+//	v	uint32
+//}
+
+//go:nosplit
+func splock(l *spinlock_t) {
+	for {
+		if xchg(&l.v, 1) == 0 {
+			break
+		}
+		for l.v != 0 {
+			htpause()
+		}
+	}
+}
 
+//go:nosplit
+func spunlock(l *spinlock_t) {
+	atomicstore(&l.v, 0)
+}
+
+// msg must be utf-8 string
 //go:nosplit
 func G_pmsg(msg string) {
 	putch(' ');
-	for _, c := range msg {
-		putch(int8(c))
+	// can't use range since it results in calls stringiter2 which has the
+	// stack splitting proglogue
+	for i := 0; i < len(msg); i++ {
+		putch(int8(msg[i]))
+	}
+}
+
+//go:nosplit
+func _pnum(n uint) {
+	putch(' ')
+	for i := 60; i >= 0; i -= 4 {
+		cn := (n >> uint(i)) & 0xf
+		if cn <= 9 {
+			putch(int8('0' + cn))
+		} else {
+			putch(int8('A' + cn - 10))
+		}
 	}
 }
 
@@ -587,3 +586,229 @@ func pancake(msg *int8, addr uint) {
 		*p = 0x1400 | 'F'
 	}
 }
+//go:nosplit
+func G_pancake(msg string, addr uint) {
+	cli()
+	atomicstore(&Halt, 1)
+	G_pmsg(msg)
+	_pnum(addr)
+	G_pmsg("PANCAKE")
+	for {
+		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
+		*p = 0x1400 | 'F'
+	}
+}
+
+
+//go:nosplit
+func chkalign(_p unsafe.Pointer, n uint) {
+	p := uint(uintptr(_p))
+	if p & (n - 1) != 0 {
+		G_pancake("not aligned", n)
+	}
+}
+
+//go:nosplit
+func chksize(n uintptr, exp uintptr) {
+	if n != exp {
+		G_pancake("size mismatch", uint(n))
+	}
+}
+
+type pdesc_t struct {
+	limit	uint16
+	addr	[8]uint8
+}
+
+type seg64_t struct {
+	lim	uint16
+	baselo	uint16
+	rest	uint32
+}
+
+type tss_t struct {
+	_res0 uint32
+
+	rsp0l uint32
+	rsp0h uint32
+	rsp1l uint32
+	rsp1h uint32
+	rsp2l uint32
+	rsp2h uint32
+
+	_res1 [2]uint32
+
+	ist1l uint32
+	ist1h uint32
+	ist2l uint32
+	ist2h uint32
+	ist3l uint32
+	ist3h uint32
+	ist4l uint32
+	ist4h uint32
+	ist5l uint32
+	ist5h uint32
+	ist6l uint32
+	ist6h uint32
+	ist7l uint32
+	ist7h uint32
+
+	_res2 [2]uint32
+
+	_res3 uint16
+	iobmap uint16
+	_align uint64
+}
+
+const (
+	P 	uint32 = (1 << 15)
+	PS	uint32 = (P | (1 << 12))
+	G 	uint32 = (0 << 23)
+	D 	uint32 = (1 << 22)
+	L 	uint32 = (1 << 21)
+	CODE	uint32 = (0x0a << 8)
+	DATA	uint32 = (0x02 << 8)
+	TSS	uint32 = (0x09 << 8)
+	USER	uint32 = (0x60 << 8)
+)
+
+var _pgdt pdesc_t
+
+var _segs = [7 + 2*MAXCPUS]seg64_t{
+	// 0: null segment
+	{0, 0, 0},
+	// 1: 64 bit kernel code
+	{0, 0, PS | CODE | G | L},
+	// 2: 64 bit kernel data
+	{0, 0, PS | DATA | G | D},
+	// 3: FS segment
+	{0, 0, PS | DATA | G | D},
+	// 4: GS segment. the sysexit instruction also requires that the
+	// difference in indicies for the user code segment descriptor and the
+	// kernel code segment descriptor is 4.
+	{0, 0, PS | DATA | G | D},
+	// 5: 64 bit user code
+	{0, 0, PS | CODE | USER | G | L},
+	// 6: 64 bit user data
+	{0, 0, PS | DATA | USER | G | D},
+	// 7: 64 bit TSS segment (occupies two segment descriptor entries)
+	{0, 0, P | TSS | G},
+	{0, 0, 0},
+}
+
+var _tss [MAXCPUS]tss_t
+
+//go:nosplit
+func tss_set(id int, rsp, nmi uintptr) *tss_t {
+	sz := unsafe.Sizeof(_tss[id])
+	if sz != 104 + 8 {
+		panic("bad tss_t")
+	}
+	p := &_tss[id]
+	p.rsp0l = uint32(rsp)
+	p.rsp0h = uint32(rsp >> 32)
+
+	p.ist1l = uint32(rsp)
+	p.ist1h = uint32(rsp >> 32)
+
+	p.ist2l = uint32(nmi)
+	p.ist2h = uint32(nmi >> 32)
+
+	p.iobmap = uint16(sz)
+
+	up := unsafe.Pointer(p)
+	chkalign(up, 16)
+	return p
+}
+
+// maps cpu number to TSS segment descriptor in the GDT
+//go:nosplit
+func segnum(cpunum int) int {
+	return 7 + 2*cpunum
+}
+
+//go:nosplit
+func tss_seginit(cpunum int, _tssaddr *tss_t, lim uintptr) {
+	seg := &_segs[segnum(cpunum)]
+	seg.rest |= P | TSS | G
+
+	seg.lim = uint16(lim)
+	seg.rest |= uint32((lim >> 16) & 0xf) << 16
+
+	base := uintptr(unsafe.Pointer(_tssaddr))
+	seg.baselo = uint16(base)
+	seg.rest |= uint32(uint8(base >> 16))
+	seg.rest |= uint32(uint8(base >> 24) << 24)
+
+	seg = &_segs[segnum(cpunum) + 1]
+	seg.lim = uint16(base >> 32)
+	seg.baselo = uint16(base >> 48)
+}
+
+//go:nosplit
+func tss_init(cpunum int) uintptr {
+	intstk := uintptr(0xa100001000 + cpunum*4*PGSIZE)
+	nmistk := uintptr(0xa100003000 + cpunum*4*PGSIZE)
+	// BSP maps AP's stack for them
+	if cpunum == 0 {
+		alloc_map(intstk - 1, PTE_W, 1)
+		alloc_map(nmistk - 1, PTE_W, 1)
+	}
+	rsp := intstk
+	rspnmi := nmistk
+	tss := tss_set(cpunum, rsp, rspnmi)
+	tss_seginit(cpunum, tss, unsafe.Sizeof(tss_t{}) - 1)
+	segselect := segnum(cpunum) << 3
+	ltr(segselect)
+	cpus[lap_id()].rsp = rsp
+	return rsp
+}
+
+//go:nosplit
+func pdsetup(pd *pdesc_t, _addr unsafe.Pointer, lim uintptr) {
+	addr := uintptr(_addr)
+	pd.limit = uint16(lim)
+	for i := uint(0); i < 8; i++ {
+		pd.addr[i] = uint8((addr >> (i*8)) & 0xff)
+	}
+}
+
+//go:nosplit
+func dur(_p unsafe.Pointer, sz uintptr) {
+	for i := uintptr(0); i < sz; i++ {
+		p := (*uint8)(unsafe.Pointer(uintptr(_p) + i))
+		_pnum(uint(*p))
+	}
+}
+
+//go:nosplit
+func segsetup() *pdesc_t {
+	chksize(unsafe.Sizeof(_pgdt), 10)
+	chksize(unsafe.Sizeof(seg64_t{}), 8)
+	chkalign(unsafe.Pointer(&_pgdt), 8)
+	pdsetup(&_pgdt, unsafe.Pointer(&_segs[0]), unsafe.Sizeof(_segs) - 1)
+	return &_pgdt
+}
+
+//go:nosplit
+func fs0init(tls0 uintptr) {
+	// elf tls defines user tls at -16(%fs)
+	tlsaddr := int(tls0 + 16)
+	// we must set fs/gs, the only segment descriptors in ia32e mode, at
+	// least once before we use the MSRs to change their base address. the
+	// MSRs write directly to hidden segment descriptor cache, and if we
+	// don't explicitly fill the segment descriptor cache, the writes to
+	// the MSRs are thrown out (presumably because the caches are thought
+	// to be invalid).
+	fs_null()
+	ia32_fs_base := 0xc0000100
+	Wrmsr(ia32_fs_base, tlsaddr)
+}
+
+const (
+	PGSIZE	= 1 << 12
+	PTE_P	= 1 << 0
+	PTE_W	= 1 << 1
+	PTE_U	= 1 << 2
+	PTE_PCD	= 1 << 4
+)

commit 802c0853b6181d05707a9617d894d4c20364403b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 4 16:50:07 2016 -0500

    note

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fe188930a0..6168ee18a4 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -144,6 +144,9 @@ func Cprint(byte, int)
 func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
     pms []*[512]int, fastret bool) (int, int) {
 
+	// {enter,exit}syscall() may not be worth the overhead. i believe the
+	// only benefit for biscuit is that cpus running in the kernel could GC
+	// while other cpus execute user programs.
 	entersyscall()
 	fl := Pushcli()
 	ct := (*thread_t)(unsafe.Pointer(uintptr(Gscpu().mythread)))

commit 3b52f875747da9139455aff1a73b0f19e6ed712f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 4 16:45:19 2016 -0500

    start the probably long and terrible road to merging with 1.5
    
    will then merge with 1.6 if all goes well. 1.4 -> 1.5 will probably be more
    work than 1.5 -> 1.6 since i need to convert all my C code to go code.
    fortunately, conversion to go code has a few benefits.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3ee2f045a9..fe188930a0 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -54,7 +54,6 @@ func Crash()
 func Fnaddr(func()) uintptr
 func Fnaddri(func(int)) uintptr
 func Tfdump(*[24]int)
-func Usleep(int)
 func Rflags() int
 func Resetgcticks() uint64
 func Gcticks() uint64
@@ -554,3 +553,34 @@ func Trapsched() {
 func Trapinit() {
 	mcall(trapinit_m)
 }
+
+// TEMPORARY CRAP
+// G_ prefix means a function had to have both C and Go versions while the
+// conversion is underway. remove prefix afterwards.
+var Halt uint32
+
+func cli()
+func sti()
+func _pnum(uint)
+func _pmsg(*int8)
+
+//go:nosplit
+func G_pmsg(msg string) {
+	putch(' ');
+	for _, c := range msg {
+		putch(int8(c))
+	}
+}
+
+//go:nosplit
+func pancake(msg *int8, addr uint) {
+	cli()
+	atomicstore(&Halt, 1)
+	_pmsg(msg)
+	_pnum(addr)
+	G_pmsg("PANCAKE")
+	for {
+		p := (*uint16)(unsafe.Pointer(uintptr(0xb8002)))
+		*p = 0x1400 | 'F'
+	}
+}

commit ffb4f0660ba1f3eb6ad0922f5cb7da2ad8c82036
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Mar 2 12:15:05 2016 -0500

    un-derpify some of the first go code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 33a1eee87e..3ee2f045a9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -26,7 +26,6 @@ func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(uintptr)
-func Memmove(unsafe.Pointer, unsafe.Pointer, int)
 func Nanotime() int
 func Inb(int) int
 func Inl(int) int

commit 519474451a44b861e54466998a893a173bd54c4b
Author: Brad Fitzpatrick <bradfitz@golang.org>
Date:   Tue Mar 1 22:57:46 2016 +0000

    all: make copyright headers consistent with one space after period
    
    This is a subset of https://golang.org/cl/20022 with only the copyright
    header lines, so the next CL will be smaller and more reviewable.
    
    Go policy has been single space after periods in comments for some time.
    
    The copyright header template at:
    
        https://golang.org/doc/contribute.html#copyright
    
    also uses a single space.
    
    Make them all consistent.
    
    Change-Id: Icc26c6b8495c3820da6b171ca96a74701b4a01b0
    Reviewed-on: https://go-review.googlesource.com/20111
    Run-TryBot: Brad Fitzpatrick <bradfitz@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>
    Reviewed-by: Matthew Dempsky <mdempsky@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 51a7fa0a75..dd69743e10 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -1,4 +1,4 @@
-// Copyright 2014 The Go Authors.  All rights reserved.
+// Copyright 2014 The Go Authors. All rights reserved.
 // Use of this source code is governed by a BSD-style
 // license that can be found in the LICENSE file.
 

commit 20c620212cbff204c09ce38c3ddc60213e144ec2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 29 11:05:00 2016 -0500

    generalize/cleanup NMI profiling

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 5b752e4cd8..33a1eee87e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -73,6 +73,7 @@ type cpu_t struct {
 	num		uint
 	pmap		*[512]int
 	pms		[]*[512]int
+	//pid		uintptr
 }
 
 var Cpumhz uint
@@ -154,6 +155,7 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
 	cpu := Gscpu()
 	cpu.pmap = pmap
 	cpu.pms = pms
+	//cpu.pid = uintptr(pid)
 	if Rcr3() != p_pmap {
 		Lcr3(p_pmap)
 	}
@@ -245,10 +247,53 @@ func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
 	return uintptr(unsafe.Pointer(p))
 }
 
-var LVTPerfMask bool = true
+type nmiprof_t struct {
+	buf		[]uintptr
+	bufidx		uint64
+	LVTmask		bool
+	evtsel		int
+	evtmin		uint
+	evtmax		uint
+}
+
+var _nmibuf [4096]uintptr
+var nmiprof = nmiprof_t{buf: _nmibuf[:]}
+
+func SetNMI(mask bool, evtsel int, min, max uint) {
+	nmiprof.LVTmask = mask
+	nmiprof.evtsel = evtsel
+	nmiprof.evtmin = min
+	nmiprof.evtmax = max
+}
+
+func TakeNMIBuf() ([]uintptr, bool) {
+	ret := nmiprof.buf
+	l := int(nmiprof.bufidx)
+	if l > len(ret) {
+		l = len(ret)
+	}
+	ret = ret[:l]
+	full := false
+	if l == len(_nmibuf) {
+		full = true
+	}
+
+	nmiprof.bufidx = 0
+	return ret, full
+}
+
+var _seed uint
 
-var Byoof [4096]uintptr
-var Byidx uint64 = 0
+//go:nosplit
+func dumrand(low, high uint) uint {
+	ra := high - low
+	if ra == 0 {
+		return low
+	}
+	_seed = _seed * 1103515245 + 12345
+	ret := _seed & 0x7fffffffffffffff
+	return low + (ret % ra)
+}
 
 //go:nosplit
 func _consumelbr() {
@@ -260,7 +305,7 @@ func _consumelbr() {
 	// XXX stacklen
 	l := 16 * 2
 	l++
-	idx := int(xadd64(&Byidx, int64(l)))
+	idx := int(xadd64(&nmiprof.bufidx, int64(l)))
 	idx -= l
 	for i := 0; i < 16; i++ {
 		cur := (last - i)
@@ -271,24 +316,52 @@ func _consumelbr() {
 		to := uintptr(Rdmsr(lastbranch_0_to_ip + cur))
 		Wrmsr(lastbranch_0_from_ip + cur, 0)
 		Wrmsr(lastbranch_0_to_ip + cur, 0)
-		if idx + 2*i + 1 >= len(Byoof) {
+		if idx + 2*i + 1 >= len(nmiprof.buf) {
 			Cprint('!', 1)
 			break
 		}
-		Byoof[idx+2*i] = from
-		Byoof[idx+2*i+1] = to
+		nmiprof.buf[idx+2*i] = from
+		nmiprof.buf[idx+2*i+1] = to
 	}
 	idx += l - 1
-	if idx < len(Byoof) {
-		Byoof[idx] = ^uintptr(0)
+	if idx < len(nmiprof.buf) {
+		nmiprof.buf[idx] = ^uintptr(0)
 	}
 }
 
+//go:nosplit
+func _lbrreset(en bool) {
+	ia32_debugctl := 0x1d9
+	if !en {
+		Wrmsr(ia32_debugctl, 0)
+		return
+	}
+
+	// enable last branch records. filter every branch but to direct
+	// calls/jmps (sandybridge onward has better filtering)
+	lbr_select := 0x1c8
+	jcc := 1 << 2
+	indjmp := 1 << 6
+	//reljmp := 1 << 7
+	farbr := 1 << 8
+	dv := jcc | farbr | indjmp
+	Wrmsr(lbr_select, dv)
+
+	freeze_lbrs_on_pmi := 1 << 11
+	lbrs := 1 << 0
+	dv = lbrs | freeze_lbrs_on_pmi
+	Wrmsr(ia32_debugctl, dv)
+}
+
 //go:nosplit
 func perfgather(tf *[TFSIZE]uintptr) {
-	idx := xadd64(&Byidx, 1) - 1
-	if idx <= uint64(len(Byoof)) {
-		Byoof[idx] = tf[TF_RIP]
+	idx := xadd64(&nmiprof.bufidx, 1) - 1
+	if idx < uint64(len(nmiprof.buf)) {
+		//nmiprof.buf[idx] = tf[TF_RIP]
+		//pid := Gscpu().pid
+		//v := tf[TF_RIP] | (pid << 56)
+		v := tf[TF_RIP]
+		nmiprof.buf[idx] = v
 	}
 	//_consumelbr()
 }
@@ -300,7 +373,7 @@ func perfmask() {
 	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
 
 	perfmonc := 208
-	if LVTPerfMask {
+	if nmiprof.LVTmask {
 		mask := uint32(1 << 16)
 		lap[perfmonc] = mask
 		_pmcreset(false)
@@ -312,30 +385,6 @@ func perfmask() {
 	}
 }
 
-//go:nosplit
-func _lrbreset(en bool) {
-	ia32_debugctl := 0x1d9
-	if !en {
-		Wrmsr(ia32_debugctl, 0)
-		return
-	}
-
-	// enable last branch records. filter every branch but to direct
-	// calls/jmps (sandybridge onward has better filtering)
-	lbr_select := 0x1c8
-	jcc := 1 << 2
-	indjmp := 1 << 6
-	//reljmp := 1 << 7
-	farbr := 1 << 8
-	dv := jcc | farbr | indjmp
-	Wrmsr(lbr_select, dv)
-
-	freeze_lbrs_on_pmi := 1 << 11
-	lbrs := 1 << 0
-	dv = lbrs | freeze_lbrs_on_pmi
-	Wrmsr(ia32_debugctl, dv)
-}
-
 //go:nosplit
 func _pmcreset(en bool) {
 	ia32_pmc0 := 0xc1
@@ -345,10 +394,6 @@ func _pmcreset(en bool) {
 	ia32_global_ctrl := 0x38f
 	//ia32_perf_global_status := uint32(0x38e)
 
-	// "unhalted core cycles"
-	umask := 0
-	event := 0x3c
-
 	if en {
 		// disable perf counter before clearing
 		Wrmsr(ia32_perfevtsel0, 0)
@@ -356,13 +401,8 @@ func _pmcreset(en bool) {
 		// clear overflow
 		Wrmsr(ia32_perf_global_ovf_ctrl, 1)
 
-		usr := 1 << 16
-		os := 1 << 17
-		en := 1 << 22
-		inte := 1 << 20
-
-		ticks := int(Cpumhz * 10000)
-		Wrmsr(ia32_pmc0, -ticks)
+		r := dumrand(nmiprof.evtmin, nmiprof.evtmax)
+		Wrmsr(ia32_pmc0, -int(r))
 
 		freeze_pmc_on_pmi := 1 << 12
 		Wrmsr(ia32_debugctl, freeze_pmc_on_pmi)
@@ -370,8 +410,7 @@ func _pmcreset(en bool) {
 		// re-enable
 		Wrmsr(ia32_global_ctrl, 1)
 
-		umask = umask << 8
-		v := umask | event | usr | os | en | inte
+		v := nmiprof.evtsel
 		Wrmsr(ia32_perfevtsel0, v)
 	} else {
 		Wrmsr(ia32_perfevtsel0, 0)
@@ -379,7 +418,7 @@ func _pmcreset(en bool) {
 
 	// the write to debugctl enabling LBR must come after clearing overflow
 	// via global_ovf_ctrl; otherwise the processor instantly clears lbr...
-	//_lrbreset(en)
+	//_lbrreset(en)
 }
 
 //go:nosplit

commit 3c5f27f4d00edcb9515b7029905f46e079486ce0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Feb 26 17:16:00 2016 -0500

    fix more types

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 22688eec66..5b752e4cd8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -25,7 +25,7 @@ func Install_traphandler(func(tf *[24]int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
-func Lcr3(int)
+func Lcr3(uintptr)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
 func Nanotime() int
 func Inb(int) int
@@ -40,8 +40,9 @@ func Pnum(int)
 func Kreset()
 func Ktime() int
 func Rdtsc() uint64
-func Rcr2() int
-func Rcr3() int
+func Rcr2() uintptr
+func Rcr3() uintptr
+func Rcr4() uintptr
 func Rrsp() int
 func Sgdt(*uintptr)
 func Sidt(*uintptr)
@@ -141,7 +142,7 @@ func _Userrun(*[24]int, bool) (int, int)
 
 func Cprint(byte, int)
 
-func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
+func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap uintptr,
     pms []*[512]int, fastret bool) (int, int) {
 
 	entersyscall()

commit fbc0a61061615ef4ff8df6fa3aec24efb77aca32
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 25 09:55:01 2016 -0500

    don't count performance events during NMI handler

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 12b9833687..22688eec66 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -340,6 +340,8 @@ func _pmcreset(en bool) {
 	ia32_pmc0 := 0xc1
 	ia32_perfevtsel0 := 0x186
 	ia32_perf_global_ovf_ctrl := 0x390
+	ia32_debugctl := 0x1d9
+	ia32_global_ctrl := 0x38f
 	//ia32_perf_global_status := uint32(0x38e)
 
 	// "unhalted core cycles"
@@ -361,6 +363,12 @@ func _pmcreset(en bool) {
 		ticks := int(Cpumhz * 10000)
 		Wrmsr(ia32_pmc0, -ticks)
 
+		freeze_pmc_on_pmi := 1 << 12
+		Wrmsr(ia32_debugctl, freeze_pmc_on_pmi)
+		// cpu clears global_ctrl on PMI if freeze-on-pmi is set.
+		// re-enable
+		Wrmsr(ia32_global_ctrl, 1)
+
 		umask = umask << 8
 		v := umask | event | usr | os | en | inte
 		Wrmsr(ia32_perfevtsel0, v)

commit e4b26400804dcdde503520bea560c8bd467d63bc
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 23 14:33:30 2016 -0500

    whoops; don't forget to shift mask

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6bfab35663..12b9833687 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -361,6 +361,7 @@ func _pmcreset(en bool) {
 		ticks := int(Cpumhz * 10000)
 		Wrmsr(ia32_pmc0, -ticks)
 
+		umask = umask << 8
 		v := umask | event | usr | os | en | inte
 		Wrmsr(ia32_perfevtsel0, v)
 	} else {

commit 818a1d8121dcaaf94c7f199bac4af3c1ab9c6be4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Feb 19 15:59:10 2016 -0500

    move gettimeofday(2) to userspace, similar to linux's vDSO
    
    gives redis +30%. linux is only 2x faster now!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 86d54c9c16..6bfab35663 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -130,6 +130,8 @@ const(
     TF_FL_IF     = 1 << 9
 )
 
+var Pspercycle uint
+
 func Pushcli() int
 func Popcli(int)
 func Gscpu() *cpu_t

commit 117a717ce6b4d4ca2138eceaddaa4f797f4ee640
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 16 14:35:20 2016 -0500

    note to prevent baffled cody later

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 95ed205a3e..86d54c9c16 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -161,6 +161,9 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
 	kfsbase := Rdmsr(ia32_fs_base)
 	Wrmsr(ia32_fs_base, tf[TF_FSBASE])
 
+	// we only save/restore SSE registers on cpu exception/interrupt, not
+	// during syscall exit/return. this is OK since sys5ABI defines the SSE
+	// registers to be caller-saved.
 	ct.user.tf = uintptr(unsafe.Pointer(tf))
 	ct.user.fxbuf = uintptr(unsafe.Pointer(fxbuf))
 	intno, aux := _Userrun(tf, fastret)

commit 52f1a8f45c66b3d4feb75984ba8b1a9db45ef565
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Feb 12 16:43:51 2016 -0500

    poor man's NMI backtracing via Intel branch profiling
    
    if only my cpu was one generation newer, the backtrace would be way more useful

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 57a7695805..95ed205a3e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -242,14 +242,49 @@ func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
 var LVTPerfMask bool = true
 
 var Byoof [4096]uintptr
-var Byidx uint64 = ^uint64(0)
+var Byidx uint64 = 0
+
+//go:nosplit
+func _consumelbr() {
+	lastbranch_tos := 0x1c9
+	lastbranch_0_from_ip := 0x680
+	lastbranch_0_to_ip := 0x6c0
+
+	last := Rdmsr(lastbranch_tos) & 0xf
+	// XXX stacklen
+	l := 16 * 2
+	l++
+	idx := int(xadd64(&Byidx, int64(l)))
+	idx -= l
+	for i := 0; i < 16; i++ {
+		cur := (last - i)
+		if cur < 0 {
+			cur += 16
+		}
+		from := uintptr(Rdmsr(lastbranch_0_from_ip + cur))
+		to := uintptr(Rdmsr(lastbranch_0_to_ip + cur))
+		Wrmsr(lastbranch_0_from_ip + cur, 0)
+		Wrmsr(lastbranch_0_to_ip + cur, 0)
+		if idx + 2*i + 1 >= len(Byoof) {
+			Cprint('!', 1)
+			break
+		}
+		Byoof[idx+2*i] = from
+		Byoof[idx+2*i+1] = to
+	}
+	idx += l - 1
+	if idx < len(Byoof) {
+		Byoof[idx] = ^uintptr(0)
+	}
+}
 
 //go:nosplit
 func perfgather(tf *[TFSIZE]uintptr) {
-	idx := xadd64(&Byidx, 1)
+	idx := xadd64(&Byidx, 1) - 1
 	if idx <= uint64(len(Byoof)) {
 		Byoof[idx] = tf[TF_RIP]
 	}
+	//_consumelbr()
 }
 
 //go:nosplit
@@ -259,28 +294,52 @@ func perfmask() {
 	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
 
 	perfmonc := 208
-	mask := uint32(1 << 16)
-	nmidelmode :=  uint32(0x4 << 8)
 	if LVTPerfMask {
+		mask := uint32(1 << 16)
 		lap[perfmonc] = mask
-		_perfcnt(false)
+		_pmcreset(false)
 	} else {
 		// unmask perf LVT, reset pmc
+		nmidelmode :=  uint32(0x4 << 8)
 		lap[perfmonc] = nmidelmode
-		_perfcnt(true)
+		_pmcreset(true)
 	}
 }
 
 //go:nosplit
-func _perfcnt(en bool) {
+func _lrbreset(en bool) {
+	ia32_debugctl := 0x1d9
+	if !en {
+		Wrmsr(ia32_debugctl, 0)
+		return
+	}
+
+	// enable last branch records. filter every branch but to direct
+	// calls/jmps (sandybridge onward has better filtering)
+	lbr_select := 0x1c8
+	jcc := 1 << 2
+	indjmp := 1 << 6
+	//reljmp := 1 << 7
+	farbr := 1 << 8
+	dv := jcc | farbr | indjmp
+	Wrmsr(lbr_select, dv)
+
+	freeze_lbrs_on_pmi := 1 << 11
+	lbrs := 1 << 0
+	dv = lbrs | freeze_lbrs_on_pmi
+	Wrmsr(ia32_debugctl, dv)
+}
+
+//go:nosplit
+func _pmcreset(en bool) {
 	ia32_pmc0 := 0xc1
 	ia32_perfevtsel0 := 0x186
 	ia32_perf_global_ovf_ctrl := 0x390
 	//ia32_perf_global_status := uint32(0x38e)
 
 	// "unhalted core cycles"
-	umask := uint8(0)
-	event := uint8(0x3c)
+	umask := 0
+	event := 0x3c
 
 	if en {
 		// disable perf counter before clearing
@@ -289,19 +348,23 @@ func _perfcnt(en bool) {
 		// clear overflow
 		Wrmsr(ia32_perf_global_ovf_ctrl, 1)
 
-		usr := uint(1 << 16)
-		os := uint(1 << 17)
-		en := uint(1 << 22)
-		inte := uint(1 << 20)
+		usr := 1 << 16
+		os := 1 << 17
+		en := 1 << 22
+		inte := 1 << 20
 
 		ticks := int(Cpumhz * 10000)
 		Wrmsr(ia32_pmc0, -ticks)
 
-		v := uint(umask) | uint(event) | usr | os | en | inte
-		Wrmsr(ia32_perfevtsel0, int(v))
+		v := umask | event | usr | os | en | inte
+		Wrmsr(ia32_perfevtsel0, v)
 	} else {
 		Wrmsr(ia32_perfevtsel0, 0)
 	}
+
+	// the write to debugctl enabling LBR must come after clearing overflow
+	// via global_ovf_ctrl; otherwise the processor instantly clears lbr...
+	//_lrbreset(en)
 }
 
 //go:nosplit

commit 571984098ed696fe6d8e34627ba6b6bae164b3c7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 11 14:33:11 2016 -0500

    profiling cleanup

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 28b25716ef..57a7695805 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -245,7 +245,15 @@ var Byoof [4096]uintptr
 var Byidx uint64 = ^uint64(0)
 
 //go:nosplit
-func perfmask(tf *[TFSIZE]uintptr) {
+func perfgather(tf *[TFSIZE]uintptr) {
+	idx := xadd64(&Byidx, 1)
+	if idx <= uint64(len(Byoof)) {
+		Byoof[idx] = tf[TF_RIP]
+	}
+}
+
+//go:nosplit
+func perfmask() {
 	lapaddr := 0xfee00000
 	const PGSIZE = 1 << 12
 	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
@@ -257,14 +265,6 @@ func perfmask(tf *[TFSIZE]uintptr) {
 		lap[perfmonc] = mask
 		_perfcnt(false)
 	} else {
-		// collect infos
-		if tf != nil {
-			idx := xadd64(&Byidx, 1)
-			if idx <= uint64(len(Byoof)) {
-				Byoof[idx] = tf[TF_RIP]
-			}
-		}
-
 		// unmask perf LVT, reset pmc
 		lap[perfmonc] = nmidelmode
 		_perfcnt(true)

commit 58b3eb92190abc687f8f6338a61f7187408f7155
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 10 15:17:06 2016 -0500

    barebones RIP sampling

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 20b45964f2..28b25716ef 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -241,8 +241,11 @@ func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
 
 var LVTPerfMask bool = true
 
+var Byoof [4096]uintptr
+var Byidx uint64 = ^uint64(0)
+
 //go:nosplit
-func perfmask() {
+func perfmask(tf *[TFSIZE]uintptr) {
 	lapaddr := 0xfee00000
 	const PGSIZE = 1 << 12
 	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
@@ -254,6 +257,15 @@ func perfmask() {
 		lap[perfmonc] = mask
 		_perfcnt(false)
 	} else {
+		// collect infos
+		if tf != nil {
+			idx := xadd64(&Byidx, 1)
+			if idx <= uint64(len(Byoof)) {
+				Byoof[idx] = tf[TF_RIP]
+			}
+		}
+
+		// unmask perf LVT, reset pmc
 		lap[perfmonc] = nmidelmode
 		_perfcnt(true)
 	}

commit bb5a1721fff47c5a8e5614c72e91775653da9530
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 10 11:46:20 2016 -0500

    more robust kernel fault printer
    
    so that faults taken in weird scenarios, like when the print lock has been
    taken, are more likely to print to console.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 82c0b6a42f..20b45964f2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -54,7 +54,6 @@ func Crash()
 func Fnaddr(func()) uintptr
 func Fnaddri(func(int)) uintptr
 func Tfdump(*[24]int)
-func Stackdump(int)
 func Usleep(int)
 func Rflags() int
 func Resetgcticks() uint64

commit 96fa737ec3f75661d98cb3a2b00eb8cf6f4cdeed
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 10 09:33:43 2016 -0500

    performance profiling skeleton via NMI and performance counters
    
    and an IPI to enable/disable the counters on all available cores.
    
    should have done this a long time ago; preliminary profiles look promising!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1b255ca81d..82c0b6a42f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -75,6 +75,8 @@ type cpu_t struct {
 	pms		[]*[512]int
 }
 
+var Cpumhz uint
+
 var cpus [32]cpu_t
 
 type tuser_t struct {
@@ -238,6 +240,59 @@ func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
 	return uintptr(unsafe.Pointer(p))
 }
 
+var LVTPerfMask bool = true
+
+//go:nosplit
+func perfmask() {
+	lapaddr := 0xfee00000
+	const PGSIZE = 1 << 12
+	lap := (*[PGSIZE/4]uint32)(unsafe.Pointer(uintptr(lapaddr)))
+
+	perfmonc := 208
+	mask := uint32(1 << 16)
+	nmidelmode :=  uint32(0x4 << 8)
+	if LVTPerfMask {
+		lap[perfmonc] = mask
+		_perfcnt(false)
+	} else {
+		lap[perfmonc] = nmidelmode
+		_perfcnt(true)
+	}
+}
+
+//go:nosplit
+func _perfcnt(en bool) {
+	ia32_pmc0 := 0xc1
+	ia32_perfevtsel0 := 0x186
+	ia32_perf_global_ovf_ctrl := 0x390
+	//ia32_perf_global_status := uint32(0x38e)
+
+	// "unhalted core cycles"
+	umask := uint8(0)
+	event := uint8(0x3c)
+
+	if en {
+		// disable perf counter before clearing
+		Wrmsr(ia32_perfevtsel0, 0)
+
+		// clear overflow
+		Wrmsr(ia32_perf_global_ovf_ctrl, 1)
+
+		usr := uint(1 << 16)
+		os := uint(1 << 17)
+		en := uint(1 << 22)
+		inte := uint(1 << 20)
+
+		ticks := int(Cpumhz * 10000)
+		Wrmsr(ia32_pmc0, -ticks)
+
+		v := uint(umask) | uint(event) | usr | os | en | inte
+		Wrmsr(ia32_perfevtsel0, int(v))
+	} else {
+		Wrmsr(ia32_perfevtsel0, 0)
+	}
+}
+
 //go:nosplit
 func sc_setup() {
 	// disable interrupts

commit 709aad3a882250082360b61c38475e2b646228d9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 9 14:37:24 2016 -0500

    allocate/map AP's NMI stack

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index cd19bbce95..1b255ca81d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -217,7 +217,7 @@ type tss_t struct {
 var alltss [32]tss_t
 
 //go:nosplit
-func tss_set(id uintptr, rsp uintptr, rsz *uintptr) uintptr {
+func tss_set(id, rsp, nmi uintptr, rsz *uintptr) uintptr {
 	sz := unsafe.Sizeof(alltss[id])
 	if sz != 104 + 8 {
 		panic("bad tss_t")
@@ -228,7 +228,12 @@ func tss_set(id uintptr, rsp uintptr, rsz *uintptr) uintptr {
 
 	p.ist1l = uint32(rsp)
 	p.ist1h = uint32(rsp >> 32)
+
+	p.ist2l = uint32(nmi)
+	p.ist2h = uint32(nmi >> 32)
+
 	p.iobmap = uint16(sz)
+
 	*rsz = sz
 	return uintptr(unsafe.Pointer(p))
 }

commit cd37c7a96d6fd5d1628af4ccaa8fb9382227d7f3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 9 12:46:52 2016 -0500

    unstupidefy and convert TSS64 code C code to Go
    
    in preparation for NMI interrupts

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index aa250235be..cd19bbce95 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -180,6 +180,59 @@ func shadow_clear() {
 	cpu.pms = nil
 }
 
+type tss_t struct {
+	_res0 uint32
+
+	rsp0l uint32
+	rsp0h uint32
+	rsp1l uint32
+	rsp1h uint32
+	rsp2l uint32
+	rsp2h uint32
+
+	_res1 [2]uint32
+
+	ist1l uint32
+	ist1h uint32
+	ist2l uint32
+	ist2h uint32
+	ist3l uint32
+	ist3h uint32
+	ist4l uint32
+	ist4h uint32
+	ist5l uint32
+	ist5h uint32
+	ist6l uint32
+	ist6h uint32
+	ist7l uint32
+	ist7h uint32
+
+	_res2 [2]uint32
+
+	_res3 uint16
+	iobmap uint16
+	_align uint64
+}
+
+var alltss [32]tss_t
+
+//go:nosplit
+func tss_set(id uintptr, rsp uintptr, rsz *uintptr) uintptr {
+	sz := unsafe.Sizeof(alltss[id])
+	if sz != 104 + 8 {
+		panic("bad tss_t")
+	}
+	p := &alltss[id]
+	p.rsp0l = uint32(rsp)
+	p.rsp0h = uint32(rsp >> 32)
+
+	p.ist1l = uint32(rsp)
+	p.ist1h = uint32(rsp >> 32)
+	p.iobmap = uint16(sz)
+	*rsz = sz
+	return uintptr(unsafe.Pointer(p))
+}
+
 //go:nosplit
 func sc_setup() {
 	// disable interrupts

commit 936269fb17bc7a205b7b177d751ab6a5c441b844
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 9 11:53:48 2016 -0500

    start fixing some types

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 293bdf76d4..aa250235be 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -43,16 +43,16 @@ func Rdtsc() uint64
 func Rcr2() int
 func Rcr3() int
 func Rrsp() int
-func Sgdt(*int)
-func Sidt(*int)
+func Sgdt(*uintptr)
+func Sidt(*uintptr)
 func Sti()
 func Tlbadmit(int, int, int, int) uint
 func Tlbwait(uint)
 func Vtop(*[512]int) int
 
 func Crash()
-func Fnaddr(func()) int
-func Fnaddri(func(int)) int
+func Fnaddr(func()) uintptr
+func Fnaddri(func(int)) uintptr
 func Tfdump(*[24]int)
 func Stackdump(int)
 func Usleep(int)

commit dadfaf660e7674d8c42d5ceb9c5fb62feb56e2aa
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 3 12:55:20 2016 -0500

    use a baud rate that is not from antiquity
    
    this saves me probably 30 seconds of waiting for a performance profile to dump.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index bdb74c18e6..293bdf76d4 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -189,7 +189,7 @@ func sc_setup() {
 	Outb(com1 + 3, 0x80)
 
 	// set both bytes for divisor baud rate
-	Outb(com1 + 0, 115200/9600)
+	Outb(com1 + 0, 115200/115200)
 	Outb(com1 + 1, 0)
 
 	// 8 bit words, one stop bit, no parity

commit 1a1aaa718bca2205e3407cc40802dfc1519fb82c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Nov 3 14:01:51 2015 -0500

    document uart config

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6ffa9ff784..bdb74c18e6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -182,30 +182,21 @@ func shadow_clear() {
 
 //go:nosplit
 func sc_setup() {
-	// XXX figure out and document this shit!
-	//com1  := 0x3f8
-	//data  := 0
-	//intr  := 1
-	//ififo := 2
-	//lctl  := 3
-	//mctl  := 4
-
-	//Outb(com1 + ififo, 0x0)
-	//Outb(com1 + lctl, 0x80)
-	//Outb(com1 + data, 115200/9600)
-	//Outb(com1 + intr, 0x0)
-	//Outb(com1 + lctl, 0x03)
-	//Outb(com1 + mctl, 0x0)
-	//Outb(com1 + intr, 0x01)
-
-	//inb(com1 + ififo)
-	//inb(com1 + data)
-
+	// disable interrupts
 	Outb(com1 + 1, 0)
+
+	// set divisor latch bit to set divisor bytes
 	Outb(com1 + 3, 0x80)
+
+	// set both bytes for divisor baud rate
 	Outb(com1 + 0, 115200/9600)
 	Outb(com1 + 1, 0)
+
+	// 8 bit words, one stop bit, no parity
 	Outb(com1 + 3, 0x03)
+
+	// configure FIFO for interrupts: maximum FIFO size, clear
+	// transmit/receive FIFOs, and enable FIFOs.
 	Outb(com1 + 2, 0xc7)
 	Outb(com1 + 4, 0x0b)
 	Outb(com1 + 1, 1)

commit a900f8371a90ee27205fe4daf0f5b7f4ae6dd854
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 30 13:32:59 2015 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6fdda44d17..6ffa9ff784 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -203,7 +203,7 @@ func sc_setup() {
 
 	Outb(com1 + 1, 0)
 	Outb(com1 + 3, 0x80)
-	Outb(com1 + 0, 12)
+	Outb(com1 + 0, 115200/9600)
 	Outb(com1 + 1, 0)
 	Outb(com1 + 3, 0x03)
 	Outb(com1 + 2, 0xc7)

commit 054253568d1dbde6e9c379ab3f228ec2baade3c7
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 30 11:58:22 2015 -0400

    get com0 interrupts working on hardware too
    
    now i can finally do performance experiments on my test hardware remotely!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 5cfbabf286..6fdda44d17 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -182,23 +182,33 @@ func shadow_clear() {
 
 //go:nosplit
 func sc_setup() {
-	com1  := 0x3f8
-	data  := 0
-	intr  := 1
-	ififo := 2
-	lctl  := 3
-	mctl  := 4
-
-	Outb(com1 + ififo, 0x0)
-	Outb(com1 + lctl, 0x80)
-	Outb(com1 + data, 115200/9600)
-	Outb(com1 + intr, 0x0)
-	Outb(com1 + lctl, 0x03)
-	Outb(com1 + mctl, 0x0)
-	Outb(com1 + intr, 0x01)
-
-	inb(com1 + ififo)
-	inb(com1 + data)
+	// XXX figure out and document this shit!
+	//com1  := 0x3f8
+	//data  := 0
+	//intr  := 1
+	//ififo := 2
+	//lctl  := 3
+	//mctl  := 4
+
+	//Outb(com1 + ififo, 0x0)
+	//Outb(com1 + lctl, 0x80)
+	//Outb(com1 + data, 115200/9600)
+	//Outb(com1 + intr, 0x0)
+	//Outb(com1 + lctl, 0x03)
+	//Outb(com1 + mctl, 0x0)
+	//Outb(com1 + intr, 0x01)
+
+	//inb(com1 + ififo)
+	//inb(com1 + data)
+
+	Outb(com1 + 1, 0)
+	Outb(com1 + 3, 0x80)
+	Outb(com1 + 0, 12)
+	Outb(com1 + 1, 0)
+	Outb(com1 + 3, 0x03)
+	Outb(com1 + 2, 0xc7)
+	Outb(com1 + 4, 0x0b)
+	Outb(com1 + 1, 1)
 }
 
 const com1 = 0x3f8

commit e387f1e3cdbe53aab07e8fc150153ec62d729ac3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Oct 27 12:01:19 2015 -0400

    bugfix: prevent deadlock with TLB shootdown+GC
    
    the old TLB shootdown code would wait for all shootdowns to complete by cas'ing
    the variable holding the number of outstanding shootdowns with 0. it was
    possible that, before the CPU waiting for all shootdowns to complete could
    observe the 0 count, another CPU would install a new TLB count.
    
    i thought this was fine since the waiting CPU must observe 0 at some point.
    turns out it may not if the CPU that installs the new count is preempted by a
    GC before it actually sends the IPIs. then the GC loops forever, trying to
    preempt the waiting CPU (which is not preemptable as it is looping for the
    count to reach 0).
    
    this new code ensures that the waiting CPU figures out that its shootdown is
    complete, even if some other CPU installs a new count before the waiting CPU
    observes a 0 count.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f8152efad4..5cfbabf286 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -46,7 +46,8 @@ func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
 func Sti()
-func Tlbadmit(int, int, int, int)
+func Tlbadmit(int, int, int, int) uint
+func Tlbwait(uint)
 func Vtop(*[512]int) int
 
 func Crash()

commit 24af809c65c819399fbb23b963b159c1b3e2083a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Oct 27 12:00:16 2015 -0400

    Cprint(), a useful debugging function

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 47df190f40..f8152efad4 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -135,6 +135,8 @@ func Rdmsr(int) int
 func Wrmsr(int, int)
 func _Userrun(*[24]int, bool) (int, int)
 
+func Cprint(byte, int)
+
 func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
     pms []*[512]int, fastret bool) (int, int) {
 

commit 58e3ae2fae7dcb22a67ba802c250ba0abf894b1d
Author: Matthew Dempsky <mdempsky@google.com>
Date:   Wed Oct 21 12:48:53 2015 -0700

    runtime: split plan9 and solaris's m fields into new embedded mOS type
    
    Reduces the size of m by ~8% on linux/amd64 (1040 bytes -> 960 bytes).
    
    There are also windows-specific fields, but they're currently
    referenced in OS-independent source files (but only when
    GOOS=="windows").
    
    Change-Id: I13e1471ff585ccced1271f74209f8ed6df14c202
    Reviewed-on: https://go-review.googlesource.com/16173
    Run-TryBot: Matthew Dempsky <mdempsky@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index dc932dbaa0..51a7fa0a75 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -6,6 +6,8 @@ package runtime
 
 import "unsafe"
 
+type mOS struct{}
+
 //go:noescape
 func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer, val3 uint32) int32
 

commit 7079490f36e84e7f5e53fe5457b4b36ee837c473
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Oct 2 10:22:55 2015 -0400

    cleanup pmap shadow pointers -- convert more C code to Go code
    
    move the definition the array of per-cpu structs from C to Go so it is all
    GC'ed.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1be93be2f2..47df190f40 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -66,12 +66,16 @@ var gcticks uint64
 var No_pml4 int
 
 type cpu_t struct {
-	this		int
-	mythread	int
-	rsp		int
-	num		int
+	this		uint
+	mythread	uint
+	rsp		uint
+	num		uint
+	pmap		*[512]int
+	pms		[]*[512]int
 }
 
+var cpus [32]cpu_t
+
 type tuser_t struct {
 	tf	uintptr
 	fxbuf	uintptr
@@ -131,18 +135,6 @@ func Rdmsr(int) int
 func Wrmsr(int, int)
 func _Userrun(*[24]int, bool) (int, int)
 
-// XXX should use cpu_t
-type gcpu_t struct {
-	pmap	*[512]int
-	pms	[]*[512]int
-}
-
-var gcpus []gcpu_t
-
-func Userinit(numcpu int) {
-	gcpus = make([]gcpu_t, numcpu)
-}
-
 func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
     pms []*[512]int, fastret bool) (int, int) {
 
@@ -152,9 +144,9 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
 
 	// set shadow pointers for user pmap so it isn't free'd out from under
 	// us if the process terminates soon
-	ci := Gscpu().num
-	gcpus[ci].pmap = pmap
-	gcpus[ci].pms = pms
+	cpu := Gscpu()
+	cpu.pmap = pmap
+	cpu.pms = pms
 	if Rcr3() != p_pmap {
 		Lcr3(p_pmap)
 	}
@@ -180,13 +172,9 @@ func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
 // caller must have interrupts cleared
 //go:nosplit
 func shadow_clear() {
-	// do nothing if gscpus is not init'ed yet
-	if len(gcpus) == 0 {
-		return
-	}
-	ci := Gscpu().num
-	gcpus[ci].pmap = nil
-	gcpus[ci].pms = nil
+	cpu := Gscpu()
+	cpu.pmap = nil
+	cpu.pms = nil
 }
 
 //go:nosplit

commit 3a3cef58aba170adf42e824bf2029d88331c0c8e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Oct 1 20:53:58 2015 -0400

    better idea: use this fancy thing called a "garbage collector"
    
    before returning to a user program, set shadow pointers to the program's pmap.
    all interrupts currently load the kernel pmap, thus it is safe to clear the
    current CPU's shadow pointers in the interrupt handler after loading the kernel
    pmap.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fb43df451a..1be93be2f2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -131,15 +131,30 @@ func Rdmsr(int) int
 func Wrmsr(int, int)
 func _Userrun(*[24]int, bool) (int, int)
 
-func Userrun(tf *[24]int, fxbuf *[64]int, p_pmap int,
-    fastret bool) (int, int) {
+// XXX should use cpu_t
+type gcpu_t struct {
+	pmap	*[512]int
+	pms	[]*[512]int
+}
+
+var gcpus []gcpu_t
+
+func Userinit(numcpu int) {
+	gcpus = make([]gcpu_t, numcpu)
+}
+
+func Userrun(tf *[24]int, fxbuf *[64]int, pmap *[512]int, p_pmap int,
+    pms []*[512]int, fastret bool) (int, int) {
+
 	entersyscall()
 	fl := Pushcli()
 	ct := (*thread_t)(unsafe.Pointer(uintptr(Gscpu().mythread)))
-	// don't need to set the threads pmap to the user prog's pmap since any
-	// interrupt taken while in user code forces a return to kernel mode
-	// through Userrun
 
+	// set shadow pointers for user pmap so it isn't free'd out from under
+	// us if the process terminates soon
+	ci := Gscpu().num
+	gcpus[ci].pmap = pmap
+	gcpus[ci].pms = pms
 	if Rcr3() != p_pmap {
 		Lcr3(p_pmap)
 	}
@@ -162,6 +177,18 @@ func Userrun(tf *[24]int, fxbuf *[64]int, p_pmap int,
 	return intno, aux
 }
 
+// caller must have interrupts cleared
+//go:nosplit
+func shadow_clear() {
+	// do nothing if gscpus is not init'ed yet
+	if len(gcpus) == 0 {
+		return
+	}
+	ci := Gscpu().num
+	gcpus[ci].pmap = nil
+	gcpus[ci].pms = nil
+}
+
 //go:nosplit
 func sc_setup() {
 	com1  := 0x3f8

commit 9fb79380f069348b317865aafa4023d3013137cb
Author: Mikio Hara <mikioh.mikioh@gmail.com>
Date:   Sun Jul 12 09:23:37 2015 +0900

    runtime: drop sigfwd from signal forwarding unsupported platforms
    
    This change splits signal_unix.go into signal_unix.go and
    signal2_unix.go and removes the fake symbol sigfwd from signal
    forwarding unsupported platforms for clarification purpose.
    
    Change-Id: I205eab5cf1930fda8a68659b35cfa9f3a0e67ca6
    Reviewed-on: https://go-review.googlesource.com/12062
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index bd492f5e3b..dc932dbaa0 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,9 +18,6 @@ func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
 //go:noescape
 func sigaltstack(new, old *sigaltstackt)
 
-//go:noescape
-func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)
-
 //go:noescape
 func setitimer(mode int32, new, old *itimerval)
 

commit e8a09a0d95cb53bdeb4c34d6a51167eb46d57e9c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Oct 1 20:28:32 2015 -0400

    Revert "page map free'ing daemon"
    
    This reverts commit 28f7b69fd71acdbabe5b664f6930a52997a1bae2.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fb792f5134..fb43df451a 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -66,10 +66,10 @@ var gcticks uint64
 var No_pml4 int
 
 type cpu_t struct {
-	this		uint
-	mythread	uint
-	rsp		uint
-	num		uint
+	this		int
+	mythread	int
+	rsp		int
+	num		int
 }
 
 type tuser_t struct {

commit 28f7b69fd71acdbabe5b664f6930a52997a1bae2
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Oct 1 20:05:44 2015 -0400

    page map free'ing daemon
    
    in order to do system calls really fast, we cannot switch page maps on
    user->kernel or kernel->user transitions (on the test hardware, always
    switching page maps is 2x slower on a getpid benchmark!).
    
    but now it is tricky to know when it is safe to free a page map since any CPU
    in kernel mode may have the page map in question loaded at any time.
    
    thus, have a daemon that collects page maps once all threads of a user process
    have terminated. then, once a second, broadcast an IPI that causes all kernel
    threads to load the kernel page map, ensuring that no CPU can have the user
    page map loaded. finally, free all collected page maps. repeat.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fb43df451a..fb792f5134 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -66,10 +66,10 @@ var gcticks uint64
 var No_pml4 int
 
 type cpu_t struct {
-	this		int
-	mythread	int
-	rsp		int
-	num		int
+	this		uint
+	mythread	uint
+	rsp		uint
+	num		uint
 }
 
 type tuser_t struct {

commit 7b14bae3b1ede63d14cd972c27b40a2816838078
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Oct 1 11:47:43 2015 -0400

    delete code that is now unused, fix process termination
    
    the C scheduler no longer cares about pids or notifying. still need to fix
    threads, pmap freeing, and kernel profiling.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e9ed442fc0..fb43df451a 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -21,8 +21,7 @@ func trapinit_m(*g)
 func Ap_setup(int)
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
-type Ptid_t uint
-func Install_traphandler(func(tf *[24]int, ptid Ptid_t, notify int))
+func Install_traphandler(func(tf *[24]int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
@@ -38,15 +37,8 @@ func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
 func Pmsga(*uint8, int, int8)
 func Pnum(int)
-func Procadd(ptid Ptid_t, tf *[24]int, p_pmap int)
-func Proccontinue()
-func Prockill(Ptid_t)
-func Procnotify(Ptid_t) int
-func Procrunnable(ptid Ptid_t, tf *[24]int, p_pmap int)
-func Proctime(ptid Ptid_t) int
 func Kreset()
 func Ktime() int
-func Procyield()
 func Rdtsc() uint64
 func Rcr2() int
 func Rcr3() int
@@ -106,9 +98,8 @@ type thread_t struct {
 	sleepfor	int
 	sleepret	int
 	futaddr		int
-	pid		int
 	pmap		int
-	notify		int
+	//_pad		int
 }
 
 const(
@@ -138,7 +129,7 @@ func Popcli(int)
 func Gscpu() *cpu_t
 func Rdmsr(int) int
 func Wrmsr(int, int)
-func Userrun_(*[24]int, bool) (int, int)
+func _Userrun(*[24]int, bool) (int, int)
 
 func Userrun(tf *[24]int, fxbuf *[64]int, p_pmap int,
     fastret bool) (int, int) {
@@ -161,7 +152,7 @@ func Userrun(tf *[24]int, fxbuf *[64]int, p_pmap int,
 
 	ct.user.tf = uintptr(unsafe.Pointer(tf))
 	ct.user.fxbuf = uintptr(unsafe.Pointer(fxbuf))
-	intno, aux := Userrun_(tf, fastret)
+	intno, aux := _Userrun(tf, fastret)
 
 	Wrmsr(ia32_fs_base, kfsbase)
 	ct.user.tf = 0

commit 3ad9816a4631fe8ffdb70106969aeaff89718350
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Sep 25 11:37:17 2015 -0400

    Userrun() for fast syscalls and better interface to user programs
    
    system call overhead is reduced 10x using this approach, putting Biscuit on par
    with Linux. i think the main reasons for this speedup are that, during
    syscalls, there is little state saved when switching to/from kernel mode and we
    now use a single go routine to handle all of a thread's CPU exceptions/syscalls
    instead of creating a new one for each event.
    
    Userrun() also makes the code for handling all user events (page faults, timer
    interrupts) clearer and exposes timer interrupts during user program execution
    to the kernel, allowing for finer grained control and time accounting.
    
    i still need to fix thread termination: since user programs are no longer
    managed via global state, it should be easier but i haven't done it yet.
    
    also, start converting C code to Go code. its painful because we have to have
    two defintions of every struct that is used by both kinds of code -- it is up
    to the programmer to make sure they are coherent. maybe this will help motivate
    me to convert more C to Go.
    
    cleanup trap() and trap circular buffer code since only IRQs/IPIs go through
    the circular buffer now. sadly, we can't handle IRQs using the new method
    because IRQs/IPIs are truly asynchronous, thus we need a way to inject them
    into the runtime regardless of whether the CPU is currently in user or kernel
    mode.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 768a4d7a8d..e9ed442fc0 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -22,7 +22,7 @@ func Ap_setup(int)
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
 type Ptid_t uint
-func Install_traphandler(func(tf *[23]int, ptid Ptid_t, notify int))
+func Install_traphandler(func(tf *[24]int, ptid Ptid_t, notify int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
@@ -38,11 +38,11 @@ func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
 func Pmsga(*uint8, int, int8)
 func Pnum(int)
-func Procadd(ptid Ptid_t, tf *[23]int, p_pmap int)
+func Procadd(ptid Ptid_t, tf *[24]int, p_pmap int)
 func Proccontinue()
 func Prockill(Ptid_t)
 func Procnotify(Ptid_t) int
-func Procrunnable(ptid Ptid_t, tf *[23]int, p_pmap int)
+func Procrunnable(ptid Ptid_t, tf *[24]int, p_pmap int)
 func Proctime(ptid Ptid_t) int
 func Kreset()
 func Ktime() int
@@ -60,7 +60,7 @@ func Vtop(*[512]int) int
 func Crash()
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
-func Tfdump(*[23]int)
+func Tfdump(*[24]int)
 func Stackdump(int)
 func Usleep(int)
 func Rflags() int
@@ -73,6 +73,104 @@ func inb(int) int
 var gcticks uint64
 var No_pml4 int
 
+type cpu_t struct {
+	this		int
+	mythread	int
+	rsp		int
+	num		int
+}
+
+type tuser_t struct {
+	tf	uintptr
+	fxbuf	uintptr
+}
+
+type prof_t struct {
+	enabled		int
+	totaltime	int
+	stampstart	int
+}
+
+type thread_t struct {
+	tf		[24]int
+	fx		[64]int
+	user		tuser_t
+	sigtf		[24]int
+	sigfx		[64]int
+	sigstatus	int
+	siglseepfor	int
+	status		int
+	doingsig	int
+	sigstack	int
+	prof		prof_t
+	sleepfor	int
+	sleepret	int
+	futaddr		int
+	pid		int
+	pmap		int
+	notify		int
+}
+
+const(
+  TFSIZE       = 24
+  TFREGS       = 17
+  TF_SYSRSP    = 0
+  TF_FSBASE    = 1
+  TF_R8        = 9
+  TF_RBP       = 10
+  TF_RSI       = 11
+  TF_RDI       = 12
+  TF_RDX       = 13
+  TF_RCX       = 14
+  TF_RBX       = 15
+  TF_RAX       = 16
+  TF_TRAP      = TFREGS
+  TF_RIP       = TFREGS + 2
+  TF_CS        = TFREGS + 3
+  TF_RSP       = TFREGS + 5
+  TF_SS        = TFREGS + 6
+  TF_RFLAGS    = TFREGS + 4
+    TF_FL_IF     = 1 << 9
+)
+
+func Pushcli() int
+func Popcli(int)
+func Gscpu() *cpu_t
+func Rdmsr(int) int
+func Wrmsr(int, int)
+func Userrun_(*[24]int, bool) (int, int)
+
+func Userrun(tf *[24]int, fxbuf *[64]int, p_pmap int,
+    fastret bool) (int, int) {
+	entersyscall()
+	fl := Pushcli()
+	ct := (*thread_t)(unsafe.Pointer(uintptr(Gscpu().mythread)))
+	// don't need to set the threads pmap to the user prog's pmap since any
+	// interrupt taken while in user code forces a return to kernel mode
+	// through Userrun
+
+	if Rcr3() != p_pmap {
+		Lcr3(p_pmap)
+	}
+
+	// if doing a fast return after a syscall, we need to restore some user
+	// state manually
+	ia32_fs_base := 0xc0000100
+	kfsbase := Rdmsr(ia32_fs_base)
+	Wrmsr(ia32_fs_base, tf[TF_FSBASE])
+
+	ct.user.tf = uintptr(unsafe.Pointer(tf))
+	ct.user.fxbuf = uintptr(unsafe.Pointer(fxbuf))
+	intno, aux := Userrun_(tf, fastret)
+
+	Wrmsr(ia32_fs_base, kfsbase)
+	ct.user.tf = 0
+	ct.user.fxbuf = 0
+	Popcli(fl)
+	exitsyscall()
+	return intno, aux
+}
+
 //go:nosplit
 func sc_setup() {
 	com1  := 0x3f8

commit 0ee2cfce58f7d0f401d3701786e15486cf08b0af
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Sep 23 14:30:33 2015 -0400

    prepare for new interrupt/syscall handling
    
    we will not switch page maps when taking an interrupt/syscall, thus add
    paranoia checks to ensure that the runtime never adds new entries to the
    kernel's PML4.
    
    thus, since user processes share kernel page tables, all mem allocated by the
    runtime will always be mapped in all processes.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 16444ab0c7..768a4d7a8d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -71,6 +71,7 @@ func Trapwake()
 func inb(int) int
 // os_linux.c
 var gcticks uint64
+var No_pml4 int
 
 //go:nosplit
 func sc_setup() {

commit 17aa36ef02950b6ec41fa50645e327808b0df09b
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Jul 29 15:03:49 2015 -0400

    count kernel CPU time

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fee6180d81..16444ab0c7 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -44,6 +44,8 @@ func Prockill(Ptid_t)
 func Procnotify(Ptid_t) int
 func Procrunnable(ptid Ptid_t, tf *[23]int, p_pmap int)
 func Proctime(ptid Ptid_t) int
+func Kreset()
+func Ktime() int
 func Procyield()
 func Rdtsc() uint64
 func Rcr2() int

commit 872b168fe344914550c29b4f1b0cac9f2e70e7fc
Author: Ian Lance Taylor <iant@golang.org>
Date:   Tue Jul 21 22:34:48 2015 -0700

    runtime: if we don't handle a signal on a non-Go thread, raise it
    
    In the past badsignal would crash the program.  In
    https://golang.org/cl/10757044 badsignal was changed to call sigsend,
    to fix issue #3250.  The effect of this was that when a non-Go thread
    received a signal, and os/signal.Notify was not being used to check
    for occurrences of the signal, the signal was ignored.
    
    This changes the code so that if os/signal.Notify is not being used,
    then the signal handler is reset to what it was, and the signal is
    raised again.  This lets non-Go threads handle the signal as they
    wish.  In particular, it means that a segmentation violation in a
    non-Go thread will ordinarily crash the process, as it should.
    
    Fixes #10139.
    Update #11794.
    
    Change-Id: I2109444aaada9d963ad03b1d071ec667760515e5
    Reviewed-on: https://go-review.googlesource.com/12503
    Reviewed-by: Russ Cox <rsc@golang.org>
    Run-TryBot: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 523d28b210..bd492f5e3b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -29,8 +29,8 @@ func rtsigprocmask(sig uint32, new, old *sigset, size int32)
 
 //go:noescape
 func getrlimit(kind int32, limit unsafe.Pointer) int32
-func raise(sig uint32)
-func raiseproc(sig uint32)
+func raise(sig int32)
+func raiseproc(sig int32)
 
 //go:noescape
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32

commit f5b812e2c082dc66ca8d544c552b514d62b84c4c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Jul 18 16:47:05 2015 -0400

    user time accounting

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index bf27976954..fee6180d81 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -43,6 +43,7 @@ func Proccontinue()
 func Prockill(Ptid_t)
 func Procnotify(Ptid_t) int
 func Procrunnable(ptid Ptid_t, tf *[23]int, p_pmap int)
+func Proctime(ptid Ptid_t) int
 func Procyield()
 func Rdtsc() uint64
 func Rcr2() int

commit 25519899364645e69efad0eab93a08e6719be5cc
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jul 17 17:54:52 2015 -0400

    accounting for system/sleep time; no user time yet

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index f9e4b1161b..bf27976954 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -28,6 +28,7 @@ func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
+func Nanotime() int
 func Inb(int) int
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)

commit 4188417ffa117ea3ca927335a72d8f5bf536c69f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jul 7 13:07:13 2015 -0400

    TLB shootdowns with test
    
    i learned that gcc will not warn if it fails to align data as requested.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ba5aaab000..f9e4b1161b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -50,6 +50,7 @@ func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
 func Sti()
+func Tlbadmit(int, int, int, int)
 func Vtop(*[512]int) int
 
 func Crash()

commit 80fa2b793062868560cfe2c6143906652cfbd005
Author: Jon Gjengset <jon@thesquareplanet.com>
Date:   Thu May 14 17:03:34 2015 -0400

    Make sc_put wait correctly again

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 07a5c532d4..ba5aaab000 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -88,20 +88,26 @@ func sc_setup() {
 	inb(com1 + data)
 }
 
+const com1 = 0x3f8
+const lstatus = 5
+
 //go:nosplit
-func sc_put(c int8) {
-	com1 := 0x3f8
-	lstatus := 5
+func sc_put_(c int8) {
 	for inb(com1 + lstatus) & 0x20 == 0 {
 	}
+	Outb(com1, int(c))
+}
+
+//go:nosplit
+func sc_put(c int8) {
 	if c == '\n' {
-		Outb(com1, int('\r'))
+		sc_put_('\r')
 	}
-	Outb(com1, int(c))
+	sc_put_(c)
 	if c == '\b' {
 		// clear the previous character
-		Outb(com1, int(' '))
-		Outb(com1, int('\b'))
+		sc_put_(' ')
+		sc_put_('\b')
 	}
 }
 

commit ec50cd95caf0cdd85222a6b05da00ca290fbc75a
Author: Jon Gjengset <jon@thesquareplanet.com>
Date:   Thu May 14 14:55:50 2015 -0400

    Avoid extra stack frame for sc_put(\n)

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a08a530267..07a5c532d4 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -92,11 +92,11 @@ func sc_setup() {
 func sc_put(c int8) {
 	com1 := 0x3f8
 	lstatus := 5
-	if c == '\n' {
-		sc_put('\r')
-	}
 	for inb(com1 + lstatus) & 0x20 == 0 {
 	}
+	if c == '\n' {
+		Outb(com1, int('\r'))
+	}
 	Outb(com1, int(c))
 	if c == '\b' {
 		// clear the previous character

commit 864f37688f95337cd9d7424cd493cabe56af1329
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed May 13 19:50:43 2015 -0400

    input support for serial console driver
    
    now i can easily run biscuit remotely via qemu -- i no longer need X11
    forwarding/qemu's vga screen! this will be useful while i'm in utah...

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6dfba098b5..a08a530267 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -76,13 +76,16 @@ func sc_setup() {
 	lctl  := 3
 	mctl  := 4
 
-	Outb(com1 + intr, 0x00)
+	Outb(com1 + ififo, 0x0)
 	Outb(com1 + lctl, 0x80)
 	Outb(com1 + data, 115200/9600)
-	Outb(com1 + intr, 0x00)
+	Outb(com1 + intr, 0x0)
 	Outb(com1 + lctl, 0x03)
-	Outb(com1 + ififo, 0xc7)
-	Outb(com1 + mctl, 0x0b)
+	Outb(com1 + mctl, 0x0)
+	Outb(com1 + intr, 0x01)
+
+	inb(com1 + ififo)
+	inb(com1 + data)
 }
 
 //go:nosplit

commit a0ef4afa30df7ffa7524f41ae5b77fb4460eb990
Author: Jon Gjengset <jon@thesquareplanet.com>
Date:   Wed May 13 17:38:39 2015 -0400

    Implement backspace

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 72eda15455..6dfba098b5 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -95,6 +95,11 @@ func sc_put(c int8) {
 	for inb(com1 + lstatus) & 0x20 == 0 {
 	}
 	Outb(com1, int(c))
+	if c == '\b' {
+		// clear the previous character
+		Outb(com1, int(' '))
+		Outb(com1, int('\b'))
+	}
 }
 
 type put_t struct {
@@ -111,9 +116,16 @@ func vga_put(c int8, attr int8) {
 	if c != '\n' {
 		// erase the previous line
 		a := int16(attr) << 8
+		backspace := c == '\b'
+		if backspace {
+			put.vx--
+			c = ' '
+		}
 		v := a | int16(c)
 		p[put.vy * 80 + put.vx] = v
-		put.vx++
+		if !backspace {
+			put.vx++
+		}
 		put.fakewrap = false
 	} else {
 		// if we wrapped the text because of a long line in the

commit 0c9544f66afccab98449e99e9ed95f247407ed9d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri May 8 09:44:44 2015 -0400

    start threads
    
    let a process have more than one schedule entry and cleanup notification
    mechanism since terminating a process is more annoying now. also try coding
    style that makes frequent use of types to help differentiate identifiers.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a0f936a81e..72eda15455 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -21,7 +21,8 @@ func trapinit_m(*g)
 func Ap_setup(int)
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
-func Install_traphandler(func(tf *[23]int, uc int, pidk int))
+type Ptid_t uint
+func Install_traphandler(func(tf *[23]int, ptid Ptid_t, notify int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
@@ -36,11 +37,11 @@ func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
 func Pmsga(*uint8, int, int8)
 func Pnum(int)
-func Procadd(tf *[23]int, uc int, p_pmap int)
+func Procadd(ptid Ptid_t, tf *[23]int, p_pmap int)
 func Proccontinue()
-func Prockill(int)
-func Procnotify(int) int
-func Procrunnable(int, *[23]int, int)
+func Prockill(Ptid_t)
+func Procnotify(Ptid_t) int
+func Procrunnable(ptid Ptid_t, tf *[23]int, p_pmap int)
 func Procyield()
 func Rdtsc() uint64
 func Rcr2() int

commit 79fe6a2aab12e92cc2a5e658bdd7880f795e37bd
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon May 4 14:42:35 2015 -0400

    cleanup exec
    
    fix clunky interface instead of working around it

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 66415bf1eb..a0f936a81e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -40,7 +40,7 @@ func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
 func Prockill(int)
 func Procnotify(int) int
-func Procrunnable(int, *[23]int)
+func Procrunnable(int, *[23]int, int)
 func Procyield()
 func Rdtsc() uint64
 func Rcr2() int

commit 76a288d752cd38fecdb0f2a739d337695e75001f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri May 1 00:39:38 2015 -0400

    sys_kill
    
    need to cooperate with the runtime since go code is completely unaware of timer
    interrupts but a timer interrupt may be the only way that a user program stops
    executing.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8a0589c2ca..66415bf1eb 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -21,7 +21,7 @@ func trapinit_m(*g)
 func Ap_setup(int)
 func Cli()
 func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
-func Install_traphandler(func(tf *[23]int, uc int))
+func Install_traphandler(func(tf *[23]int, uc int, pidk int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
 func Kpmap_p() int
@@ -39,6 +39,7 @@ func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
 func Prockill(int)
+func Procnotify(int) int
 func Procrunnable(int, *[23]int)
 func Procyield()
 func Rdtsc() uint64

commit f48b075a57db04c9d9877cfe050debb49f2fc918
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Apr 28 14:58:07 2015 -0400

    x

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 4fb0064941..8a0589c2ca 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -63,7 +63,6 @@ func Trapwake()
 
 func inb(int) int
 // os_linux.c
-var newlines int32
 var gcticks uint64
 
 //go:nosplit
@@ -97,38 +96,48 @@ func sc_put(c int8) {
 }
 
 type put_t struct {
-	vx	int
-	vy	int
+	vx		int
+	vy		int
+	fakewrap	bool
 }
 
 var put put_t
 
 //go:nosplit
 func vga_put(c int8, attr int8) {
+	p := (*[1999]int16)(unsafe.Pointer(uintptr(0xb8000)))
 	if c != '\n' {
-		p := (*[1999]int16)(unsafe.Pointer(uintptr(0xb8000)))
 		// erase the previous line
-		if put.vx == 0 {
-			for i := 0; i < 79; i++ {
-				p[put.vy * 80 + put.vx + i] = 0
-			}
-		}
 		a := int16(attr) << 8
 		v := a | int16(c)
 		p[put.vy * 80 + put.vx] = v
 		put.vx++
-	} else if newlines != 0 {
-		put.vx = 0
-		put.vy++
+		put.fakewrap = false
+	} else {
+		// if we wrapped the text because of a long line in the
+		// immediately previous call to vga_put, don't add another
+		// newline if we asked to print '\n'.
+		if put.fakewrap {
+			put.fakewrap = false
+		} else {
+			put.vx = 0
+			put.vy++
+		}
 	}
 	if put.vx >= 79 {
 		put.vx = 0
 		put.vy++
+		put.fakewrap = true
 	}
 
 	if put.vy >= 25 {
 		put.vy = 0
 	}
+	if put.vx == 0 {
+		for i := 0; i < 79; i++ {
+			p[put.vy * 80 + put.vx + i] = 0
+		}
+	}
 }
 
 var SCenable bool = true

commit 5c8fbc6f1e4ba78133c53ce73f82ad10e81b42f8
Author: Srdjan Petrovic <spetrovic@google.com>
Date:   Thu Apr 9 11:12:12 2015 -0700

    runtime: signal forwarding
    
    Forward signals to signal handlers installed before Go installs its own,
    under certain circumstances.  In particular, as iant@ suggests, signals are
    forwarded iff:
       (1) a non-SIG_DFL signal handler existed before Go, and
       (2) signal is synchronous (i.e., one of SIGSEGV, SIGBUS, SIGFPE), and
            (3a) signal occured on a non-Go thread, or
            (3b) signal occurred on a Go thread but in CGo code.
    
    Supported only on Linux, for now.
    
    Change-Id: I403219ee47b26cf65da819fb86cf1ec04d3e25f5
    Reviewed-on: https://go-review.googlesource.com/8712
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index abea5d61aa..523d28b210 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,6 +18,9 @@ func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
 //go:noescape
 func sigaltstack(new, old *sigaltstackt)
 
+//go:noescape
+func sigfwd(fn uintptr, sig uint32, info *siginfo, ctx unsafe.Pointer)
+
 //go:noescape
 func setitimer(mode int32, new, old *itimerval)
 

commit ca9128f18fe75878ba2d5e0df09ae755c085f72a
Author: Srdjan Petrovic <spetrovic@google.com>
Date:   Fri Apr 17 17:27:07 2015 -0700

    runtime: merge clone0 and clone
    
    We initially added clone0 to handle the case when G or M don't exist, but
    it turns out that we could have just modified clone.  (It also helps that
    the function we're invoking in clone0 no longer needs arguments.)
    
    As a side-effect, newosproc0 is now supported on all linux archs.
    
    Change-Id: Ie603af75d8f164310fc16446052d83743961f3ca
    Reviewed-on: https://go-review.googlesource.com/9164
    Reviewed-by: David Crawshaw <crawshaw@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8e4c05db93..abea5d61aa 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -12,9 +12,6 @@ func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer,
 //go:noescape
 func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
 
-//go:noescape
-func clone0(flags int32, stk, fn, fnarg unsafe.Pointer) int32
-
 //go:noescape
 func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
 

commit d5600b6e4f75d73444053ba79074aed81105d9bf
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Apr 8 11:29:39 2015 -0400

    print carriage return before newline for serial console
    
    so it doesn't look terrible on my serial terminal emulator

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b594f97a4b..4fb0064941 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -88,6 +88,9 @@ func sc_setup() {
 func sc_put(c int8) {
 	com1 := 0x3f8
 	lstatus := 5
+	if c == '\n' {
+		sc_put('\r')
+	}
 	for inb(com1 + lstatus) & 0x20 == 0 {
 	}
 	Outb(com1, int(c))

commit e8694c8196e39328d6d61b1f32228d21112008d7
Author: Srdjan Petrovic <spetrovic@google.com>
Date:   Wed Mar 25 17:50:35 2015 -0700

    runtime: initialize shared library at library-load time
    
    This is Part 2 of the change, see Part 1 here: in https://go-review.googlesource.com/#/c/7692/
    
    Suggested by iant@, we use the library initialization entry point to:
        - create a new OS thread and run the "regular" runtime init stack on
          that thread
        - return immediately from the main (i.e., loader) thread
        - at the first CGO invocation, we wait for the runtime initialization
          to complete.
    
    The above mechanism is implemented only on linux_amd64.  Next step is to
    support it on linux_arm.  Other platforms don't yet support shared library
    compiling/linking, but we intend to use the same strategy there as well.
    
    Change-Id: Ib2c81b1b83bee837134084b75a3beecfb8de6bf4
    Reviewed-on: https://go-review.googlesource.com/8094
    Run-TryBot: Srdjan Petrovic <spetrovic@google.com>
    TryBot-Result: Gobot Gobot <gobot@golang.org>
    Reviewed-by: Ian Lance Taylor <iant@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index abea5d61aa..8e4c05db93 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -12,6 +12,9 @@ func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer,
 //go:noescape
 func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
 
+//go:noescape
+func clone0(flags int32, stk, fn, fnarg unsafe.Pointer) int32
+
 //go:noescape
 func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
 

commit f926f3e8f2fb8c72090cb47780bb78ac16e2267d
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 23 11:15:24 2015 -0400

    Gcticks()

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ab49f782ff..b594f97a4b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -58,6 +58,7 @@ func Stackdump(int)
 func Usleep(int)
 func Rflags() int
 func Resetgcticks() uint64
+func Gcticks() uint64
 func Trapwake()
 
 func inb(int) int

commit ab6e4e6208b9f9df9760b11556f294d6f6a083a3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 20 14:37:07 2015 -0400

    wakeup a goroutine for CPU interrupts/no polling
    
    biscuit starts a special trap consuming goroutine, as before, but now instead
    of the Gosched/Usleep nonsense, it calls Trapsched which puts the goroutine to
    sleep if there are no more interrupts to process. when an interrupt comes in,
    the handler sets a flag which is checked by all Ps when looking for goroutines
    to run. they race for ownership of the trap consuming goroutine and the winner
    puts it on his run queue.
    
    another notable change: when an M can't find a goroutine to run, it yields
    instead of stopping itself.
    
    the difference is immediately noticable -- the test hardware with 1 or 8 CPUs
    is much more responsive. unfortunately, it doesn't improve our numbers in the
    bmwrite benchmark! interesting.
    
    the sleep/wakeup code is a little complicated; i hope to simplify it.
    
    comment out my addition to the compiler that generates the check in the
    function prologue since we don't need it. it may be useful later.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 4b382496a2..ab49f782ff 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -15,6 +15,8 @@ func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
 func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+func trapsched_m(*g)
+func trapinit_m(*g)
 
 func Ap_setup(int)
 func Cli()
@@ -46,7 +48,6 @@ func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
 func Sti()
-func Trapwake()
 func Vtop(*[512]int) int
 
 func Crash()
@@ -57,6 +58,7 @@ func Stackdump(int)
 func Usleep(int)
 func Rflags() int
 func Resetgcticks() uint64
+func Trapwake()
 
 func inb(int) int
 // os_linux.c
@@ -153,3 +155,12 @@ func cls() {
 	sc_put('s')
 	sc_put('\n')
 }
+
+func Trapsched() {
+	mcall(trapsched_m)
+}
+
+// called only once to setup
+func Trapinit() {
+	mcall(trapinit_m)
+}

commit 9f5d74ba51e450ba0fe0c643dee2a36e1ad0bf24
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 19 22:17:53 2015 -0400

    checkpoint code for event-driven CPU exception handling instead of polling
    
    insert 4 instructions into the go function call prologue to check for
    interrupts and call the interrupt handler if necessary. be careful not to
    clobber the context or TLS registers.
    
    i just realized this approach has a big problem though: what if there are no
    runnable go routines? thus we really need to start/wakeup a goroutine if there
    aren't any runnable goroutines.
    
    starting a goroutine from interrupt context is difficult though (i tried it
    earlier today) since we need to manipulate run queues in interrupt context --
    obviously interrupt code cannot attempt to acquire the mutex protecting the run
    queues.
    
    need to figure something out.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 9dded432ad..4b382496a2 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -46,6 +46,7 @@ func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
 func Sti()
+func Trapwake()
 func Vtop(*[512]int) int
 
 func Crash()

commit c383c58af8e91f68b9f582057db8349da495d762
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 17 20:28:22 2015 -0400

    easy disabled of serial console
    
    serial console driver is slow since it uses polling. it can impact performance
    measurements by as much as 5x.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d445a0b601..9dded432ad 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -124,16 +124,22 @@ func vga_put(c int8, attr int8) {
 	}
 }
 
+var SCenable bool = true
+
 //go:nosplit
 func putch(c int8) {
 	vga_put(c, 0x7)
-	sc_put(c)
+	if SCenable {
+		sc_put(c)
+	}
 }
 
 //go:nosplit
 func putcha(c int8, a int8) {
 	vga_put(c, a)
-	sc_put(c)
+	if SCenable {
+		sc_put(c)
+	}
 }
 
 //go:nosplit

commit 9ffeb4776bdb74bd0c63dfa4ff5ddfeec38a749e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Mar 17 13:30:43 2015 -0400

    make vga easier to read

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index c3ce860d1f..d445a0b601 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -100,6 +100,12 @@ var put put_t
 func vga_put(c int8, attr int8) {
 	if c != '\n' {
 		p := (*[1999]int16)(unsafe.Pointer(uintptr(0xb8000)))
+		// erase the previous line
+		if put.vx == 0 {
+			for i := 0; i < 79; i++ {
+				p[put.vy * 80 + put.vx + i] = 0
+			}
+		}
 		a := int16(attr) << 8
 		v := a | int16(c)
 		p[put.vy * 80 + put.vx] = v

commit f05e63dbd6110153c57c3b815123f35d4267e7fd
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 19:27:29 2015 -0400

    count cycles spent in GC

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8aadbd1f75..c3ce860d1f 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -55,10 +55,12 @@ func Tfdump(*[23]int)
 func Stackdump(int)
 func Usleep(int)
 func Rflags() int
+func Resetgcticks() uint64
 
 func inb(int) int
 // os_linux.c
 var newlines int32
+var gcticks uint64
 
 //go:nosplit
 func sc_setup() {

commit 7e612ff4f61aded306b539a1c42237c99e19a6d0
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 18:34:30 2015 -0400

    cycle counting

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index dfed56e767..8aadbd1f75 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -39,6 +39,7 @@ func Proccontinue()
 func Prockill(int)
 func Procrunnable(int, *[23]int)
 func Procyield()
+func Rdtsc() uint64
 func Rcr2() int
 func Rcr3() int
 func Rrsp() int

commit cc32fca80b4e18eceb05b2468024428b278e6103
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 14:10:57 2015 -0400

    set serial console speed
    
    gee i wish i would have put useful comments in this initialization routine

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 450660f2a6..dfed56e767 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -70,7 +70,7 @@ func sc_setup() {
 
 	Outb(com1 + intr, 0x00)
 	Outb(com1 + lctl, 0x80)
-	Outb(com1 + data, 0x03)
+	Outb(com1 + data, 115200/9600)
 	Outb(com1 + intr, 0x00)
 	Outb(com1 + lctl, 0x03)
 	Outb(com1 + ififo, 0xc7)

commit 24e6d973dc265c98e43bc4bd58f80c19fc12b50f
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 16 14:02:28 2015 -0400

    Outw, Rflags

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 9539bf2136..450660f2a6 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -29,6 +29,7 @@ func Inb(int) int
 func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
 func Outb(int, int)
+func Outw(int, int)
 func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
 func Pmsga(*uint8, int, int8)
@@ -52,6 +53,7 @@ func Fnaddri(func(int)) int
 func Tfdump(*[23]int)
 func Stackdump(int)
 func Usleep(int)
+func Rflags() int
 
 func inb(int) int
 // os_linux.c

commit 5b3cdb9e54912084087189fa5fc26ba71d5a026a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 14 16:44:59 2015 -0400

    synchronize user console print too

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 2ebb36e198..9539bf2136 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -31,6 +31,7 @@ func Insl(int, unsafe.Pointer, int)
 func Outb(int, int)
 func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
+func Pmsga(*uint8, int, int8)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
@@ -118,7 +119,8 @@ func putch(c int8) {
 	sc_put(c)
 }
 
-func Putcha(c int8, a int8) {
+//go:nosplit
+func putcha(c int8, a int8) {
 	vga_put(c, a)
 	sc_put(c)
 }

commit c9ac55d98df5dda3b9d387cede4ba3925ef96201
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Mar 13 17:38:22 2015 -0400

    PCI config space reads
    
    necessary to clear disk interrupts for PCI-native IDE.
    
    fix panic newlines too.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0602529f6b..2ebb36e198 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -26,8 +26,10 @@ func Kpmap_p() int
 func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
 func Inb(int) int
+func Inl(int) int
 func Insl(int, unsafe.Pointer, int)
 func Outb(int, int)
+func Outl(int, int)
 func Outsl(int, unsafe.Pointer, int)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
@@ -44,7 +46,6 @@ func Sti()
 func Vtop(*[512]int) int
 
 func Crash()
-func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
 func Tfdump(*[23]int)
@@ -52,6 +53,8 @@ func Stackdump(int)
 func Usleep(int)
 
 func inb(int) int
+// os_linux.c
+var newlines int32
 
 //go:nosplit
 func sc_setup() {
@@ -95,7 +98,7 @@ func vga_put(c int8, attr int8) {
 		v := a | int16(c)
 		p[put.vy * 80 + put.vx] = v
 		put.vx++
-	} else {
+	} else if newlines != 0 {
 		put.vx = 0
 		put.vy++
 	}

commit 52b5355129b49f193a85a3f3adfd0018338b8aa6
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Mar 12 15:53:37 2015 -0400

    get disk working with recent SATA controllers
    
    SATA controllers can be configured to support ATA/IDE, typically in the BIOS.
    There are two IDE modes, however: PCI-native and legacy. In legacy mode, the
    controller (supposedly) exactly emulates legacy IDE disks (IO ports
    0x1f6/0x3f6, IRQ 14). In PCI-native mode, PCI may have chosen different IO
    ports/mem and IRQs -- we should get the command/control block port offsets from
    the BARs and the IRQ from the Interrupt Line Register of the SATA controller's
    PCI registers. We can also put the SATA controller into legacy/PCI-native IDE
    mode directly via PCI register writes.
    
    Hard code the IDE port offset/IRQ as per the PCI registers for now. If we are
    using Qemu, use the legacy IO ports.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0e1ae93d7e..0602529f6b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -25,10 +25,10 @@ func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
-func Inb(int32) int
-func Insl(int32, unsafe.Pointer, int)
+func Inb(int) int
+func Insl(int, unsafe.Pointer, int)
 func Outb(int, int)
-func Outsl(int32, unsafe.Pointer, int)
+func Outsl(int, unsafe.Pointer, int)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()

commit 955ff35b30f876da6cf9c07bbe27883ea6b8b85c
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 9 14:57:10 2015 -0400

    prevent kernel threads from polling interrupt queue until timer interrupt
    
    timeslice size was a red-herring -- the syscall latency problem was that once
    all syscall work was completed, all kernel threads just spin in trap() looking
    for interrupts in the queue until the timeslice expires, only then finally
    returning to user-space (only to immediately syscall again).
    
    explicitly yield after Gosched to prevent this. this is not a great solution
    because, if there are many runnable user processes, an unlucky kernel thread
    may yield and start running a user process instead of handling the syscall.
    maybe this doesn't matter much.
    
    it would probably be better to cooperatively switch to trap() -- maybe we can
    do this in the stack growth prologue.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 699bdde985..0e1ae93d7e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -49,6 +49,7 @@ func Fnaddr(func()) int
 func Fnaddri(func(int)) int
 func Tfdump(*[23]int)
 func Stackdump(int)
+func Usleep(int)
 
 func inb(int) int
 

commit d048b0d057860b97eb416153f2c5b9510e1a52db
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Mar 9 09:11:39 2015 -0400

    send 8259a EOI on disk interrupt
    
    biscuit now seems to be fully working on at least one computer.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0a34d34c5b..699bdde985 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -27,7 +27,7 @@ func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
 func Inb(int32) int
 func Insl(int32, unsafe.Pointer, int)
-func Outb(int32, int32)
+func Outb(int, int)
 func Outsl(int32, unsafe.Pointer, int)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
@@ -51,7 +51,6 @@ func Tfdump(*[23]int)
 func Stackdump(int)
 
 func inb(int) int
-func outb(int, int)
 
 //go:nosplit
 func sc_setup() {
@@ -62,13 +61,13 @@ func sc_setup() {
 	lctl  := 3
 	mctl  := 4
 
-	outb(com1 + intr, 0x00)
-	outb(com1 + lctl, 0x80)
-	outb(com1 + data, 0x03)
-	outb(com1 + intr, 0x00)
-	outb(com1 + lctl, 0x03)
-	outb(com1 + ififo, 0xc7)
-	outb(com1 + mctl, 0x0b)
+	Outb(com1 + intr, 0x00)
+	Outb(com1 + lctl, 0x80)
+	Outb(com1 + data, 0x03)
+	Outb(com1 + intr, 0x00)
+	Outb(com1 + lctl, 0x03)
+	Outb(com1 + ififo, 0xc7)
+	Outb(com1 + mctl, 0x0b)
 }
 
 //go:nosplit
@@ -77,7 +76,7 @@ func sc_put(c int8) {
 	lstatus := 5
 	for inb(com1 + lstatus) & 0x20 == 0 {
 	}
-	outb(com1, int(c))
+	Outb(com1, int(c))
 }
 
 type put_t struct {

commit 8686c9080ffc0f472c9a672b99c6a7c881bc868a
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sat Mar 7 16:58:34 2015 -0500

    use 2MB pages for direct map if CPU doesn't support 1GB pages
    
    my test hardware sadly does not.
    
    biscuit now boots on real hardware!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e0025afd9e..0a34d34c5b 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,6 +18,7 @@ func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
 func Ap_setup(int)
 func Cli()
+func Cpuid(uint32, uint32) (uint32, uint32, uint32, uint32)
 func Install_traphandler(func(tf *[23]int, uc int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int

commit c14beb90191e74163c20c48596a5eed576fbc664
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 23 19:23:22 2015 -0500

    journaling fs
    
    neat! fs syscalls talk to the logging daemon for admission control and
    committing.
    
    haven't written the recovery procedure yet. poorly tested so far too.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 0dd7d705f5..e0025afd9e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -42,6 +42,7 @@ func Sidt(*int)
 func Sti()
 func Vtop(*[512]int) int
 
+func Crash()
 func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int

commit 5c0351af5230d7b8b13635ee0402a5359372ba18
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Feb 15 20:47:30 2015 -0500

    ide disk driver and ide server go-routine daemon
    
    kernel code submits requests for ide reads/writes through a channel. its very
    straight-forward! we get kernel thread sleeping/wakeup and safe queue access
    for free via go channels. see ide_test() for example code demonstrating how to
    use the ide daemon.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3557527d66..0dd7d705f5 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -24,7 +24,10 @@ func Kpmap() *[512]int
 func Kpmap_p() int
 func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
+func Inb(int32) int
+func Insl(int32, unsafe.Pointer, int)
 func Outb(int32, int32)
+func Outsl(int32, unsafe.Pointer, int)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()

commit 45823c67235655908504cff858bbc1560d1a6d6e
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 12 11:53:47 2015 -0500

    lockfree per-cpu interrupt circular buffers

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b17761ddd9..3557527d66 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -43,9 +43,7 @@ func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
 func Tfdump(*[23]int)
-func Turdyprog()
 func Stackdump(int)
-func Hackunlock()
 
 func inb(int) int
 func outb(int, int)

commit fb559b6811b566960600daa383f55a2c534ed3d6
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Feb 10 19:56:10 2015 -0500

    horrible goddamn bug
    
    go uses SSE non-temporal moves for fast copying and zeroing of newly allocated
    objects. if you are very unlucky, a zero-initializing go routine may be context
    switched after a copying go routine. since my scheduler did not save/restore
    FPU/SSE state, the SSE registers used were not saved/restored on a context
    switch.
    
    the end result was newly allocated maps having segments of 128 non-zero bytes
    and copied objects having 128 byte segments that were zeroed.
    
    save restore SSE state, and fix a few other bugs like serial access to mmaps,
    trap queue, and syscalls with locks (i'll remove these coarse locks next).
    
    also do go routine stack checks manually in C stubs -- the stack check inserted
    by the compiler panicks if any C code runs on a go routine stack. this makes me
    nervous because i'm not exactly sure why...
    
    goodbye four days of my life

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 385b3d957c..b17761ddd9 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -42,8 +42,10 @@ func Vtop(*[512]int) int
 func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
+func Tfdump(*[23]int)
 func Turdyprog()
 func Stackdump(int)
+func Hackunlock()
 
 func inb(int) int
 func outb(int, int)

commit 4f03c124a56f2eef79bac417c21fb7c55d1fd7d5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Feb 5 15:40:50 2015 -0500

    write syscall

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index ef4fe0a8d8..385b3d957c 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -29,9 +29,10 @@ func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
 func Prockill(int)
-func Procrunnable(int)
+func Procrunnable(int, *[23]int)
 func Procyield()
 func Rcr2() int
+func Rcr3() int
 func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
@@ -82,10 +83,11 @@ type put_t struct {
 var put put_t
 
 //go:nosplit
-func vga_put(c int8) {
+func vga_put(c int8, attr int8) {
 	if c != '\n' {
 		p := (*[1999]int16)(unsafe.Pointer(uintptr(0xb8000)))
-		v := 0x0700 | int16(c)
+		a := int16(attr) << 8
+		v := a | int16(c)
 		p[put.vy * 80 + put.vx] = v
 		put.vx++
 	} else {
@@ -104,14 +106,19 @@ func vga_put(c int8) {
 
 //go:nosplit
 func putch(c int8) {
-	vga_put(c)
+	vga_put(c, 0x7)
+	sc_put(c)
+}
+
+func Putcha(c int8, a int8) {
+	vga_put(c, a)
 	sc_put(c)
 }
 
 //go:nosplit
 func cls() {
 	for i:= 0; i < 1974; i++ {
-		vga_put(' ')
+		vga_put(' ', 0x7)
 	}
 	sc_put('c')
 	sc_put('l')

commit 39dad4bd6d345346414167243e5e091da239aeb8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Feb 4 15:43:46 2015 -0500

    evil bugfixes, fix futex
    
    all bugs were races in the scheduler/trap handling. they were hidden by a slow
    implementation of futex. there were 3 races:
    
    1- since go code is in ring 0, timer interrupts during go code are taken on go
    stacks. consider the following events on CPU1:
            - takes a timer interrupt while executing go process alpha
            - acquires the scheduling lock
            - marks alpha as runnable
            - finds and marks a new process as running
            - releases the lock
    then before CPU 1 has time to iret to the new process and stop using alpha's
    stack, CPU 2 starts running alpha and itself takes a timer interrupt
    overwriting CPU 1's stack.
    
    solve this by using the IST to always take timer interrupts on kernel stacks.
    alternatively, we could release the lock in trapret() once the stack will no
    longer be used, but it seems better to always take traps on the interrupt stack
    anyway.
    
    2- user application page faults. trap handler running on CPU 1 posts exception
    to go kernel, which is running on CPU 2, and go kernel immediately responds by
    terminating the application, reclaiming the page tables while CPU 1 is using
    them.
    
    solve this by changing to the kernel's page tables whenever posting a trap that
    may result in application termination.
    
    3- when a process on CPU 1 is trying to go to sleep for a small amount of time
    and marks itself sleeping, another CPU wakes and starts running the process
    before CPU 1 has yielded and stored the process' state.
    
    solve this by marking a process as sleeping in the timer interrupt, not
    futex(). thus a CPU trying to wake processes up will not mark the process as
    runnable yet.
    
    in conclusion, i am desperately trying to simplify the design!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 8bbd6e58af..ef4fe0a8d8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -21,6 +21,8 @@ func Cli()
 func Install_traphandler(func(tf *[23]int, uc int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
+func Kpmap_p() int
+func Lcr3(int)
 func Memmove(unsafe.Pointer, unsafe.Pointer, int)
 func Outb(int32, int32)
 func Pnum(int)

commit 52007cc98cb8963c1f63ef69cd35112d77b9daf3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Mon Feb 2 19:04:18 2015 -0500

    parallel runtime scheduler
    
    there were many bugs. discovered a bug in the plan9 C compiler and the plan9
    assembler on the way.
    
    now kernel parallelism should be easy. scheduling code desperately needs
    cleanup; will do this soon.
    
    hack_futex has been band-aid fixed -- need to fix it properly later.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index fcba539c39..8bbd6e58af 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -40,6 +40,7 @@ func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
 func Turdyprog()
+func Stackdump(int)
 
 func inb(int) int
 func outb(int, int)
@@ -110,5 +111,8 @@ func cls() {
 	for i:= 0; i < 1974; i++ {
 		vga_put(' ')
 	}
+	sc_put('c')
+	sc_put('l')
+	sc_put('s')
 	sc_put('\n')
 }

commit ba56baba55a83523ba96b67faa97578208fc3db4
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Sun Feb 1 15:44:08 2015 -0500

    serial console driver
    
    now i can actually debug biscuit over slow connection remotely.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b8b690e398..fcba539c39 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -40,3 +40,75 @@ func Newlines(int64)
 func Fnaddr(func()) int
 func Fnaddri(func(int)) int
 func Turdyprog()
+
+func inb(int) int
+func outb(int, int)
+
+//go:nosplit
+func sc_setup() {
+	com1  := 0x3f8
+	data  := 0
+	intr  := 1
+	ififo := 2
+	lctl  := 3
+	mctl  := 4
+
+	outb(com1 + intr, 0x00)
+	outb(com1 + lctl, 0x80)
+	outb(com1 + data, 0x03)
+	outb(com1 + intr, 0x00)
+	outb(com1 + lctl, 0x03)
+	outb(com1 + ififo, 0xc7)
+	outb(com1 + mctl, 0x0b)
+}
+
+//go:nosplit
+func sc_put(c int8) {
+	com1 := 0x3f8
+	lstatus := 5
+	for inb(com1 + lstatus) & 0x20 == 0 {
+	}
+	outb(com1, int(c))
+}
+
+type put_t struct {
+	vx	int
+	vy	int
+}
+
+var put put_t
+
+//go:nosplit
+func vga_put(c int8) {
+	if c != '\n' {
+		p := (*[1999]int16)(unsafe.Pointer(uintptr(0xb8000)))
+		v := 0x0700 | int16(c)
+		p[put.vy * 80 + put.vx] = v
+		put.vx++
+	} else {
+		put.vx = 0
+		put.vy++
+	}
+	if put.vx >= 79 {
+		put.vx = 0
+		put.vy++
+	}
+
+	if put.vy >= 25 {
+		put.vy = 0
+	}
+}
+
+//go:nosplit
+func putch(c int8) {
+	vga_put(c)
+	sc_put(c)
+}
+
+//go:nosplit
+func cls() {
+	for i:= 0; i < 1974; i++ {
+		vga_put(' ')
+	}
+	sc_put('\n')
+}

commit 3af60b7bc16a62b07aaefc4089da7c5d65ef1d30
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 30 15:32:01 2015 -0500

    initialize lapic/TSS on APs
    
    shuffle stuff around so APs can reuse init code

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b9b0ce0f60..b8b690e398 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -16,6 +16,7 @@ func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
+func Ap_setup(int)
 func Cli()
 func Install_traphandler(func(tf *[23]int, uc int))
 func Invlpg(unsafe.Pointer)
@@ -37,4 +38,5 @@ func Vtop(*[512]int) int
 
 func Newlines(int64)
 func Fnaddr(func()) int
+func Fnaddri(func(int)) int
 func Turdyprog()

commit 4fcb3dc4cfefff837f3bd453d988314e104d91f8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 30 12:36:59 2015 -0500

    setup AP stacks
    
    enter the terrible world of multicore

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 773a530198..b9b0ce0f60 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -29,6 +29,7 @@ func Prockill(int)
 func Procrunnable(int)
 func Procyield()
 func Rcr2() int
+func Rrsp() int
 func Sgdt(*int)
 func Sidt(*int)
 func Sti()

commit af4dfcf8673af71c3c914043debc982c6a5a0390
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 29 18:15:00 2015 -0500

    SMP
    
    find and initialize cpus. the test boots up all found cpus (controllable with
    CPUS env variable) each of which print a hello message, spin for a moment, and
    then fault to make sure the IDT/GDT are properly setup.
    
    there is no synchronization yet. next step is to make the runtime/kernel
    thread-safe.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index d87fb141cc..773a530198 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -20,7 +20,8 @@ func Cli()
 func Install_traphandler(func(tf *[23]int, uc int))
 func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
-func Memmove(unsafe.Pointer, unsafe.Pointer, uint)
+func Memmove(unsafe.Pointer, unsafe.Pointer, int)
+func Outb(int32, int32)
 func Pnum(int)
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
@@ -28,6 +29,8 @@ func Prockill(int)
 func Procrunnable(int)
 func Procyield()
 func Rcr2() int
+func Sgdt(*int)
+func Sidt(*int)
 func Sti()
 func Vtop(*[512]int) int
 

commit d14a07795fbfeb289f67a0ff02b0eb4eca9c3f40
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Jan 28 11:07:33 2015 -0500

    remove Death()

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1f73dcd6e8..d87fb141cc 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -31,6 +31,6 @@ func Rcr2() int
 func Sti()
 func Vtop(*[512]int) int
 
-func Death()
+func Newlines(int64)
 func Fnaddr(func()) int
 func Turdyprog()

commit a5a9d469f5e63b904b81d81754933b5912fea173
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Fri Jan 23 23:30:39 2015 -0500

    elf user programs
    
    any elf object in user/ will be inserted into the kernel as an exec'able
    binary.
    
    it would be nice to strictly separate address ranges of kernel and user
    programs.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index b467cc3e06..1f73dcd6e8 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,15 +18,17 @@ func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
 func Cli()
 func Install_traphandler(func(tf *[23]int, uc int))
+func Invlpg(unsafe.Pointer)
 func Kpmap() *[512]int
+func Memmove(unsafe.Pointer, unsafe.Pointer, uint)
 func Pnum(int)
-func Rcr2() int
-func Sti()
 func Procadd(tf *[23]int, uc int, p_pmap int)
 func Proccontinue()
 func Prockill(int)
 func Procrunnable(int)
 func Procyield()
+func Rcr2() int
+func Sti()
 func Vtop(*[512]int) int
 
 func Death()

commit 19ee6ffbb2a0706e341df96bfe64f7cb45c831ce
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 15:53:34 2015 -0500

    processes
    
    reclaim memory from killed procs using GC(). cool!

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e2ef9a92db..b467cc3e06 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -22,11 +22,11 @@ func Kpmap() *[512]int
 func Pnum(int)
 func Rcr2() int
 func Sti()
-func Useradd(tf *[23]int, uc int, p_pmap int)
-func Usercontinue()
-func Userkill(int)
-func Userrunnable(int)
-func Useryield()
+func Procadd(tf *[23]int, uc int, p_pmap int)
+func Proccontinue()
+func Prockill(int)
+func Procrunnable(int)
+func Procyield()
 func Vtop(*[512]int) int
 
 func Death()

commit 64c545e1227a47cd047a5aa41e257f34a8387597
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 14:01:34 2015 -0500

    more interesting test program
    
    reschedule after syscall, terminate if page fault

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1d3f071dbd..e2ef9a92db 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -24,7 +24,8 @@ func Rcr2() int
 func Sti()
 func Useradd(tf *[23]int, uc int, p_pmap int)
 func Usercontinue()
-func Userrunnable()
+func Userkill(int)
+func Userrunnable(int)
 func Useryield()
 func Vtop(*[512]int) int
 

commit 77bb8f3ddd168e95bea2b7951ed97382efb0e799
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 13:05:50 2015 -0500

    whoops

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index e446981281..1d3f071dbd 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -26,6 +26,7 @@ func Useradd(tf *[23]int, uc int, p_pmap int)
 func Usercontinue()
 func Userrunnable()
 func Useryield()
+func Vtop(*[512]int) int
 
 func Death()
 func Fnaddr(func()) int

commit eee205a1aa4dbd4e9080398be06a0384ee10f8f3
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 22 12:50:12 2015 -0500

    finish test user program
    
    uses its own page tables now.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a42a5d5fa7..e446981281 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -17,23 +17,16 @@ func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
 func Cli()
-func Copy_pgt(*[512]int) *[512]int
 func Install_traphandler(func(tf *[23]int, uc int))
-func Install_trapstub(func())
-func Kpgdir() *[512]int
-func Lapic_eoi()
-func Pgdir_walk(va uintptr) int
+func Kpmap() *[512]int
 func Pnum(int)
 func Rcr2() int
 func Sti()
-func Trapret(tf *[23]int)
-func Useradd(tf *[23]int, uc int, pgtbl *[512]int)
+func Useradd(tf *[23]int, uc int, p_pmap int)
 func Usercontinue()
 func Userrunnable()
 func Useryield()
-func Vtop(*[512]int) int
 
 func Death()
 func Fnaddr(func()) int
-func Newstack() int
 func Turdyprog()

commit 50ecbbdbd584d578e325aa6bc8fcbf6ec6ee2cc1
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Wed Jan 21 16:31:09 2015 -0500

    go virtual memory management
    
    this will be useful for user program setup.  cleanup types too.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 1e6f76e87e..a42a5d5fa7 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -18,20 +18,22 @@ func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
 func Cli()
 func Copy_pgt(*[512]int) *[512]int
-func Install_traphandler(func(tf *[23]uint64, uc int))
+func Install_traphandler(func(tf *[23]int, uc int))
 func Install_trapstub(func())
 func Kpgdir() *[512]int
 func Lapic_eoi()
 func Pgdir_walk(va uintptr) int
-func Pnum(m uint64)
+func Pnum(int)
+func Rcr2() int
 func Sti()
-func Trapret(tf *[23]uint64)
-func Useradd(tf *[23]uint64, uc int, pgtbl *[512]int)
+func Trapret(tf *[23]int)
+func Useradd(tf *[23]int, uc int, pgtbl *[512]int)
 func Usercontinue()
 func Userrunnable()
 func Useryield()
+func Vtop(*[512]int) int
 
 func Death()
-func Fnaddr(func()) uint64
-func Newstack() uint64
+func Fnaddr(func()) int
+func Newstack() int
 func Turdyprog()

commit 373f222385a59c8e30b980c817d63c94712f40a9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 19:20:50 2015 -0500

    put user programs in ring 3
    
    the test user program will crash unless the page tables are marked PTE_U.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 4235b5e99d..1e6f76e87e 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -17,14 +17,16 @@ func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
 func Cli()
+func Copy_pgt(*[512]int) *[512]int
 func Install_traphandler(func(tf *[23]uint64, uc int))
 func Install_trapstub(func())
+func Kpgdir() *[512]int
 func Lapic_eoi()
 func Pgdir_walk(va uintptr) int
 func Pnum(m uint64)
 func Sti()
 func Trapret(tf *[23]uint64)
-func Useradd(tf *[23]uint64, uc int)
+func Useradd(tf *[23]uint64, uc int, pgtbl *[512]int)
 func Usercontinue()
 func Userrunnable()
 func Useryield()

commit 418cba6989018144ba0a8f8e4affb65cd41fcad8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 16:31:15 2015 -0500

    checkpoint user programs + syscalls
    
    moving user programs to ring 3 next

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 6d6d1e5bc5..4235b5e99d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -16,15 +16,20 @@ func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 
-func Current_thread() int
 func Cli()
-func Install_traphandler(func(tf *[23]uint64))
+func Install_traphandler(func(tf *[23]uint64, uc int))
 func Install_trapstub(func())
 func Lapic_eoi()
 func Pgdir_walk(va uintptr) int
 func Pnum(m uint64)
 func Sti()
 func Trapret(tf *[23]uint64)
-func Tf_get(idx int, tf *[23]uint64) int32
-func Yieldy()
+func Useradd(tf *[23]uint64, uc int)
+func Usercontinue()
+func Userrunnable()
+func Useryield()
+
 func Death()
+func Fnaddr(func()) uint64
+func Newstack() uint64
+func Turdyprog()

commit a9476ab6c941c56ad37dc2c46c9662c320b76767
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 11:31:42 2015 -0500

    new trap/scheduling design and syscall stub

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 3f83b23708..6d6d1e5bc5 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -27,3 +27,4 @@ func Sti()
 func Trapret(tf *[23]uint64)
 func Tf_get(idx int, tf *[23]uint64) int32
 func Yieldy()
+func Death()

commit 69fee87513877a754faafd75c810b1a3a09928f9
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 20 11:29:01 2015 -0500

    silly bugfix

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 130834373d..3f83b23708 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -25,5 +25,5 @@ func Pgdir_walk(va uintptr) int
 func Pnum(m uint64)
 func Sti()
 func Trapret(tf *[23]uint64)
-func Tf_get(idx int, tf *[23]uint64) int
+func Tf_get(idx int, tf *[23]uint64) int32
 func Yieldy()

commit 40dab6659da8f4a78bc994df24ae8446caf3dec5
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Thu Jan 15 18:18:39 2015 -0500

    checkpoint bizarre go traphandler/scheduler
    
    need an idle OS proc if we want to handle faults caused by kernel go code which
    require immediate service. set maxprocs = 2 and do other stuff to make it work.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 751434f498..130834373d 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -19,9 +19,11 @@ func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
 func Current_thread() int
 func Cli()
 func Install_traphandler(func(tf *[23]uint64))
+func Install_trapstub(func())
 func Lapic_eoi()
 func Pgdir_walk(va uintptr) int
 func Pnum(m uint64)
 func Sti()
 func Trapret(tf *[23]uint64)
 func Tf_get(idx int, tf *[23]uint64) int
+func Yieldy()

commit 5bfed7c6c03bf3cc9a0a1d7a0ab056b9dfaae920
Author: Russ Cox <rsc@golang.org>
Date:   Wed Jan 14 11:18:24 2015 -0500

    runtime: log all thread stack traces during GODEBUG=crash on Linux and OS X
    
    Normally, a panic/throw only shows the thread stack for the current thread
    and all paused goroutines. Goroutines running on other threads, or other threads
    running on their system stacks, are opaque. Change that when GODEBUG=crash,
    by passing a SIGQUIT around to all the threads when GODEBUG=crash.
    If this works out reasonably well, we might make the SIGQUIT relay part of
    the standard panic/throw death, perhaps eliding idle m's.
    
    Change-Id: If7dd354f7f3a6e326d17c254afcf4f7681af2f8b
    Reviewed-on: https://go-review.googlesource.com/2811
    Reviewed-by: Rick Hudson <rlh@golang.org>

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 113219aab0..abea5d61aa 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -27,6 +27,7 @@ func rtsigprocmask(sig uint32, new, old *sigset, size int32)
 //go:noescape
 func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig uint32)
+func raiseproc(sig uint32)
 
 //go:noescape
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32

commit c1ee664573dc563f1c65e92230cfd6205fc42ed8
Author: Cody Cutler <ccutler@mat.lcs.mit.edu>
Date:   Tue Jan 13 21:44:37 2015 -0500

    baby go scheduler
    
    unfortunately, the trap handler itself is probably not that useful to write in
    go since, as go uses cooperative threads, a go trap handler interrupting its
    own go code is likely to cause problems.
    
    export helper functions too

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 41123ad570..751434f498 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -15,3 +15,13 @@ func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
 func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+
+func Current_thread() int
+func Cli()
+func Install_traphandler(func(tf *[23]uint64))
+func Lapic_eoi()
+func Pgdir_walk(va uintptr) int
+func Pnum(m uint64)
+func Sti()
+func Trapret(tf *[23]uint64)
+func Tf_get(idx int, tf *[23]uint64) int

commit e785e3acf8a4845ada7bed96f1a88355b1e0cde3
Author: Russ Cox <rsc@golang.org>
Date:   Tue Nov 11 17:08:54 2014 -0500

    [dev.cc] runtime: convert operating system support code from C to Go
    
    The conversion was done with an automated tool and then
    modified only as necessary to make it compile and run.
    
    [This CL is part of the removal of C code from package runtime.
    See golang.org/s/dev.cc for an overview.]
    
    LGTM=r
    R=r
    CC=austin, dvyukov, golang-codereviews, iant, khr
    https://golang.org/cl/174830044

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index 41123ad570..113219aab0 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -6,12 +6,28 @@ package runtime
 
 import "unsafe"
 
+//go:noescape
 func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer, val3 uint32) int32
+
+//go:noescape
 func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
-func rt_sigaction(sig uintptr, new, old unsafe.Pointer, size uintptr) int32
-func sigaltstack(new, old unsafe.Pointer)
-func setitimer(mode int32, new, old unsafe.Pointer)
-func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
+
+//go:noescape
+func rt_sigaction(sig uintptr, new, old *sigactiont, size uintptr) int32
+
+//go:noescape
+func sigaltstack(new, old *sigaltstackt)
+
+//go:noescape
+func setitimer(mode int32, new, old *itimerval)
+
+//go:noescape
+func rtsigprocmask(sig uint32, new, old *sigset, size int32)
+
+//go:noescape
 func getrlimit(kind int32, limit unsafe.Pointer) int32
-func raise(sig int32)
+func raise(sig uint32)
+
+//go:noescape
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+func osyield()

commit d2574e2adb658b46ea2d8e22b3195cc14da1affe
Author: Russ Cox <rsc@golang.org>
Date:   Tue Sep 16 10:22:15 2014 -0400

    runtime: remove duplicated Go constants
    
    The C header files are the single point of truth:
    every C enum constant Foo is available to Go as _Foo.
    Remove or redirect duplicate Go declarations so they
    cannot be out of sync.
    
    Eventually we will need to put constants in Go, but for now having
    them be out of sync with C is too risky. These predate the build
    support for auto-generating Go constants from the C definitions.
    
    LGTM=iant
    R=iant
    CC=golang-codereviews
    https://golang.org/cl/141510043

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
index a6799cd414..41123ad570 100644
--- a/src/runtime/os_linux.go
+++ b/src/runtime/os_linux.go
@@ -15,5 +15,3 @@ func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
 func getrlimit(kind int32, limit unsafe.Pointer) int32
 func raise(sig int32)
 func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
-
-const stackSystem = 0

commit c007ce824d9a4fccb148f9204e04c23ed2984b71
Author: Russ Cox <rsc@golang.org>
Date:   Mon Sep 8 00:08:51 2014 -0400

    build: move package sources from src/pkg to src
    
    Preparation was in CL 134570043.
    This CL contains only the effect of 'hg mv src/pkg/* src'.
    For more about the move, see golang.org/s/go14nopkg.

diff --git a/src/runtime/os_linux.go b/src/runtime/os_linux.go
new file mode 100644
index 0000000000..a6799cd414
--- /dev/null
+++ b/src/runtime/os_linux.go
@@ -0,0 +1,19 @@
+// Copyright 2014 The Go Authors.  All rights reserved.
+// Use of this source code is governed by a BSD-style
+// license that can be found in the LICENSE file.
+
+package runtime
+
+import "unsafe"
+
+func futex(addr unsafe.Pointer, op int32, val uint32, ts, addr2 unsafe.Pointer, val3 uint32) int32
+func clone(flags int32, stk, mm, gg, fn unsafe.Pointer) int32
+func rt_sigaction(sig uintptr, new, old unsafe.Pointer, size uintptr) int32
+func sigaltstack(new, old unsafe.Pointer)
+func setitimer(mode int32, new, old unsafe.Pointer)
+func rtsigprocmask(sig int32, new, old unsafe.Pointer, size int32)
+func getrlimit(kind int32, limit unsafe.Pointer) int32
+func raise(sig int32)
+func sched_getaffinity(pid, len uintptr, buf *uintptr) int32
+
+const stackSystem = 0
